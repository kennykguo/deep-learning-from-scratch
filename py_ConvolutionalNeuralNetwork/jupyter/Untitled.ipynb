{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4965554e-402d-4d13-9783-03c24d34063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as signal\n",
    "import math\n",
    "# import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0758618f-d988-4f9f-bcc3-75805cdefcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ConvolutionalNN:\n",
    "    def __init__(self):\n",
    "        # Convolution\n",
    "        self.layer_weights = np.random.randn(5, 5, 2) * math.sqrt(2. / 5)\n",
    "        self.delta_conv_weights = np.zeros_like(self.layer_weights)\n",
    "        self.layer_bias = np.zeros((24, 24, 2))\n",
    "        self.delta_conv_bias = np.zeros_like(self.layer_bias)\n",
    "        \n",
    "        # Fully connected\n",
    "        self.fc_weights = np.random.randn(10, 288) * math.sqrt(2. / 288)\n",
    "        self.delta_fc_weights = np.zeros_like(self.fc_weights)\n",
    "        self.fc_bias = np.zeros((10, 1))\n",
    "        self.delta_fc_bias = np.zeros_like(self.fc_bias)\n",
    "        self.delta_fc_bias = self.delta_fc_bias.reshape(10, 1)\n",
    "\n",
    "        # Batch normalization\n",
    "        self.gamma_conv = np.ones((24, 24, 2))\n",
    "        self.delta_gamma_conv = np.zeros_like(self.gamma_conv)\n",
    "\n",
    "        self.beta_conv = np.zeros((24, 24, 2))\n",
    "        self.delta_beta_conv = np.zeros_like(self.beta_conv)\n",
    "\n",
    "        self.gamma_fc = np.ones((10, 1))\n",
    "        self.delta_gamma_fc = np.zeros_like(self.gamma_fc)\n",
    "\n",
    "        self.beta_fc = np.zeros((10, 1))\n",
    "        self.delta_beta_fc = np.zeros_like(self.beta_fc)\n",
    "        \n",
    "        # Accuracies list\n",
    "        self.accuracies = [];\n",
    "\n",
    "    # All helper functions must be declared with self as the first argument since they are methods as part of the class\n",
    "    def max_pooling(self, input_data):\n",
    "        # (24, 24, 2)\n",
    "        input_height, input_width, input_depth = input_data.shape\n",
    "\n",
    "        # Calculate the output dimensions\n",
    "        output_height = input_height // 2 # 12\n",
    "        output_width = input_width // 2 # 12\n",
    "        output_depth = input_depth # 2 - depth stays the same\n",
    "\n",
    "        # Initialize the output array and array to store indices\n",
    "        output_data = np.zeros((output_height, output_width, output_depth))\n",
    "        indices = np.zeros((output_height, output_width, output_depth, 2), dtype=int)\n",
    "\n",
    "        # Apply max pooling\n",
    "        for h in range(output_height):\n",
    "            for w in range(output_width):\n",
    "                for d in range(output_depth):\n",
    "                    # Extract the 2x2 region of interest from the input data\n",
    "                    region = input_data[h*2:(h+1)*2, w*2:(w+1)*2, d]\n",
    "                    # Compute the maximum value in the region\n",
    "                    max_val = np.max(region)\n",
    "                    output_data[h, w, d] = max_val\n",
    "                    # Find the indices of the maximum value in the region\n",
    "                    max_indices = np.unravel_index(np.argmax(region), region.shape)\n",
    "                    # Store the indices relative to the region and convert to global indices\n",
    "                    indices[h, w, d] = [h*2 + max_indices[0], w*2 + max_indices[1]]\n",
    "        return output_data, indices\n",
    "\n",
    "    def batch_norm_forward(self, x, gamma, beta, eps=1e-5):\n",
    "        mean = np.mean(x, axis=0)\n",
    "        variance = np.var(x, axis=0)\n",
    "        x_normalized = (x - mean) / np.sqrt(variance + eps)\n",
    "        out = gamma * x_normalized + beta\n",
    "        cache = (x, x_normalized, mean, variance, gamma, beta, eps)\n",
    "        return out, cache\n",
    "\n",
    "    def batch_norm_backward(self, dout, cache):\n",
    "        x, x_normalized, mean, variance, gamma, beta, eps = cache\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        dbeta = np.sum(dout, axis=0)\n",
    "        dgamma = np.sum(dout * x_normalized, axis=0)\n",
    "        \n",
    "        dx_normalized = dout * gamma\n",
    "        dvariance = np.sum(dx_normalized * (x - mean) * -0.5 * np.power(variance + eps, -1.5), axis=0)\n",
    "        dmean = np.sum(dx_normalized * -1 / np.sqrt(variance + eps), axis=0) + dvariance * np.sum(-2 * (x - mean), axis=0) / N\n",
    "        \n",
    "        dx = dx_normalized / np.sqrt(variance + eps) + dvariance * 2 * (x - mean) / N + dmean / N\n",
    "        return dx, dgamma, dbeta\n",
    "\n",
    "    # Forward propogation\n",
    "    def forward_propagation(self, layer_input, layer_output, dropout_rate):\n",
    "        # Convolution\n",
    "        for i in range(2): # 2 filters in total\n",
    "            layer_output[:,:,i] = signal.correlate2d(layer_input, self.layer_weights[:,:,i], mode='valid')\n",
    "        layer_output = layer_output + self.layer_bias   # (24, 24, 2)\n",
    "        \n",
    "        # Batch Normalization for Conv Layer\n",
    "        # Functions that access the attributes of the class should not pass in a self to the function itself when called\n",
    "        layer_output, bn_cache_conv = self.batch_norm_forward(layer_output, self.gamma_conv, self.beta_conv)\n",
    "\n",
    "        # Activation layer\n",
    "        layer_output = self.ReLU(layer_output)  # (24, 24, 2)\n",
    "        \n",
    "        # Pool layer\n",
    "        layer_pool, layer_indices = self.max_pooling(layer_output)  # (12, 12, 2)\n",
    "        \n",
    "        # Flattening\n",
    "        layer_pool = layer_pool.reshape(288, 1) # (288, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        dropout_mask = (np.random.rand(*layer_pool.shape) < dropout_rate) / dropout_rate\n",
    "        layer_pool *= dropout_mask # (288, 1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        final_output = self.fc_weights.dot(layer_pool)  # (10, 288) (288, 1) = (10, 1)\n",
    "        final_output = final_output + self.fc_bias # (10, 1) + (10, 1) = (10, 1)\n",
    "        \n",
    "        # Batch Norm for FC\n",
    "        final_output, bn_cache_fc = self.batch_norm_forward(final_output, self.gamma_fc, self.beta_fc) # (10, 1)\n",
    "        \n",
    "        final_output = self.softmax(final_output) # (10, 1)\n",
    "        \n",
    "        return layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask\n",
    "\n",
    "\n",
    "    def back_prop(self, layer_input, layer_output, layer_pool, layer_indices, final_output, label,  bn_cache_conv, bn_cache_fc, dropout_mask):\n",
    "        self.delta_conv_weights *= 0\n",
    "        self.delta_conv_bias *= 0\n",
    "        self.delta_fc_weights *= 0\n",
    "        self.delta_fc_bias *= 0\n",
    "        self.delta_fc_bias = self.delta_fc_bias.reshape(10, 1)\n",
    "\n",
    "        # Backpropagate cost\n",
    "        x = self.create(label)\n",
    "        dZ = (final_output - x)  # (10, 1) - (10, 1) = (10, 1)\n",
    "        \n",
    "        # Backpropagate through Batch Normalization for Fully Connected Layer\n",
    "        dZ, self.delta_gamma_fc, self.delta_beta_fc = self.batch_norm_backward(dZ, bn_cache_fc)\n",
    "        \n",
    "        # Backpropagate weights and biases for Fully Connected Layer\n",
    "        self.delta_fc_weights = dZ.dot(layer_pool.T)  # (10, 1) (1, 288) = (10, 288)\n",
    "        self.delta_fc_bias = dZ\n",
    "        \n",
    "        # Backpropagate error\n",
    "        dZ_pool_output = np.dot(self.fc_weights.T, dZ) * self.der_ReLU(layer_pool)  # (288, 10) (10, 1) = (288, 1)\n",
    "        \n",
    "        # Undo Dropout\n",
    "        dZ_pool_output *= dropout_mask\n",
    "        \n",
    "        # Unflattening\n",
    "        dZ_pool_output = dZ_pool_output.reshape(12, 12, 2)\n",
    "        \n",
    "        # Unpooling\n",
    "        dZ_pool_input = np.zeros((24, 24, 2))\n",
    "        for i in range(12):  # height\n",
    "            for j in range(12):  # width\n",
    "                for k in range(2):  # depth\n",
    "                    # Get the global indices from layer_indices\n",
    "                    x_index, y_index = layer_indices[i, j, k]\n",
    "                    # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n",
    "                    dZ_pool_input[x_index, y_index, k] = dZ_pool_output[i, j, k]\n",
    "        \n",
    "        # Backpropagate through ReLU activation\n",
    "        dZ_pool_input *= self.der_ReLU(layer_output)\n",
    "        \n",
    "        # Backpropagate through Batch Normalization\n",
    "        dZ_pool_input, self.delta_gamma_conv, self.delta_beta_conv = self.batch_norm_backward(dZ_pool_input, bn_cache_conv)\n",
    "        \n",
    "        # Backpropagate Conv layer\n",
    "        for i in range(2):  # For each filter in the kernel\n",
    "            self.delta_conv_weights[:, :, i] = signal.correlate(layer_input, dZ_pool_input[:, :, i], mode=\"valid\")\n",
    "        self.delta_conv_bias = dZ_pool_input\n",
    "        \n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        self.layer_weights -= learning_rate * self.delta_conv_weights\n",
    "        self.layer_bias -= learning_rate * self.delta_conv_bias\n",
    "        self.fc_weights -= learning_rate * self.delta_fc_weights\n",
    "        self.fc_bias -= learning_rate * self.delta_fc_bias\n",
    "        self.gamma_conv -= learning_rate * self.delta_gamma_conv\n",
    "        self.beta_conv -= learning_rate * self.delta_beta_conv\n",
    "        self.gamma_fc -= learning_rate * self.delta_gamma_fc\n",
    "        self.beta_fc -= learning_rate * self.delta_beta_fc\n",
    "\n",
    "\n",
    "    # Helper functions (only using ReLU but can use others)\n",
    "    def get_prediction(self, A2):\n",
    "        return np.argmax(A2, 0)\n",
    "    \n",
    "    def create(self, Y):\n",
    "        column_Y = np.zeros((10, 1))\n",
    "        column_Y[Y] = 1\n",
    "        column_Y = column_Y.T\n",
    "        return column_Y.reshape(10,1)\n",
    "    \n",
    "    def der_ReLU(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    def ReLU2(self, Z, alpha=0.01):\n",
    "        return np.where(Z > 0, Z, alpha * Z)\n",
    "    \n",
    "    def ReLU(self, Z):\n",
    "        return np.maximum(Z, 0)\n",
    "    \n",
    "    def ReLU2(self, Z):\n",
    "        return np.maximum(Z, 0)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        # Compute the sigmoid function element-wise\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        # Apply softmax column-wise\n",
    "        exp_Z = np.exp(Z - np.max(Z, axis=0)) # Subtracting the maximum value in each column to avoid overflow\n",
    "        return exp_Z / np.sum(exp_Z, axis=0)\n",
    "    \n",
    "    def stochastic_gradient_descent(self, X_train, X_dev, Y_train, Y_dev, epochs, learning_rate, dropout_rate, batch_size):\n",
    "        \n",
    "        num_examples = X_train.shape[2]\n",
    "\n",
    "        layer_output = np.zeros((24, 24, 2))\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch:\", i + 1)\n",
    "            \n",
    "            # Generate a random permutation of indices\n",
    "            permuted_indices = np.random.permutation(X_train.shape[2])\n",
    "            \n",
    "            # Shuffle both X_train and Y_train using the same permutation of indices\n",
    "            X_train_shuffled = X_train[:, :, permuted_indices]\n",
    "            Y_train_shuffled = Y_train[permuted_indices]\n",
    "            \n",
    "            # We generate batches for a smaller subsection of the training set (the entire training set takes too much time currently)\n",
    "            for batch_start in range(0, int(X_train_shuffled.shape[2]/100), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, num_examples)\n",
    "                batch_gradients = [0, 0, 0, 0, 0, 0, 0, 0]  # Accumulate gradients over the batch and reset them to zero at each loop of rebatching\n",
    "                for j in range(batch_start, batch_end):\n",
    "                    # Get a single training example\n",
    "                    layer_input = X_train_shuffled[:, :, j]\n",
    "                    label = Y_train_shuffled[j]\n",
    "                    \n",
    "                    # Forward propagation (same input as in Jupyter)\n",
    "                    layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask = self.forward_propagation(layer_input, layer_output, dropout_rate)\n",
    "                    \n",
    "                    # Back propagation\n",
    "                    self.back_prop(layer_input, layer_output, layer_pool, layer_indices, final_output, label, bn_cache_conv, bn_cache_fc, dropout_mask\n",
    "                    )\n",
    "                    \n",
    "                    # Accumulate gradients\n",
    "                    batch_gradients[0] += self.delta_conv_weights\n",
    "                    batch_gradients[1] += self.delta_conv_bias\n",
    "                    batch_gradients[2] += self.delta_fc_weights\n",
    "                    batch_gradients[3] += self.delta_fc_bias\n",
    "                    batch_gradients[4] += self.delta_gamma_conv\n",
    "                    batch_gradients[5] += self.delta_beta_conv\n",
    "                    batch_gradients[6] += self.delta_gamma_fc\n",
    "                    batch_gradients[7] += self.delta_beta_fc\n",
    "                \n",
    "                # Average gradients after processing the batch\n",
    "                batch_gradients = [grad / batch_size for grad in batch_gradients]\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_params(learning_rate)\n",
    "            \n",
    "            # Get training accuracy\n",
    "            counter = 0\n",
    "            for j in range(int(X_train_shuffled.shape[2]/100)):\n",
    "                test_input = X_train_shuffled[:, :, j]\n",
    "                layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask = self.forward_propagation(test_input, layer_output, dropout_rate\n",
    "                )\n",
    "                prediction = self.get_prediction(final_output)\n",
    "                predicted_label = prediction[0]\n",
    "                if Y_train_shuffled[j] == predicted_label:\n",
    "                    counter += 1\n",
    "            print(\"Training Accuracy:\", counter / int(X_train_shuffled.shape[2]/100))\n",
    "\n",
    "            counter = 0\n",
    "            for j in range(500):\n",
    "                test_input = X_dev[:, :, j]\n",
    "                layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask = self.forward_propagation(test_input, layer_output, dropout_rate\n",
    "                )\n",
    "                prediction = self.get_prediction(final_output)\n",
    "                predicted_label = prediction[0]\n",
    "                if Y_dev[j] == predicted_label:\n",
    "                    counter += 1\n",
    "            print(\"Validation Accuracy:\", counter / 500)\n",
    "        \n",
    "        plt.imshow(layer_output[:,:,1], cmap='gray')\n",
    "        plt.axis('off')  # Turn off axis\n",
    "        plt.show()\n",
    "        plt.imshow(layer_output[:,:,0], cmap='gray')\n",
    "        plt.axis('off')  # Turn off axis\n",
    "        plt.show()\n",
    "    def plot_accuracies(self):\n",
    "        epochs = range(1, len(self.accuracies) + 1)\n",
    "        plt.plot(epochs, self.accuracies, 'b-', label='Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Model Accuracy over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83042421-ddd2-4686-9970-13bda10e907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, file_path):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.X_dev = None\n",
    "        self.Y_dev = None\n",
    "        self.X_train = None\n",
    "        self.Y_train = None\n",
    "        self.X_test = None\n",
    "        self.Y_test = None\n",
    "\n",
    "    def process_data(self):\n",
    "        self.data = np.array(self.data)\n",
    "        m, n = self.data.shape\n",
    "\n",
    "        np.random.shuffle(self.data) # Shuffles all the individual rows\n",
    "\n",
    "        data_dev = self.data[0:1000].T #Take the first 1000 rows, and transpose the matrix to get 1000 examples as column vectors\n",
    "        self.Y_dev = data_dev[0] #Takes the first row, which contains all of the answers to the numbers (the Y is what we want)\n",
    "        self.X_dev = data_dev[1:n]\n",
    "        self.X_dev = self.X_dev.reshape(28,28,1000)\n",
    "\n",
    "        data_train = self.data[2000:m].T\n",
    "        self.Y_train = data_train[0]\n",
    "        self.X_train= data_train[1:n] #Takes all of the data corresponding to all of the entries\n",
    "        self.X_train = self.X_train.reshape(28,28,-1)\n",
    "\n",
    "        data_test = self.data[1000:2000].T\n",
    "        self.Y_test = data_test[0]\n",
    "        self.X_test= data_test[1:n] #Takes all of the data corresponding to all of the entries\n",
    "        self.X_test = self.X_test.reshape(28,28,-1)\n",
    "\n",
    "    def plot_mnist_image(self, image_array):\n",
    "        image = image_array.reshape(28, 28)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')  # Turn off axis\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "670238e5-ce89-4ccd-be9d-ebc48c28bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Accuracy: 0.1425\n",
      "Validation Accuracy: 0.116\n",
      "Epoch: 2\n",
      "Training Accuracy: 0.135\n",
      "Validation Accuracy: 0.114\n",
      "Epoch: 3\n",
      "Training Accuracy: 0.11\n",
      "Validation Accuracy: 0.116\n",
      "Epoch: 4\n",
      "Training Accuracy: 0.1\n",
      "Validation Accuracy: 0.116\n",
      "Epoch: 5\n",
      "Training Accuracy: 0.1225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m Model \u001b[38;5;241m=\u001b[39m ConvolutionalNN()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Epochs, learning_rate, dropout, batch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstochastic_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# np.savez(\"model_parameters.npz\", Model.dW1, Model.b1, dW2=Model.W2, db2=Model.db2)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Model.plot_accuracies()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 281\u001b[0m, in \u001b[0;36mConvolutionalNN.stochastic_gradient_descent\u001b[0;34m(self, X_train, X_dev, Y_train, Y_dev, epochs, learning_rate, dropout_rate, batch_size)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m    280\u001b[0m     test_input \u001b[38;5;241m=\u001b[39m X_dev[:, :, j]\n\u001b[0;32m--> 281\u001b[0m     layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prediction(final_output)\n\u001b[1;32m    284\u001b[0m     predicted_label \u001b[38;5;241m=\u001b[39m prediction[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 103\u001b[0m, in \u001b[0;36mConvolutionalNN.forward_propagation\u001b[0;34m(self, layer_input, layer_output, dropout_rate)\u001b[0m\n\u001b[1;32m    100\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReLU(layer_output)  \u001b[38;5;66;03m# (24, 24, 2)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Pool layer\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m layer_pool, layer_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pooling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (12, 12, 2)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Flattening\u001b[39;00m\n\u001b[1;32m    106\u001b[0m layer_pool \u001b[38;5;241m=\u001b[39m layer_pool\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m288\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (288, 1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m, in \u001b[0;36mConvolutionalNN.max_pooling\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     59\u001b[0m output_data[h, w, d] \u001b[38;5;241m=\u001b[39m max_val\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Find the indices of the maximum value in the region\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m max_indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munravel_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Store the indices relative to the region and convert to global indices\u001b[39;00m\n\u001b[1;32m     63\u001b[0m indices[h, w, d] \u001b[38;5;241m=\u001b[39m [h\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m max_indices[\u001b[38;5;241m0\u001b[39m], w\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m max_indices[\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Interface\n",
    "Data = Dataset('../data/train.csv')\n",
    "\n",
    "Data.process_data()\n",
    "\n",
    "Model = ConvolutionalNN()\n",
    "\n",
    "# Epochs, learning_rate, dropout, batch\n",
    "Model.stochastic_gradient_descent(Data.X_train, Data.X_dev, Data.Y_train, Data.Y_dev, 10, 0.5, 0.5, 32)\n",
    "\n",
    "# np.savez(\"model_parameters.npz\", Model.dW1, Model.b1, dW2=Model.W2, db2=Model.db2)\n",
    "\n",
    "# Model.plot_accuracies()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a700751-f953-4fb7-815d-63a5eac2d741",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m label \u001b[38;5;241m=\u001b[39m Data\u001b[38;5;241m.\u001b[39mY_dev[i]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(label)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# plt.imshow(test, cmap='gray')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# plt.axis('off')  # Turn off axis\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# print(\"Testing model:\")\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m _, _, _, final_output, _, _, _ \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39mforward_propagation(test, \u001b[43mlayer_output\u001b[49m, dropout_rate)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print(\"Model prediction: \")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m prediction \u001b[38;5;241m=\u001b[39m get_prediction(final_output)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_output' is not defined"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in range(1000):\n",
    "    test = Data.X_dev[:,:,i]\n",
    "    label = Data.Y_dev[i]\n",
    "    # print(label)\n",
    "    # plt.imshow(test, cmap='gray')\n",
    "    # plt.axis('off')  # Turn off axis\n",
    "    # plt.show()\n",
    "    # print(\"Testing model:\")\n",
    "    _, _, _, final_output, _, _, _ = Model.forward_propagation(test, layer_output, dropout_rate)\n",
    "    # print(\"Model prediction: \")\n",
    "    prediction = get_prediction(final_output)\n",
    "    if (prediction == label):\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
