{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f74cad-a18b-4772-a59a-074086702076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0624881d-5967-451c-a759-0bc6805e5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 40000)\n",
      "(40000,)\n",
      "(28, 28, 1000)\n",
      "(1000,)\n",
      "(28, 28, 1000)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "\n",
    "np.random.shuffle(data) # Shuffles all the individual rows\n",
    "\n",
    "data_dev = data[0:1000].T #Take the first 1000 rows, and transpose the matrix to get 1000 examples as column vectors\n",
    "Y_dev = data_dev[0] #Takes the first row, which contains all of the answers to the numbers (the Y is what we want)\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev.reshape(28,28,1000)\n",
    "\n",
    "data_train = data[2000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train= data_train[1:n] #Takes all of the data corresponding to all of the entries\n",
    "X_train = X_train.reshape(28,28,-1)\n",
    "\n",
    "data_test = data[1000:2000].T\n",
    "Y_test = data_test[0]\n",
    "X_test= data_test[1:n] #Takes all of the data corresponding to all of the entries\n",
    "X_test = X_test.reshape(28,28,-1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5864e0c-acf3-4def-b894-6913bcc0d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_forward(x, gamma, beta, eps=1e-5):\n",
    "    mean = np.mean(x, axis=0)\n",
    "    variance = np.var(x, axis=0)\n",
    "    x_normalized = (x - mean) / np.sqrt(variance + eps)\n",
    "    out = gamma * x_normalized + beta\n",
    "    cache = (x, x_normalized, mean, variance, gamma, beta, eps)\n",
    "    return out, cache\n",
    "\n",
    "def batch_norm_backward(dout, cache):\n",
    "    x, x_normalized, mean, variance, gamma, beta, eps = cache\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgamma = np.sum(dout * x_normalized, axis=0)\n",
    "    \n",
    "    dx_normalized = dout * gamma\n",
    "    dvariance = np.sum(dx_normalized * (x - mean) * -0.5 * np.power(variance + eps, -1.5), axis=0)\n",
    "    dmean = np.sum(dx_normalized * -1 / np.sqrt(variance + eps), axis=0) + dvariance * np.sum(-2 * (x - mean), axis=0) / N\n",
    "    \n",
    "    dx = dx_normalized / np.sqrt(variance + eps) + dvariance * 2 * (x - mean) / N + dmean / N\n",
    "    return dx, dgamma, dbeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb5c539-b2e6-4cea-ab1d-9f7580cd0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(input_data):\n",
    "    # 24, 24, 2\n",
    "    input_height, input_width, input_depth = input_data.shape\n",
    "\n",
    "    # Calculate the output dimensions\n",
    "    output_height = input_height // 2 # 12\n",
    "    output_width = input_width // 2 # 12\n",
    "    output_depth = input_depth # 2 - depth stays the same\n",
    "\n",
    "    # Initialize the output array and array to store indices\n",
    "    output_data = np.zeros((output_height, output_width, output_depth))\n",
    "    indices = np.zeros((output_height, output_width, output_depth, 2), dtype=int)\n",
    "\n",
    "    # Apply max pooling\n",
    "    for h in range(output_height):\n",
    "        for w in range(output_width):\n",
    "            for d in range(output_depth):\n",
    "                # Extract the 2x2 region of interest from the input data\n",
    "                region = input_data[h*2:(h+1)*2, w*2:(w+1)*2, d]\n",
    "                # Compute the maximum value in the region\n",
    "                max_val = np.max(region)\n",
    "                output_data[h, w, d] = max_val\n",
    "                # Find the indices of the maximum value in the region\n",
    "                max_indices = np.unravel_index(np.argmax(region), region.shape)\n",
    "                # Store the indices relative to the region and convert to global indices\n",
    "                indices[h, w, d] = [h*2 + max_indices[0], w*2 + max_indices[1]]\n",
    "\n",
    "    return output_data, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec77b82a-5105-48bd-bba6-eea0ae38b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_ReLU(Z):\n",
    "  return Z > 0\n",
    "\n",
    "def ReLU2(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "def ReLU(Z): # Takes in a scalar, returns a scalar\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU2(Z): # Takes in a scalar, returns a scalar\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "  return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Compute the sigmoid function element-wise\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def softmax(Z):\n",
    "    # Apply softmax column-wise\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0))  # Subtracting the maximum value in each column to avoid overflow\n",
    "    return exp_Z / np.sum(exp_Z, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce95ada-3428-48ba-9156-6bcb650d8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def params():\n",
    "    layer_weights = np.random.randn(5, 5, 2) * np.sqrt(2. / 5)\n",
    "    layer_bias = np.zeros((24, 24, 2))\n",
    "    layer_output = np.zeros((24, 24, 2)) #(24, 24, 2)\n",
    "    fc_weights = np.random.randn(10, 288) * np.sqrt(2. / 288)\n",
    "    fc_bias = np.zeros((10, 1))\n",
    "    gamma_conv = np.ones((24, 24, 2))\n",
    "    beta_conv = np.zeros((24, 24, 2))\n",
    "    gamma_fc = np.ones((10, 1))\n",
    "    beta_fc = np.zeros((10, 1))\n",
    "    return layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "\n",
    "\n",
    "def forward_propagation(layer_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc, dropout_rate=0.5):\n",
    "    # Convolution\n",
    "    for i in range(2): # 2 filters in total\n",
    "        layer_output[:,:,i] = signal.correlate2d(layer_input, layer_weights[:,:,i], mode='valid')\n",
    "    layer_output = layer_output + layer_bias   # (24, 24, 2)\n",
    "    \n",
    "    # Batch Normalization for Convolutional Layer\n",
    "    layer_output, bn_cache_conv = batch_norm_forward(layer_output, gamma_conv, beta_conv)\n",
    "    \n",
    "    # Activation layer\n",
    "    layer_output = ReLU(layer_output)  # (24, 24, 2)\n",
    "    \n",
    "    # Pool layer\n",
    "    layer_pool, layer_indices = max_pooling(layer_output)  # (12, 12, 2)\n",
    "    \n",
    "    # Flattening\n",
    "    layer_pool = layer_pool.reshape(288, 1) # (288, 1)\n",
    "    \n",
    "    # Dropout\n",
    "    dropout_mask = (np.random.rand(*layer_pool.shape) < dropout_rate) / dropout_rate\n",
    "    layer_pool *= dropout_mask\n",
    "    \n",
    "    # Fully connected layer\n",
    "    final_output = fc_weights.dot(layer_pool)  # (10, 288) (288, 1) = (10, 1)\n",
    "    final_output = final_output + fc_bias # (10, 1) + (10, 1) = (10, 1)\n",
    "    \n",
    "    # Batch Normalization for Fully Connected Layer\n",
    "    final_output, bn_cache_fc = batch_norm_forward(final_output, gamma_fc, beta_fc)\n",
    "    \n",
    "    final_output = softmax(final_output) # (10, 1)\n",
    "    \n",
    "    return layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask\n",
    "\n",
    "\n",
    "def create(Y):\n",
    "  column_Y = np.zeros((10, 1))\n",
    "  column_Y[Y] = 1\n",
    "  column_Y = column_Y.T\n",
    "  return column_Y.reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea97843b-5871-4db2-bc7d-1baae6ec752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(layer_input, layer_output, layer_pool, layer_indices, final_output, label, layer_weights, layer_bias, fc_weights, fc_bias, bn_cache_conv, bn_cache_fc, dropout_mask):\n",
    "    # Initialize parameters\n",
    "    delta_conv_weights = np.zeros_like(layer_weights)\n",
    "    delta_conv_bias = np.zeros_like(layer_bias)\n",
    "    delta_fc_weights = np.zeros_like(fc_weights)\n",
    "    delta_fc_bias = np.zeros_like(fc_bias)\n",
    "    delta_fc_bias = delta_fc_bias.reshape(10, 1)\n",
    "    \n",
    "    # Backpropagate cost\n",
    "    x = create(label)\n",
    "    dZ = (final_output - x)  # (10, 1) - (10, 1) = (10, 1)\n",
    "    \n",
    "    # Backpropagate through Batch Normalization for Fully Connected Layer\n",
    "    dZ, dgamma_fc, dbeta_fc = batch_norm_backward(dZ, bn_cache_fc)\n",
    "    \n",
    "    # Backpropagate weights and biases for Fully Connected Layer\n",
    "    delta_fc_weights = dZ.dot(layer_pool.T)  # (10, 1) (1, 288) = (10, 288)\n",
    "    delta_fc_bias = dZ\n",
    "    \n",
    "    # Backpropagate error\n",
    "    dZ_pool_output = np.dot(fc_weights.T, dZ) * der_ReLU(layer_pool)  # (288, 10) (10, 1) = (288, 1)\n",
    "    \n",
    "    # Undo Dropout\n",
    "    dZ_pool_output *= dropout_mask\n",
    "    \n",
    "    # Unflattening\n",
    "    dZ_pool_output = dZ_pool_output.reshape(12, 12, 2)\n",
    "    \n",
    "    # Unpooling\n",
    "    dZ_pool_input = np.zeros((24, 24, 2))\n",
    "    for i in range(12):  # height\n",
    "        for j in range(12):  # width\n",
    "            for k in range(2):  # depth\n",
    "                # Get the global indices from layer_indices\n",
    "                x_index, y_index = layer_indices[i, j, k]\n",
    "                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n",
    "                dZ_pool_input[x_index, y_index, k] = dZ_pool_output[i, j, k]\n",
    "    \n",
    "    # Backpropagate through ReLU activation\n",
    "    dZ_pool_input *= der_ReLU(layer_output)\n",
    "    \n",
    "    # Backpropagate through Batch Normalization\n",
    "    dZ_pool_input, dgamma_conv, dbeta_conv = batch_norm_backward(dZ_pool_input, bn_cache_conv)\n",
    "    \n",
    "    # Backpropagate Conv layer\n",
    "    for i in range(2):  # For each filter in the kernel\n",
    "        delta_conv_weights[:, :, i] = signal.correlate(layer_input, dZ_pool_input[:, :, i], mode=\"valid\")\n",
    "    delta_conv_bias = dZ_pool_input\n",
    "    \n",
    "    return delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias, dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc\n",
    "\n",
    " \n",
    "def update_params(layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc,\n",
    "                  delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias,\n",
    "                  dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc, learning_rate):\n",
    "    layer_weights -= learning_rate * delta_conv_weights\n",
    "    layer_bias -= learning_rate * delta_conv_bias\n",
    "    fc_weights -= learning_rate * delta_fc_weights\n",
    "    fc_bias -= learning_rate * delta_fc_bias\n",
    "    gamma_conv -= learning_rate * dgamma_conv\n",
    "    beta_conv -= learning_rate * dbeta_conv\n",
    "    gamma_fc -= learning_rate * dgamma_fc\n",
    "    beta_fc -= learning_rate * dbeta_fc\n",
    "    return layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "\n",
    "\n",
    "def get_prediction(A2):\n",
    "  return np.argmax(A2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82019a37-ccc9-4530-86e4-4ef9272aaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X_train, X_dev, Y_train, Y_dev, epochs, learning_rate, batch_size):\n",
    "    # Initialize parameters\n",
    "    layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = params()\n",
    "    gamma_conv = np.ones((24, 24, 2))\n",
    "    beta_conv = np.zeros((24, 24, 2))\n",
    "    gamma_fc = np.ones((10, 1))\n",
    "    beta_fc = np.zeros((10, 1))\n",
    "    num_examples = X_train.shape[2]\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch:\", i + 1)\n",
    "        \n",
    "        # Generate a random permutation of indices\n",
    "        permuted_indices = np.random.permutation(X_train.shape[2])\n",
    "        \n",
    "        # Shuffle both X_train and Y_train using the same permutation of indices\n",
    "        X_train_shuffled = X_train[:, :, permuted_indices]\n",
    "        Y_train_shuffled = Y_train[permuted_indices]\n",
    "        \n",
    "        for batch_start in range(0, int(X_train_shuffled.shape[2]/100), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, num_examples)\n",
    "            batch_gradients = [0, 0, 0, 0, 0, 0, 0, 0]  # Accumulate gradients over the batch\n",
    "            for j in range(batch_start, batch_end):\n",
    "                # Get a single training example\n",
    "                layer_input = X_train_shuffled[:, :, j]\n",
    "                label = Y_train_shuffled[j]\n",
    "                # print(label, end='')\n",
    "                \n",
    "                # Forward propagation\n",
    "                layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask = forward_propagation(\n",
    "                    layer_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "                )\n",
    "                \n",
    "                # Back propagation\n",
    "                delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias, dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc = back_prop(\n",
    "                    layer_input, layer_output, layer_pool, layer_indices, final_output, label,\n",
    "                    layer_weights, layer_bias, fc_weights, fc_bias, bn_cache_conv, bn_cache_fc, dropout_mask\n",
    "                )\n",
    "                \n",
    "                # Accumulate gradients\n",
    "                batch_gradients[0] += delta_conv_weights\n",
    "                batch_gradients[1] += delta_conv_bias\n",
    "                batch_gradients[2] += delta_fc_weights\n",
    "                batch_gradients[3] += delta_fc_bias\n",
    "                batch_gradients[4] += dgamma_conv\n",
    "                batch_gradients[5] += dbeta_conv\n",
    "                batch_gradients[6] += dgamma_fc\n",
    "                batch_gradients[7] += dbeta_fc\n",
    "            # print(\"\\n\")\n",
    "            # Average gradients after processing the batch\n",
    "            batch_gradients = [grad / batch_size for grad in batch_gradients]\n",
    "            \n",
    "            # Update parameters after processing the batch\n",
    "            layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = update_params(\n",
    "                layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc,\n",
    "                *batch_gradients,  # Use averaged gradients\n",
    "                learning_rate\n",
    "            )\n",
    "        \n",
    "        # Get training accuracy\n",
    "        counter = 0\n",
    "        for j in range(int(X_train_shuffled.shape[2]/100)):\n",
    "            test_input = X_train_shuffled[:, :, j]\n",
    "            layer_output, layer_pool, layer_indices, final_output, _, _, _ = forward_propagation(\n",
    "                test_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "            )\n",
    "            prediction = get_prediction(final_output)\n",
    "            predicted_label = prediction[0]\n",
    "            if Y_train_shuffled[j] == predicted_label:\n",
    "                counter += 1\n",
    "        print(\"Training Accuracy:\", counter / int(X_train_shuffled.shape[2]/100))\n",
    "        counter = 0\n",
    "        for j in range(500):\n",
    "            test_input = X_dev[:, :, j]\n",
    "            layer_output, layer_pool, layer_indices, final_output, _, _, _ = forward_propagation(\n",
    "                test_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "            )\n",
    "            prediction = get_prediction(final_output)\n",
    "            predicted_label = prediction[0]\n",
    "            if Y_dev[j] == predicted_label:\n",
    "                counter += 1\n",
    "        print(\"Validation Accuracy:\", counter / 500)\n",
    "    return layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d00daad9-e573-40a7-bb41-e8c87e4b6b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "92575\n",
      "\n",
      "14281\n",
      "\n",
      "35137\n",
      "\n",
      "94194\n",
      "\n",
      "57552\n",
      "\n",
      "25108\n",
      "\n",
      "98473\n",
      "\n",
      "78166\n",
      "\n",
      "39602\n",
      "\n",
      "11790\n",
      "\n",
      "94379\n",
      "\n",
      "60423\n",
      "\n",
      "82996\n",
      "\n",
      "73050\n",
      "\n",
      "91142\n",
      "\n",
      "81593\n",
      "\n",
      "00138\n",
      "\n",
      "60103\n",
      "\n",
      "93210\n",
      "\n",
      "95702\n",
      "\n",
      "79416\n",
      "\n",
      "14268\n",
      "\n",
      "90011\n",
      "\n",
      "22671\n",
      "\n",
      "23287\n",
      "\n",
      "52169\n",
      "\n",
      "43199\n",
      "\n",
      "72279\n",
      "\n",
      "11998\n",
      "\n",
      "91499\n",
      "\n",
      "94031\n",
      "\n",
      "08156\n",
      "\n",
      "43235\n",
      "\n",
      "67190\n",
      "\n",
      "97477\n",
      "\n",
      "65660\n",
      "\n",
      "41050\n",
      "\n",
      "22707\n",
      "\n",
      "32251\n",
      "\n",
      "17513\n",
      "\n",
      "80462\n",
      "\n",
      "12545\n",
      "\n",
      "11818\n",
      "\n",
      "67691\n",
      "\n",
      "03669\n",
      "\n",
      "03804\n",
      "\n",
      "72110\n",
      "\n",
      "43566\n",
      "\n",
      "40729\n",
      "\n",
      "48152\n",
      "\n",
      "56451\n",
      "\n",
      "67133\n",
      "\n",
      "45668\n",
      "\n",
      "34944\n",
      "\n",
      "73549\n",
      "\n",
      "41347\n",
      "\n",
      "67115\n",
      "\n",
      "72950\n",
      "\n",
      "15035\n",
      "\n",
      "42305\n",
      "\n",
      "92848\n",
      "\n",
      "49327\n",
      "\n",
      "02913\n",
      "\n",
      "11457\n",
      "\n",
      "98682\n",
      "\n",
      "89606\n",
      "\n",
      "06885\n",
      "\n",
      "35734\n",
      "\n",
      "08274\n",
      "\n",
      "81156\n",
      "\n",
      "23736\n",
      "\n",
      "70160\n",
      "\n",
      "53309\n",
      "\n",
      "66320\n",
      "\n",
      "11501\n",
      "\n",
      "74586\n",
      "\n",
      "31638\n",
      "\n",
      "37970\n",
      "\n",
      "57508\n",
      "\n",
      "87344\n",
      "\n",
      "Training Accuracy: 0.7075\n",
      "Validation Accuracy: 0.622\n",
      "Epoch: 2\n",
      "97349\n",
      "\n",
      "12331\n",
      "\n",
      "81906\n",
      "\n",
      "81374\n",
      "\n",
      "38536\n",
      "\n",
      "59464\n",
      "\n",
      "82615\n",
      "\n",
      "30290\n",
      "\n",
      "99129\n",
      "\n",
      "79561\n",
      "\n",
      "86417\n",
      "\n",
      "91063\n",
      "\n",
      "16571\n",
      "\n",
      "42213\n",
      "\n",
      "94010\n",
      "\n",
      "77452\n",
      "\n",
      "65540\n",
      "\n",
      "56509\n",
      "\n",
      "50337\n",
      "\n",
      "09929\n",
      "\n",
      "2398"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc \u001b[38;5;241m=\u001b[39m \u001b[43mstochastic_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 30\u001b[0m, in \u001b[0;36mstochastic_gradient_descent\u001b[0;34m(X_train, X_dev, Y_train, Y_dev, epochs, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(label, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Forward propagation\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_conv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_fc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_fc\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Back propagation\u001b[39;00m\n\u001b[1;32m     35\u001b[0m delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias, dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc \u001b[38;5;241m=\u001b[39m back_prop(\n\u001b[1;32m     36\u001b[0m     layer_input, layer_output, layer_pool, layer_indices, final_output, label,\n\u001b[1;32m     37\u001b[0m     layer_weights, layer_bias, fc_weights, fc_bias, bn_cache_conv, bn_cache_fc, dropout_mask\n\u001b[1;32m     38\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m, in \u001b[0;36mforward_propagation\u001b[0;34m(layer_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc, dropout_rate)\u001b[0m\n\u001b[1;32m     40\u001b[0m final_output \u001b[38;5;241m=\u001b[39m final_output \u001b[38;5;241m+\u001b[39m fc_bias \u001b[38;5;66;03m# (10, 1) + (10, 1) = (10, 1)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Batch Normalization for Fully Connected Layer\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m final_output, bn_cache_fc \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_norm_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_fc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_fc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m final_output \u001b[38;5;241m=\u001b[39m softmax(final_output) \u001b[38;5;66;03m# (10, 1)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mbatch_norm_forward\u001b[0;34m(x, gamma, beta, eps)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_norm_forward\u001b[39m(x, gamma, beta, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m):\n\u001b[1;32m      2\u001b[0m     mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     x_normalized \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(variance \u001b[38;5;241m+\u001b[39m eps)\n\u001b[1;32m      5\u001b[0m     out \u001b[38;5;241m=\u001b[39m gamma \u001b[38;5;241m*\u001b[39m x_normalized \u001b[38;5;241m+\u001b[39m beta\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3787\u001b[0m, in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m   3784\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3785\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m var(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3787\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3788\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/numpy/core/_methods.py:146\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    142\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDegrees of freedom <= 0 for slice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Cast bool, unsigned int, and int to float64 by default\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    147\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Compute the mean.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Note that if dtype is not of inexact type then arraymean will\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# not be either.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = stochastic_gradient_descent(\n",
    "    X_train, X_dev, Y_train, Y_dev, 2, 0.2, 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1df4f1a6-6227-4556-8395-3a9d1a2d20e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer output [[[0.         0.        ]\n",
      "  [0.         0.03420106]\n",
      "  [0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.69624131 0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.03415497]\n",
      "  [0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.24216574]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.03417375]\n",
      "  [0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [2.31839008 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.04247588]\n",
      "  [0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.03469679]\n",
      "  [0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.70424194 0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.03403543]\n",
      "  [0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         2.34753873]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"layer weights\", layer_weights)\n",
    "# print(\"layer bias\", layer_bias)\n",
    "print(\"layer output\", layer_output)\n",
    "# print(\"fc weights\", fc_weights)\n",
    "# print(\"fc_bias\", fc_bias)\n",
    "plt.imshow(layer_output[:,:,1], cmap='gray')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()\n",
    "plt.imshow(layer_output[:,:,0], cmap='gray')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a1531-056b-41dd-9bb5-550c39b421fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in range(1000):\n",
    "    test = X_dev[:,:,i]\n",
    "    label = Y_dev[i]\n",
    "    # print(label)\n",
    "    # plt.imshow(test, cmap='gray')\n",
    "    # plt.axis('off')  # Turn off axis\n",
    "    # plt.show()\n",
    "    # print(\"Testing model:\")\n",
    "    _, _, _, final_output, _, _, _ = forward_propagation(\n",
    "                    test, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "                )\n",
    "    # print(\"Model prediction: \")\n",
    "    prediction = get_prediction(final_output)\n",
    "    if (prediction == label):\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
