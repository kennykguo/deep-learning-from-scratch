{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Non-Linear Regression For Classification </h1> <br>\n",
    "<b>Example of how to implement an MLP with 1 hidden layer using numpy</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading the data </h3>\n",
    "Lets load some \"toy\" data that we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can load your data using this cell\n",
    "npzfile = np.load(\"../data/toy_data_two_moon.npz\") # toy_data.npz or toy_data_two_circles.npz\n",
    "\n",
    "#The compressed Numpy file is split up into 4 parts\n",
    "#Train inputs and target outputs\n",
    "#Test inputs and target outputs\n",
    "x_train = npzfile['arr_0']\n",
    "x_test = npzfile['arr_1']\n",
    "y_train = npzfile['arr_2']\n",
    "y_test = npzfile['arr_3']\n",
    "\n",
    "# remember that each row in x_train and X_test is a sample. so x_train[1,:] is the first training sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's plot our data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see what the data looks like\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], marker='o', c=y_train[:,0], s=20, edgecolor='k')\n",
    "plt.title(\"Train data\")\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(x_test[:, 0], x_test[:, 1], marker='o', c=y_test[:,0], s=20, edgecolor='k')\n",
    "plt.title(\"Test data\")\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x) + 1e-4)\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        self.theta0 = np.random.randn(input_size, hidden_size)/(input_size)\n",
    "        self.bias0 = np.zeros((1, hidden_size))\n",
    "        self.theta1 = np.random.randn(hidden_size, 1)/(hidden_size)\n",
    "        self.bias1 = np.zeros((1, 1))\n",
    "\n",
    "    def predict(self, x):\n",
    "        h1 = np.matmul(x , self.theta0)\n",
    "        h1_ = h1 + self.bias0\n",
    "        h2 = relu(h1_)\n",
    "        output = np.matmul(h2 , self.theta1) + self.bias1\n",
    "        pred = (output >= 0.5).astype(int)\n",
    "        return pred, (h1, h1_, h2, output)\n",
    "    \n",
    "    def compute_grad(self, x, y):\n",
    "        _, layers = self.predict(x)\n",
    "        h1, h1_, h2, output = layers\n",
    "\n",
    "        dl_dtheta1 = np.matmul(h2.T , 2 * (output - y))/y.shape[0]\n",
    "        dl_dbias1 = np.matmul(np.ones(output.shape).T, 2 * (output - y))/y.shape[0]\n",
    "\n",
    "        dl_dh2 = np.matmul(2 * (output - y), self.theta1.T)\n",
    "        \n",
    "        # If using tanh\n",
    "#         dl_dh1 = dl_dh2 * (1 - (tanh(h1) ** 2))\n",
    "\n",
    "        # If using sigmoid\n",
    "#         dl_dh1 = dl_dh2 * (sigmoid(h1)*(1 - sigmoid(h1)))\n",
    "\n",
    "        # If using relu\n",
    "        dl_dh1_ = dl_dh2 * (h1_ > 0)\n",
    "\n",
    "        dl_dtheta0 = np.matmul(x.T , dl_dh1_)/y.shape[0]\n",
    "        dl_dbias0 = np.matmul(np.ones(output.shape).T , dl_dh1_)/y.shape[0]\n",
    "\n",
    "        return dl_dtheta0, dl_dbias0, dl_dtheta1, dl_dbias1\n",
    "    \n",
    "    def update_params(self, x, y, lr):\n",
    "        dl_dtheta0, dl_dbias0, dl_dtheta1, dl_dbias1 = self.compute_grad(x, y)\n",
    "        self.theta0 -= lr * dl_dtheta0 \n",
    "        self.bias0 -= lr * dl_dbias0 \n",
    "                              \n",
    "        self.theta1 -= lr * dl_dtheta1\n",
    "        self.bias1 -= lr * dl_dbias1\n",
    "                              \n",
    "    def compute_loss(self, x, y):\n",
    "        _, layers = self.predict(x)\n",
    "        _, _, _, output = layers\n",
    "        return np.mean((output - y)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(input_size=2, output_size=1, hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = []  # keep track of the loss values\n",
    "acc = []  # keep track of the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "\n",
    "# number of times we itterate over the dataset\n",
    "max_epoch = 10000\n",
    "\n",
    "for epoch in trange(max_epoch):\n",
    "    y_test_hat, _ = mlp.predict(x_test)\n",
    "\n",
    "    acc.append(float(sum(y_test_hat == y_test))/ float(len(y_test)))\n",
    "\n",
    "    # call the compute_grad_loss that is implemented above to \n",
    "    # measure the loss and the gradient\n",
    "    loss = mlp.compute_loss(x_train, y_train)\n",
    "    mlp.update_params(x_train, y_train, lr)\n",
    "\n",
    "    loss_log.append(loss)\n",
    "\n",
    "print(\"Test Accuracy of linear model(GD): %.2f%% \" %(acc[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(acc)\n",
    "_ = plt.title(\"Model accuracy per itteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(loss_log)\n",
    "_ = plt.title(\"Model loss per itteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat, _ = mlp.predict(x_test)\n",
    "_ = plt.scatter(x_test[:, 0], x_test[:, 1], marker='o', c=y_test_hat[:,0], s=25, edgecolor='k')\n",
    "_ = plt.title(\"Model Test Prediction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
