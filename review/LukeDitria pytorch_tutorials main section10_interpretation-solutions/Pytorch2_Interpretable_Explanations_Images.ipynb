{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME for images\n",
    "\n",
    "For images, it is not a good idea to perturb individual pixels to understand the behaviour of our model. This is because many more than one pixel contribute to one class. Randomly changing individual pixels would probably not change the predictions by much. Therefore, variations of the images are created by **segmenting** the image into **“superpixels”** and turning superpixels off or on. Let us take a detour and briefly discuss superpixels.   \n",
    "\n",
    "<img src=\"../data/super_pixels.jpg\" alt=\"super_pixel\" style=\"float: left; margin-right: 10px;\" align=\"right\" width=\"200\"/>\n",
    "\n",
    "Superpixel algorithms group pixels into perceptually meaningful regions while respecting potential object contours, and thereby can replace the rigid pixel grid structure. More formally,\n",
    "\n",
    "**Superpixel.** Superpixels are an **oversegmentation** of an image. A superpixel is a perceptual grouping of pixels. Instead of finding segments that correspond to objects (as done in instance segmentation), superpixel segmentation algorithms split the image into typically a few hundered (eg., 2500) segments. The objective of this oversegmentation is to partition the image such that **1)** no superpixel is split by an object boundary, **2)** while objects may be divided into multiple superpixels.\n",
    "\n",
    "\n",
    "\n",
    "The LIME algorithm for images uses super-pixels as image features to interpret a black-box model. Let's see how.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up \n",
    "\n",
    "As always, we start by loading relevant packages. From the LIME package, we import lime_image. We also make use of the skimage package for visualization. See the instructions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# reading lime_image\n",
    "from lime import lime_image\n",
    "\n",
    "from skimage.segmentation import mark_boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility.\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0) \n",
    "\n",
    "device =\"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting ResNet model\n",
    "\n",
    "We focus on interpreting ResNet model. Below, we load the model. We also need the class descriptions for the ImageNet dataset which the ResNet is trained on. This information is available in a json file which again we read below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "resnet = resnet18(pretrained=True)\n",
    "resnet = resnet.eval().to(device)\n",
    "\n",
    "# reading imagenet classes\n",
    "idx2label, cls2label, cls2idx = [], {}, {}\n",
    "with open(os.path.join(\"../data\",\"imagenet_class_index.json\"), 'r') as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}\n",
    "    cls2idx = {class_idx[str(k)][0]: k for k in range(len(class_idx))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions.\n",
    "\n",
    "We need to develop a couple of utility function. Below, we implement a function to read an image and construct a torch tensor for the ResNet model. We recall that the ResNet model requires images to be normalized (RGB channels). \n",
    "\n",
    "We also need a function to convert an image to a torch tensor withoiut normalizing. The latter is used with the LIME package.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize and take the center part of image to what our model expects\n",
    "def pil_to_torch(img):\n",
    "    transf = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]) \n",
    "    ])        \n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transf(img).unsqueeze(0)\n",
    "\n",
    "def pil_transform(img): \n",
    "    transf = T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.CenterCrop(224)\n",
    "    ])    \n",
    "\n",
    "    return transf(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function for LIME\n",
    "\n",
    "We need to write a utility function to work with LIME. LIME will provide us with images where superpixels will turn-on and off and requires predictions in the form of class-probabilities to train its local model. hence, we need to write a function that gets images in the form of numpy arrays and make predictiosn using our ResNet model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_predict(images): \n",
    "    transf = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    batch = torch.stack(tuple(transf(img) for img in images), dim=0)\n",
    "\n",
    "    logits = resnet(batch.to(device)).cpu()\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read an image\n",
    "\n",
    "Below, we read the \"puppy_kitten.jpg\" from the data folder and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file_name = \"puppy_kitten.jpg\"\n",
    "img_pil_0 = Image.open(os.path.join(\"../data\",img_file_name)).convert('RGB')\n",
    "\n",
    "_ = plt.imshow(img_pil_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet predicts as?\n",
    "\n",
    "Let's see how the ResNet will recognize the image, is it a cat or a dog?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = pil_to_torch(img_pil_0)\n",
    "\n",
    "logits = resnet(img0.to(device))\n",
    "probs = F.softmax(logits, dim=1).cpu()\n",
    "probs5 = probs.topk(5)\n",
    "tuple((p,c, idx2label[c]) for p, c in zip(probs5[0][0].detach().numpy(), probs5[1][0].detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LIME to explain the ResNet\n",
    "\n",
    "To use LIME, we need to first define an explainer object. Check the cell below for this purpose.\n",
    "Then, we can use the method [explain_instance](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_image) from the explainer object to understand the behaviour of Resnet. Run the \"explain_instance\" and discuss the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(np.array(pil_transform(img_pil_0)), \n",
    "                                         cnn_predict, # classification function\n",
    "                                         top_labels=2, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) # number of images that will be sent to classification function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running LIME on our image, you can use various methods to visualize which parts of the image contributed to the decisions. Check the method [get_image_and_mask](https://lime-ml.readthedocs.io/en/latest/lime.html#lime.lime_image.ImageExplanation.get_image_and_mask) which can be used to visulize superpixels that **positively** or **negatively** contribute to the prediction of the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(20, 10))\n",
    "\n",
    "axes[0].imshow(pil_transform(img_pil_0))\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(\"Original Image\")\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                            positive_only=True, negative_only=False, \n",
    "                                            num_features=5, hide_rest=True)\n",
    "\n",
    "img_boundry = mark_boundaries(temp/255.0, mask)\n",
    "axes[1].imshow(img_boundry)\n",
    "axes[1].set_title(\"Positive mask\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], \n",
    "                                            positive_only=False, negative_only=True, \n",
    "                                            num_features=5, hide_rest=True)\n",
    "\n",
    "img_boundry = mark_boundaries(temp/255.0, mask)\n",
    "axes[2].imshow(img_boundry)\n",
    "axes[2].set_title(\"Negative mask\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try another image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file_name = \"fruit.jpg\"\n",
    "img_pil_1 = Image.open(os.path.join(\"../data\",img_file_name)).convert('RGB')\n",
    "\n",
    "img1 = pil_to_torch(img_pil_1)\n",
    "logits = resnet(img1.to(device)).cpu()\n",
    "probs = F.softmax(logits, dim=1)\n",
    "probs5 = probs.topk(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(img_pil_1)\n",
    "labels = tuple((p,c, idx2label[c]) for p, c in zip(probs5[0][0].detach().numpy(), probs5[1][0].detach().numpy()))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(np.array(pil_transform(img_pil_1)), \n",
    "                                         cnn_predict, # classification function\n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) # number of images that will be sent to classification function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 3\n",
    "print(\"Looking at:\", labels[label][-1])\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(20, 10))\n",
    "axes[0].imshow(pil_transform(img_pil_1))\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(\"Original Image\")\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[label], positive_only=True, \n",
    "                                            negative_only=False, num_features=3, hide_rest=True)\n",
    "img_boundry = mark_boundaries(temp/255.0, mask)\n",
    "axes[1].imshow(img_boundry)\n",
    "axes[1].set_title(\"Positive mask\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[label], positive_only=False, \n",
    "                                            negative_only=True, num_features=10, hide_rest=True)\n",
    "\n",
    "img_boundry = mark_boundaries(temp/255.0, mask)\n",
    "axes[2].imshow(img_boundry)\n",
    "axes[2].set_title(\"Negative mask\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
