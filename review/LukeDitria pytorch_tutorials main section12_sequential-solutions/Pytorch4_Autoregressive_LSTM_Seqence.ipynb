{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79701287",
   "metadata": {},
   "source": [
    "# LSTM, One day at a time!\n",
    "In this Notebook we'll see how the Pytorch LSTM block allows us to pass it the whole data sequence at once. The LSTM block will very quickly process the sequence for us, without having to use a slow Python for loop. Instead of providing a sequence of data ast each time-step (like previous notebooks) we'll simply provide the LSTM a single days worth of data at a time.\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/2880px-LSTM_Cell.svg.png\">](LSTM)\n",
    "<br>\n",
    "[Corresponding Tutorial Video](https://youtu.be/lyUT6dOARGs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from Dataset import WeatherDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f34f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing the weather dataset\n",
    "dataset_file = \"../data/weather.csv\"\n",
    "\n",
    "# Define the date to split the dataset into training and testing sets\n",
    "split_date = pd.to_datetime('2023-01-01')\n",
    "\n",
    "# Number of days in the input sequence\n",
    "day_range = 30\n",
    "\n",
    "# Number of days the MLP will take as input\n",
    "days_in = 14\n",
    "\n",
    "# Ensure that the total number of days in the input sequence is larger than the MLP input size\n",
    "assert day_range > days_in, \"The total day range must be larger than the input days for the MLP\"\n",
    "\n",
    "# Define the hyperparameters for training the model\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "nepochs = 500  # Number of training epochs\n",
    "batch_size = 32  # Batch size for training\n",
    "\n",
    "# Create training dataset\n",
    "# This will load the weather data, consider sequences of length day_range,\n",
    "# and split the data such that data before split_date is used for training\n",
    "dataset_train = WeatherDataset(dataset_file, day_range=day_range, split_date=split_date, train_test=\"train\")\n",
    "\n",
    "# Create testing dataset\n",
    "# This will load the weather data, consider sequences of length day_range,\n",
    "# and split the data such that data after split_date is used for testing\n",
    "dataset_test = WeatherDataset(dataset_file, day_range=day_range, split_date=split_date, train_test=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d54e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(dataset_train)}')\n",
    "print(f'Number of testing examples: {len(dataset_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f04895",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2efd3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "_ = plt.title(\"Melbourne Max Daily Temperature (C)\")\n",
    "\n",
    "_ = plt.plot(dataset_train.dataset.index, dataset_train.dataset.values[:, 1])\n",
    "_ = plt.plot(dataset_test.dataset.index, dataset_test.dataset.values[:, 1])\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "# Note:see here how we can just directly access the data from the dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb9e27",
   "metadata": {},
   "source": [
    "## Create LSTM Model\n",
    "In this Notebook, we'll explore how to leverage PyTorch's LSTM block to process entire data sequences efficiently. Instead of providing data at each time-step, as done in previous notebooks, we'll feed the LSTM a single day's data at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5774edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our network class using nn.Module\n",
    "class ResBlockMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ResBlockMLP, self).__init__()\n",
    "        # Define layers for the MLP block\n",
    "        self.norm1 = nn.LayerNorm(input_size)\n",
    "        self.fc1 = nn.Linear(input_size, input_size//2)\n",
    "        self.norm2 = nn.LayerNorm(input_size//2)\n",
    "        self.fc2 = nn.Linear(input_size//2, output_size)\n",
    "        self.fc3 = nn.Linear(input_size, output_size)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the MLP block\n",
    "        x = self.act(self.norm1(x))\n",
    "        skip = self.fc3(x)  # Skip connection\n",
    "        x = self.act(self.norm2(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x + skip\n",
    "\n",
    "# Define the LSTM-based network\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, seq_len, output_size, num_blocks=1, hidden_size=128):\n",
    "        super(LSTM, self).__init__()\n",
    "        # Define layers for input MLP, LSTM, residual blocks, and output linear layer\n",
    "        self.input_mlp = nn.Sequential(nn.Linear(seq_len, 4 * seq_len),\n",
    "                                       nn.ELU(),\n",
    "                                       nn.Linear(4 * seq_len, hidden_size))\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        blocks = [ResBlockMLP(hidden_size, hidden_size) for _ in range(num_blocks)]\n",
    "        self.res_blocks = nn.Sequential(*blocks)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, input_seq, hidden_in, mem_in):\n",
    "        # Pass input sequence through the input MLP\n",
    "        input_vec = self.input_mlp(input_seq)\n",
    "        \n",
    "        # Pass the input MLP output through the LSTM block\n",
    "        output, (hidden_out, mem_out) = self.lstm(input_vec, (hidden_in, mem_in))\n",
    "        \n",
    "        # Pass the LSTM output through residual blocks\n",
    "        x = self.act(self.res_blocks(output))\n",
    "        \n",
    "        # Pass the output of the residual blocks through the final linear layer\n",
    "        return self.fc_out(x), hidden_out, mem_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "hidden_size = 128\n",
    "\n",
    "# Create the LSTM model\n",
    "weather_lstm = LSTM(seq_len=2, output_size=2, hidden_size=hidden_size).to(device)\n",
    "\n",
    "# Initialize the optimizer with Adam and specified learning rate\n",
    "optimizer = optim.Adam(weather_lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the loss function as Mean Squared Error\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Initialize a list to log training losses\n",
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in weather_rnn.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295224c3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training loop for each epoch\n",
    "for epoch in trange(nepochs, desc=\"Epochs\", leave=False):\n",
    "    # Set the model to training mode\n",
    "    weather_lstm.train()\n",
    "    \n",
    "    # Iterate through the training data loader\n",
    "    for day, month, data_seq in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        \n",
    "        # Pass the whole sequence of data at once\n",
    "        seq_block = data_seq[:, :-1].to(device)\n",
    "        target_seq_block = data_seq[:, 1:].to(device)\n",
    "        \n",
    "        # Initialize hidden state and memory\n",
    "        hidden = torch.zeros(1, data_seq.shape[0], hidden_size, device=device)\n",
    "        memory = torch.zeros(1, data_seq.shape[0], hidden_size, device=device)\n",
    "\n",
    "        # Pass the input sequence through the LSTM\n",
    "        data_pred, hidden, memory = weather_lstm(seq_block, hidden, memory)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(data_pred, target_seq_block)\n",
    "        \n",
    "        # Perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training loss\n",
    "        training_loss_logger.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdfa1c2",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008871ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test dataset to tensor\n",
    "data_tensor = torch.FloatTensor(dataset_test.dataset.values)\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "log_predictions = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "weather_lstm.eval()\n",
    "\n",
    "# Specify the length of the input sequence\n",
    "len_input = 30\n",
    "\n",
    "# Run prediction loop with no gradients\n",
    "with torch.no_grad():\n",
    "    # Initialize the input sequence with the first len_input data points\n",
    "    seq_block = data_tensor[:len_input].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize hidden state and memory\n",
    "    hidden = torch.zeros(1, seq_block.shape[0], hidden_size, device=device)\n",
    "    memory = torch.zeros(1, seq_block.shape[0], hidden_size, device=device)\n",
    "    \n",
    "    # Iterate over the remaining data points\n",
    "    for i in range(data_tensor.shape[0] - len_input):\n",
    "        # Predict the next data point\n",
    "        data_pred, hidden, memory = weather_lstm(seq_block, hidden, memory)\n",
    "    \n",
    "        # Update the input sequence with the predicted data point\n",
    "        seq_block = data_pred[:, -1:, :]\n",
    "        \n",
    "        # Append the prediction to the list\n",
    "        log_predictions.append(data_pred[:, -1, :].cpu())\n",
    "        \n",
    "# Concatenate all predictions\n",
    "predictions_cat = torch.cat(log_predictions)\n",
    "\n",
    "# Denormalize the predictions and original data\n",
    "un_norm_predictions = (predictions_cat * dataset_test.std) + dataset_test.mean\n",
    "un_norm_data = (data_tensor * dataset_test.std) + dataset_test.mean\n",
    "un_norm_data = un_norm_data[len_input:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = (un_norm_data - un_norm_predictions).pow(2).mean().item()\n",
    "print(\"Test MSE value %.2f\" % test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8579cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(un_norm_data[:, 0])\n",
    "_ = plt.plot(un_norm_predictions[:, 0])\n",
    "_ = plt.title(\"Rainfall (mm)\")\n",
    "\n",
    "_ = plt.legend([\"Ground Truth\", \"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(un_norm_data[:, 1])\n",
    "_ = plt.plot(un_norm_predictions[:, 1])\n",
    "_ = plt.title(\"Max Daily Temperature (C)\")\n",
    "\n",
    "_ = plt.legend([\"Ground Truth\", \"Prediction\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
