{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b520db76",
   "metadata": {},
   "source": [
    "# Predicting Sequential Data With an RNN\n",
    "In this notebook we'll introduce the concept of a [Recurrent Neural Network](https://youtu.be/AsNTP8Kwu80?si=SwLrsvavCfLhgv3V). By allowing the model to pass additional information to itself between time-steps it will be able to pass more information than just previous predictions.\n",
    "\n",
    "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/2880px-Recurrent_neural_network_unfold.svg.png\">](RNN)\n",
    "<br>\n",
    "\n",
    "[Corresponding Tutorial Video](https://youtu.be/YUGbMdfgpx0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from Dataset import WeatherDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfbbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing the weather dataset\n",
    "dataset_file = \"../data/weather.csv\"\n",
    "\n",
    "# Define the date to split the dataset into training and testing sets\n",
    "split_date = pd.to_datetime('2023-01-01')\n",
    "\n",
    "# Number of days in the input sequence\n",
    "day_range = 15\n",
    "\n",
    "# Number of days the MLP will take as input\n",
    "days_in = 14\n",
    "\n",
    "# Ensure that the total number of days in the input sequence is larger than the MLP input size\n",
    "assert day_range > days_in, \"The total day range must be larger than the input days for the MLP\"\n",
    "\n",
    "# Define the hyperparameters for training the model\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "nepochs = 500  # Number of training epochs\n",
    "batch_size = 32  # Batch size for training\n",
    "\n",
    "# Create training dataset\n",
    "# This will load the weather data, consider sequences of length day_range,\n",
    "# and split the data such that data before split_date is used for training\n",
    "dataset_train = WeatherDataset(dataset_file, day_range=day_range, split_date=split_date, train_test=\"train\")\n",
    "\n",
    "# Create testing dataset\n",
    "# This will load the weather data, consider sequences of length day_range,\n",
    "# and split the data such that data after split_date is used for testing\n",
    "dataset_test = WeatherDataset(dataset_file, day_range=day_range, split_date=split_date, train_test=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(dataset_train)}')\n",
    "print(f'Number of testing examples: {len(dataset_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "_ = plt.title(\"Melbourne Max Daily Temperature (C)\")\n",
    "\n",
    "_ = plt.plot(dataset_train.dataset.index, dataset_train.dataset.values[:, 1])\n",
    "_ = plt.plot(dataset_test.dataset.index, dataset_test.dataset.values[:, 1])\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "# Note:see here how we can just directly access the data from the dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd60fd",
   "metadata": {},
   "source": [
    "## Create RNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc20d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ResBlockMLP, self).__init__()\n",
    "        # Layer normalization for the input\n",
    "        self.norm1 = nn.LayerNorm(input_size)\n",
    "        # First fully connected layer that reduces the dimensionality by half\n",
    "        self.fc1 = nn.Linear(input_size, input_size // 2)\n",
    "        \n",
    "        # Layer normalization after the first fully connected layer\n",
    "        self.norm2 = nn.LayerNorm(input_size // 2)\n",
    "        # Second fully connected layer that outputs the desired output size\n",
    "        self.fc2 = nn.Linear(input_size // 2, output_size)\n",
    "        \n",
    "        # Skip connection layer to match the output size\n",
    "        self.fc3 = nn.Linear(input_size, output_size)\n",
    "\n",
    "        # Activation function\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply normalization and activation function to the input\n",
    "        x = self.act(self.norm1(x))\n",
    "        # Compute the skip connection output\n",
    "        skip = self.fc3(x)\n",
    "        \n",
    "        # Apply the first fully connected layer, normalization, and activation function\n",
    "        x = self.act(self.norm2(self.fc1(x)))\n",
    "        # Apply the second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Add the skip connection to the output\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, seq_len, output_size, num_blocks=1, buffer_size=128):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Compute the length of the sequence data\n",
    "        seq_data_len = seq_len * 2\n",
    "\n",
    "        # Define the input MLP with two fully connected layers and activation functions\n",
    "        self.input_mlp = nn.Sequential(\n",
    "            nn.Linear(seq_data_len, 4 * seq_data_len),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(4 * seq_data_len, 128),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        # Define the RNN layer\n",
    "        # Concatenates the input sequence with the buffer and feeds it through a linear layer\n",
    "        self.rnn = nn.Linear(256, 128)\n",
    "        \n",
    "        # Define the sequence of residual blocks\n",
    "        blocks = [ResBlockMLP(128, 128) for _ in range(num_blocks)]\n",
    "        self.res_blocks = nn.Sequential(*blocks)\n",
    "        \n",
    "        # Final output fully connected layer\n",
    "        self.fc_out = nn.Linear(128, output_size)\n",
    "        \n",
    "        # Fully connected layer for the buffer\n",
    "        self.fc_buffer = nn.Linear(128, buffer_size)\n",
    "        \n",
    "        # Activation function\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, input_seq, buffer_in):\n",
    "        # Reshape the input sequence to a flat vector\n",
    "        input_seq = input_seq.reshape(input_seq.shape[0], -1)\n",
    "        # Pass the input sequence through the input MLP\n",
    "        input_vec = self.input_mlp(input_seq)\n",
    "        \n",
    "        # Concatenate the previous step buffer with the input vector\n",
    "        x_cat = torch.cat((buffer_in, input_vec), 1)\n",
    "        \n",
    "        # Pass the concatenated vector through the RNN layer\n",
    "        x = self.rnn(x_cat)\n",
    "\n",
    "        # Pass the output through the sequence of residual blocks\n",
    "        x = self.act(self.res_blocks(x))\n",
    "        \n",
    "        # Compute the final output and the updated buffer\n",
    "        return self.fc_out(x), torch.tanh(self.fc_buffer(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cccc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define buffer size\n",
    "buffer_size = 128\n",
    "\n",
    "# Create RNN model and move it to the specified device\n",
    "weather_rnn = RNN(seq_len=days_in, output_size=2, buffer_size=buffer_size).to(device)\n",
    "\n",
    "# Initialize Adam optimizer with specified learning rate\n",
    "optimizer = optim.Adam(weather_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define Mean Squared Error (MSE) loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Initialize training loss logger\n",
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af9ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in weather_rnn.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over epochs\n",
    "for epoch in trange(nepochs, desc=\"Epochs\", leave=False):\n",
    "    # Set the model to training mode\n",
    "    weather_rnn.train()\n",
    "    \n",
    "    # Iterate over batches in training data\n",
    "    for day, month, data_seq in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        # Extract input sequence block and move to device\n",
    "        seq_block = data_seq[:, :days_in].to(device)\n",
    "        \n",
    "        # Initialize the buffer with zeros\n",
    "        buffer = torch.zeros(data_seq.shape[0], buffer_size, device=device)\n",
    "        \n",
    "        # Initialize loss\n",
    "        loss = 0\n",
    "        \n",
    "        # Iterate over time steps\n",
    "        for i in range(day_range - days_in):\n",
    "            # Extract target sequence block and move to device\n",
    "            target_seq_block = data_seq[:, i + days_in].to(device)\n",
    "            \n",
    "            # Forward pass: Get predictions and update buffer\n",
    "            data_pred, buffer = weather_rnn(seq_block, buffer)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss += loss_fn(data_pred, target_seq_block)\n",
    "\n",
    "            # Update input sequence block by removing the oldest prediction and adding the new prediction\n",
    "            seq_block = torch.cat((seq_block[:, 1:, :], data_pred.unsqueeze(1).detach()), 1)\n",
    "            \n",
    "        # Calculate average loss\n",
    "        loss /= i + 1\n",
    "        \n",
    "        # Backpropagation: Zero gradients, compute gradients, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training loss\n",
    "        training_loss_logger.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1482d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7549cf",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb77236",
   "metadata": {},
   "source": [
    "### Predicting with Feedback\n",
    "Lets perform an autoregressive prediction roll-out similar to the training loop, but on the test dataset. Finally, it concatenates the predictions, unnormalizes them along with the original data, and trims the initial sequence to match the length of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe62ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test dataset values to a PyTorch tensor\n",
    "data_tensor = torch.FloatTensor(dataset_test.dataset.values)\n",
    "\n",
    "# Initialize an empty list to store predictions\n",
    "log_predictions = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "weather_rnn.eval()\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    # Initialize the input sequence block and buffer\n",
    "    seq_block = data_tensor[:days_in, :].unsqueeze(0).to(device)\n",
    "    buffer = torch.zeros(seq_block.shape[0], buffer_size, device=device)\n",
    "    \n",
    "    # Iterate over the test data, performing autoregressive prediction\n",
    "    for i in range(data_tensor.shape[0] - days_in):\n",
    "        # Get predictions and update buffer\n",
    "        data_pred, buffer = weather_rnn(seq_block, buffer)\n",
    "        # Append predictions to the list\n",
    "        log_predictions.append(data_pred.cpu())\n",
    "\n",
    "        # Update input sequence block by removing the oldest prediction and adding the new prediction\n",
    "        seq_block = torch.cat((seq_block[:, 1:, :], data_pred.unsqueeze(1)), 1)\n",
    "\n",
    "# Concatenate the logged predictions into a single tensor\n",
    "predictions_cat = torch.cat(log_predictions)\n",
    "\n",
    "# Unnormalize the predictions and original data using dataset's standard deviation and mean\n",
    "un_norm_predictions = (predictions_cat * dataset_test.std) + dataset_test.mean\n",
    "un_norm_data = (data_tensor * dataset_test.std) + dataset_test.mean\n",
    "\n",
    "# Trim the initial sequence from the unnormalized data to match the length of predictions\n",
    "un_norm_data = un_norm_data[days_in:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376761f",
   "metadata": {},
   "source": [
    "### Using Ground Truth First\n",
    "This process involves predicting the first 30 time steps without using the model's predictions and then using the model's predictions for the subsequent time steps. This way the model gets more of the original first before predicting from its only predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233cd6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to use!\n",
    "# # Convert test dataset values to a PyTorch tensor\n",
    "# data_tensor = torch.FloatTensor(dataset_test.dataset.values)\n",
    "\n",
    "# # Initialize an empty list to store predictions\n",
    "# log_predictions = []\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# weather_rnn.eval()\n",
    "\n",
    "# # Initialize buffer with zeros\n",
    "# buffer = torch.zeros(1, buffer_size, device=device)\n",
    "\n",
    "# # Iterate over the first 30 time steps\n",
    "# for i in range(30):\n",
    "#     # Extract input sequence block for prediction\n",
    "#     seq_block = data_tensor[i:(i + days_in), :].unsqueeze(0).to(device)\n",
    "    \n",
    "#     # Predict using the input sequence block and buffer\n",
    "#     data_pred, buffer = weather_rnn(seq_block, buffer)\n",
    "    \n",
    "#     # Append prediction to the list\n",
    "#     log_predictions.append(data_pred.cpu())\n",
    "\n",
    "# # Continue predicting for the remaining time steps\n",
    "# for j in range(data_tensor.shape[0] - days_in - 30):\n",
    "#     # Update input sequence block by removing the oldest prediction and adding the new prediction\n",
    "#     seq_block = torch.cat((seq_block[:, 1:, :], data_pred.unsqueeze(1)), 1)\n",
    "    \n",
    "#     # Predict using the updated input sequence block and buffer\n",
    "#     data_pred, buffer = weather_rnn(seq_block, buffer)\n",
    "    \n",
    "#     # Append prediction to the list\n",
    "#     log_predictions.append(data_pred.cpu())\n",
    "\n",
    "# # Concatenate the logged predictions into a single tensor\n",
    "# predictions_cat = torch.cat(log_predictions)\n",
    "\n",
    "# # Unnormalize the predictions and original data using dataset's standard deviation and mean\n",
    "# un_norm_predictions = (predictions_cat * dataset_test.std) + dataset_test.mean\n",
    "# un_norm_data = (data_tensor * dataset_test.std) + dataset_test.mean\n",
    "# un_norm_data = un_norm_data[days_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = (un_norm_data - un_norm_predictions).pow(2).mean().item()\n",
    "print(\"Test MSE value %.2f\" % test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c38e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(un_norm_data[:, 0])\n",
    "_ = plt.plot(un_norm_predictions[:, 0])\n",
    "_ = plt.title(\"Rainfall (mm)\")\n",
    "\n",
    "_ = plt.legend([\"Ground Truth\", \"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 10))\n",
    "_ = plt.plot(un_norm_data[:, 1])\n",
    "_ = plt.plot(un_norm_predictions[:, 1])\n",
    "_ = plt.title(\"Max Daily Temperature (C)\")\n",
    "\n",
    "_ = plt.legend([\"Ground Truth\", \"Prediction\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
