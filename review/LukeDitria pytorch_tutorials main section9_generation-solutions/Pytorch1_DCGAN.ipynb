{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generative Adversarial Network</h1>\n",
    "In this notebook we will explore a popular method for data generation, the Generative Adversarial Network (GAN). GANs are in fact a pair of networks (at least two) a Generator and a Discriminator. The Generator takes a random sample from a distribution (usually a standard normal distribution) and produces an image. The Discriminator takes an image and tries to classify it as either coming from the Generator or the dataset of real images. The twist is that we optimize the Generator to produce an image that the Discriminator classifies as \"real\" (coming from the dataset). <br>\n",
    "\n",
    "<img src=\"../data/MNIST_GAN_DEMO.gif\" width=\"700\" align=\"center\">\n",
    "\n",
    "We do this by generating an image and passing it through the Discriminator and then calculating the loss (as if the fake image was real) and the back propagating gradients back through the discriminator to the Generator and then updating only the generator. By doing this the Generator \"sees\" how to update itself in such a way so that the Discriminator will classify it's images as \"real\". As this is happening at the same times as the Discriminator is learning to distinguish the Generators \"fake\" images from the datasets \"real\" ones we get a \"bootstrapping\" effect. As a result the Generator gets better at generating \"fake\" images and the discriminator gets better at detecting fake from real, until (ideally) the fake images are indistinguishable from real.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dg0l8uzpFLUp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "batch_size = 512\n",
    "dlr = 1e-4\n",
    "glr = 1e-4\n",
    "\n",
    "train_epoch = 100\n",
    "\n",
    "# data_loader\n",
    "img_size = 32\n",
    "\n",
    "data_set_root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Use a GPU if avaliable </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwzLOOlx6KxE"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "gpu_indx  = 0\n",
    "device = torch.device(gpu_indx if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(img_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=([0.5]), std=([0.5]))\n",
    "                                ])\n",
    "\n",
    "trainset = datasets.MNIST(data_set_root, train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Adversarial Training</h2>\n",
    "<img src=\"../data/GAN.jpg\" width=\"700\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "un8vZ4VHFLUt"
   },
   "source": [
    "<h3> Generator </h3>\n",
    "Our Generator is a simple transpose convolution network, it takes in a vector as an input and up samples it multiple times until it is the desired size.<br>\n",
    "NOTE: We use a 3D tensor for the input instead of a 1D vector as it is easier to just use transpose convolution layers for everything instead of having to reshape the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xP4N0Mr8FLUv"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z=64, ch=16):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(z, ch * 4, 4, 1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(ch * 4, ch * 4, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch * 4)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(ch * 4, ch * 2, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(ch * 2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(ch * 2, ch * 2, 3, 1, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(ch * 2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(ch * 2, ch * 2, 3, 1, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(ch * 2)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(ch * 2, 1, 3, 1, 1)\n",
    "        self.up_nn = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "    # Forward method\n",
    "    def forward(self, x):        \n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.up_nn(F.elu(self.bn1(self.conv2(x))))\n",
    "        x = self.up_nn(F.elu(self.bn2(self.conv3(x))))\n",
    "        x = self.up_nn(F.elu(self.bn3(self.conv4(x))))\n",
    "        x = F.elu(self.bn4(self.conv5(x)))\n",
    "\n",
    "        return torch.tanh(self.conv_out(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUfb2C69FLUy"
   },
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6PJrpmMFLUz"
   },
   "source": [
    "The Discriminator is simple convolutional classifier network that has a single output.<br>\n",
    "NOTE:The output is also a 3D tensor for a single output, we will squish it before calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QHFNhkcUFLU0"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, ch=16):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_in = nn.Conv2d(1, ch, 3, 1, 1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(ch, ch*2, 3, 2, 1)        \n",
    "        self.conv2 = nn.Conv2d(ch*2, ch*2, 3, 2, 1)        \n",
    "        self.conv3 = nn.Conv2d(ch*2, ch*4, 3, 2, 1)        \n",
    "        self.conv4 = nn.Conv2d(ch*4, ch*4, 3, 2, 1)\n",
    "        self.bn = nn.BatchNorm2d(ch*4)\n",
    "        \n",
    "        self.do = nn.Dropout()\n",
    "        self.conv5 = nn.Conv2d(ch*4, 1, 2, 1)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.conv_in(x))\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.do(F.elu(self.bn(self.conv4(x))))\n",
    "        x = self.conv5(x).reshape(x.shape[0], 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_bce_loss(output, real_label=True):\n",
    "    if real_label:\n",
    "        return F.binary_cross_entropy_with_logits(output, torch.ones_like(output))\n",
    "    else:     \n",
    "        return F.binary_cross_entropy_with_logits(output, torch.zeros_like(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_w_loss(output, real_label=True):\n",
    "    if real_label:\n",
    "        return -output.mean()\n",
    "    else:     \n",
    "        return output.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Lmq5ohV6Kw1"
   },
   "source": [
    "<h2> Basic GAN Model</h2>\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1600/1*M_YipQF_oC6owsU1VVrfhg.jpeg)\n",
    "\n",
    "A more formal way of describing what is going on is to characterise our dataset as a discrete sample from some high dimensional distribution $d_r$ (where every pixel value is a degree of freedom). Our Generator maps from some input distribution $N(0,1)$ to $d_f$ and the Discriminator tries to learn to distinguish $d_r$ from $d_f$ while the Generator tries to make $d_f = d_r$ at which point the Discriminator cannot tell the difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define Hyperparameters, Network and Optimizer</h3>\n",
    "GANs are very instable and sensitive to hyperparameters, see how well you can get the generated output by tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9820,
     "status": "ok",
     "timestamp": 1570750148112,
     "user": {
      "displayName": "Yunyan Xing",
      "photoUrl": "",
      "userId": "15587527606127278468"
     },
     "user_tz": -660
    },
    "id": "FL0LtpmwFLVA",
    "outputId": "9f67f2bd-db73-4b7d-be3e-365776d09ede"
   },
   "outputs": [],
   "source": [
    "# network\n",
    "latent_noise_dem = 128\n",
    "\n",
    "g_net = Generator(latent_noise_dem, ch=32).to(device)\n",
    "d_net = Discriminator(ch=32).to(device)\n",
    "\n",
    "#A fixed latent noise vector so we can see the improvement over the epochs\n",
    "fixed_latent_noise = torch.randn(16, latent_noise_dem, 1, 1).to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "g_optimizer = optim.Adam(g_net.parameters(), lr=glr)\n",
    "d_optimizer = optim.Adam(d_net.parameters(), lr=dlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using gradient clipping, initialise the params to be smaller\n",
    "# with torch.no_grad():\n",
    "#     for param in d_net.parameters():\n",
    "#         param.data *= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gbwhfk6AFLVD"
   },
   "outputs": [],
   "source": [
    "test_images_log = []\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "d_out_fake = []\n",
    "d_out_real = []\n",
    "\n",
    "g_loss = 0\n",
    "d_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JDtnJsLFLVG"
   },
   "source": [
    "# The main training loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall objective function is: ![alt text](https://miro.medium.com/max/1500/1*l9se1koH_eQdZesko5eQpw.jpeg)\n",
    "\n",
    "This is also formalised as a \"minmax\" game, where the Generator is trying to minimise the above loss function for a Discriminator that is trying to maximise it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPcdfmvsOYvV"
   },
   "source": [
    "The Loss function we are using is the binary cross entropy loss because what the Discriminator do is essentially a binary classification problem: Real or Fake. At the point where $d_f = d_r$ the output of our Discriminator should be 0.5 (halfway between 0 and 1 - note this rarely actually happens)<br>\n",
    "\n",
    "\n",
    "GANs can be difficult to Train, some common reasons are:\n",
    "1. At the beginning, the distribution of real and fake samples are far away from each other, it can be easy for the Discriminator to tell the fake from the real\n",
    "\n",
    "3. The Discriminator becomes too good and stops providing useful gradients back to the Generator\n",
    "\n",
    "2. The Discriminator becomes bad and cannot tell the difference between the real and fake samples even though the output from the generator is bad\n",
    "\n",
    "3. The Generator overpowers the Discriminator and the Discriminator cannot tell real from fake (even though $d_f  \\neq d_r$)\n",
    "\n",
    "4. The Generator starts outputting only a very few images (low variation in the images) and as the Discriminator is only looking for fake images it is not corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1021,
     "status": "error",
     "timestamp": 1570754566661,
     "user": {
      "displayName": "Yunyan Xing",
      "photoUrl": "",
      "userId": "15587527606127278468"
     },
     "user_tz": -660
    },
    "id": "IVBfpjOkFLVH",
    "outputId": "adfcd384-2571-4071-8c30-ad2f66a5f2f1"
   },
   "outputs": [],
   "source": [
    "pbar = trange(train_epoch, leave=False, desc=\"Epoch\")    \n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('G Loss: %.4f, D Loss: %.4f' % (g_loss/len(train_loader), \n",
    "                                                         d_loss/len(train_loader)))\n",
    "    g_loss = 0\n",
    "    d_loss = 0\n",
    "\n",
    "    for num_iter, (images, label) in enumerate(tqdm(train_loader, leave=False)):\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for param in d_net.parameters():\n",
    "#                 param.clamp_(-0.05, 0.05)\n",
    "\n",
    "        images = images.to(device)\n",
    "        \n",
    "        #the size of the current minibatch\n",
    "        bs = images.shape[0]\n",
    "\n",
    "        ########### Train Generator G ##############\n",
    "        #Step1: Sample a latent vector from a normal distribution and pass it through the generator\n",
    "        #to get a batch of fake images\n",
    "        latent_noise = torch.randn(bs, latent_noise_dem, 1, 1).to(device)\n",
    "        g_output = g_net(latent_noise)\n",
    "        \n",
    "        #Step3: Pass the minibatch of fake images (from the Generator) through the Discriminator and calculate\n",
    "        #the loss against the \"real\" label - the Generator wants the discriminator to think it's outputs are real\n",
    "        d_result = d_net(g_output)\n",
    "        g_train_loss = gan_bce_loss(d_result, True)\n",
    "        \n",
    "        #Step4: Backpropogate the loss through the discriminator and into the Generator and take a training step \n",
    "        g_net.zero_grad()\n",
    "        g_train_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        #log the generator training loss\n",
    "        g_losses.append(g_train_loss.item())\n",
    "        g_loss += g_train_loss.item()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for param in g_net.parameters():\n",
    "#                 param.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        ########### Train Discriminator D! ############\n",
    "        \n",
    "        #Step1: Pass the minibatch of real images through the Discriminator and calculate\n",
    "        #the loss against the \"real\" label\n",
    "        d_real_out = d_net(images)\n",
    "        d_real_loss = gan_bce_loss(d_real_out, True)\n",
    "        d_out_real.append(d_real_out.mean().item())\n",
    "        \n",
    "        #Step2: Pass the minibatch of fake images (from the Generator) through the Discriminator and calculate\n",
    "        #the loss against the \"fake\" label\n",
    "        #We \"detach()\" the output of the Generator here as we don't need it to backpropagate through the\n",
    "        #Generator in this step\n",
    "        d_fake_out = d_net(g_output.detach())\n",
    "        d_fake_loss = gan_bce_loss(d_fake_out, False)\n",
    "        d_out_fake.append(d_fake_out.mean().item())\n",
    "\n",
    "        #Step3: Add the two losses together, backpropogate through the discriminator and take a training step \n",
    "        d_train_loss = (d_real_loss + d_fake_loss)/2\n",
    "\n",
    "        d_net.zero_grad()\n",
    "        d_train_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        #log the discriminator training loss\n",
    "        d_losses.append(d_train_loss.item())\n",
    "        d_loss += d_train_loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        g_net.eval()\n",
    "        #log the output of the generator given the fixed latent noise vector\n",
    "        test_fake = g_net(fixed_latent_noise)\n",
    "        imgs = torchvision.utils.make_grid(test_fake.cpu().detach(), 4, pad_value=1, normalize=True)\n",
    "        imgs_np = (imgs.numpy().transpose((1, 2, 0)) * 255).astype(np.uint8)\n",
    "        test_images_log.append(imgs_np)\n",
    "        g_net.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot out the losses and visualize the generated images</h3>\n",
    "Notice how noisy the losses for both the Generator and Discriminator are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1570754571230,
     "user": {
      "displayName": "Yunyan Xing",
      "photoUrl": "",
      "userId": "15587527606127278468"
     },
     "user_tz": -660
    },
    "id": "a753TY7Z6KxV",
    "outputId": "6b7ebf6d-1354-4776-fe13-9b6f7e18eeae"
   },
   "outputs": [],
   "source": [
    "plt.plot(d_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1570754571959,
     "user": {
      "displayName": "Yunyan Xing",
      "photoUrl": "",
      "userId": "15587527606127278468"
     },
     "user_tz": -660
    },
    "id": "IVaSDHhc6KxY",
    "outputId": "0de0ebec-2674-4a64-b290-4f52aceef8d4"
   },
   "outputs": [],
   "source": [
    "plt.plot(g_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1197,
     "status": "ok",
     "timestamp": 1570754573085,
     "user": {
      "displayName": "Yunyan Xing",
      "photoUrl": "",
      "userId": "15587527606127278468"
     },
     "user_tz": -660
    },
    "id": "VdN0meY8FLVK",
    "outputId": "1c97838e-e848-4016-863d-b14d9ddc2b8d"
   },
   "outputs": [],
   "source": [
    "test_fake = g_net(fixed_latent_noise)\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_fake.detach().cpu(), 4, normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lets create a gif of our generated images throughout training</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('MNIST_GAN.gif', test_images_log)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DCGAN_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
