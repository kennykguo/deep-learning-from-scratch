{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da9ac14-5703-4406-a9f1-90d2fbf815fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# Using IMDB dataset\n",
    "# This function builds the dataset for training\n",
    "# Input - config (PPOConfig), dataset_name, min_max text lengths\n",
    "# Output - dataset of prompts (states) for the RL environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b664cbf8-9168-440d-b4b5-a165d7c0fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    # Build a dataset to be used for the training.\n",
    "    # It is a series of prompts (each with different length chosen randomly)\n",
    "    # We will use it to generate the responses and compute the rewards.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # load the IMDB dataset\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    \n",
    "    # Only choose reviews with more than 200 tokens\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "    \n",
    "    def tokenize(sample):\n",
    "        # From each review just keep the first `input_size` tokens, this represents the prompt used to generate the response\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "        \n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbfd4ff0-3d98-4155-bf31-933810506836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6e90ff-9937-4453-81b0-526357924052",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    # Finetuned GPT2\n",
    "    model_name=\"lvwerra/gpt2-imdb\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05d0699e-5aea-48b8-bbce-5ef6da906a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed44c15-d179-474e-a687-32d50c9eda29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae970b41-ee2b-42a4-8b1d-8a6ca5542601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24895\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3411c9f-3e4d-4698-a1c5-e1e283240de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['review', 'label', 'input_ids', 'query'],\n",
      "    num_rows: 24895\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "696ac743-e338-4248-8747-00a390d9d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the model we are going to fine-tune with PPO\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "\n",
    "# This is the reference model (frozen) for the KL divergence\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb447347-c034-4813-806f-03575300214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModelForCausalLMWithValueHead(\n",
      "  (pretrained_model): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2SdpaAttention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  )\n",
      "  (v_head): ValueHead(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a14e59b-864c-4acc-ae94-07ddb3a379f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8383b415-ad31-4775-9656-a68f779e6a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='lvwerra/gpt2-imdb', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13724852-2f45-436e-9e0e-01f99f1d67bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:z1q9fm7z) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-frost-2</strong> at: <a href='https://wandb.ai/kennykguo-developer/deep_learning_from_scratch-reinforcement_learning/runs/z1q9fm7z' target=\"_blank\">https://wandb.ai/kennykguo-developer/deep_learning_from_scratch-reinforcement_learning/runs/z1q9fm7z</a><br/> View project at: <a href='https://wandb.ai/kennykguo-developer/deep_learning_from_scratch-reinforcement_learning' target=\"_blank\">https://wandb.ai/kennykguo-developer/deep_learning_from_scratch-reinforcement_learning</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240815_233352-z1q9fm7z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:z1q9fm7z). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kennykguo/deep_learning_from_scratch/reinforcement_learning/wandb/run-20240815_233759-bdi4u3rf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kennykguo-developer/trl/runs/bdi4u3rf' target=\"_blank\">avid-terrain-2</a></strong> to <a href='https://wandb.ai/kennykguo-developer/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kennykguo-developer/trl' target=\"_blank\">https://wandb.ai/kennykguo-developer/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kennykguo-developer/trl/runs/bdi4u3rf' target=\"_blank\">https://wandb.ai/kennykguo-developer/trl/runs/bdi4u3rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)\n",
    "\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee86899d-b81e-4fe9-8dfd-84a571a622df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<trl.trainer.ppo_trainer.PPOTrainer object at 0x7fda207b7340>\n"
     ]
    }
   ],
   "source": [
    "print(ppo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4875fad6-65cf-4350-8a23-ddc650417c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f8256b4-c9d9-4c7e-8a25-4052990442aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/kennykguo/anaconda3/envs/deep-learning/lib/python3.8/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'NEGATIVE', 'score': 2.335048198699951}, {'label': 'POSITIVE', 'score': -2.726576566696167}]]\n",
      "[[{'label': 'NEGATIVE', 'score': -2.294790029525757}, {'label': 'POSITIVE', 'score': 2.557039737701416}]]\n"
     ]
    }
   ],
   "source": [
    "# This is the reward model: a \"positive\" (e.g. a positive review) response will be given a high reward, a \"negative\" response will be given a low reward\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)\n",
    "\n",
    "# Print some examples of sentiments generated by the reward model\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}\n",
    "\n",
    "text = \"this movie was really bad!!\"\n",
    "print(sentiment_pipe(text, **sent_kwargs))\n",
    "\n",
    "text = \"this movie was really good!!\"\n",
    "print(sentiment_pipe(text, **sent_kwargs)) # [{'label': 'NEGATIVE', 'score': -2.335047960281372}, {'label': 'POSITIVE', 'score': 2.557039737701416}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4f781c9-37dc-4962-b9f7-e6f8e4ccc98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "# The configuration to generate responses (trajectories)\n",
    "response_generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49446f8b-e692-47b0-9a5b-ef31653aaef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_length': -1,\n",
       " 'top_k': 0.0,\n",
       " 'top_p': 1.0,\n",
       " 'do_sample': True,\n",
       " 'pad_token_id': 50256,\n",
       " 'max_new_tokens': 7}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_generation_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1bcaf26-b825-4f40-b920-d9fc922586a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'input_ids', 'query'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataloader is passed in at initialization\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    print(batch.keys())\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Phase 1: Get trajectories from the offline policy\n",
    "    # In this case we are only generating the responses, but not computing the log probabilities, which will be computed internally by the PPOTrainer.\n",
    "    response_tensors = []\n",
    "\n",
    "    # For every one of the queries (sample starting reviews)\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        # Update the dictionary\n",
    "        response_generation_kwargs[\"max_new_tokens\"] = gen_len # Number of tokens to generate (chosen randomly)\n",
    "        # Generate a response, and only save the response token ids\n",
    "        response = ppo_trainer.generate(query, **response_generation_kwargs) # It returns the (query + response) tokens\n",
    "        response_tensors.append(response.squeeze()[-gen_len:]) # Only take the tokens corresponding to the generated response (remove the prompt/query from the beginning)\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Phase 1: Compute rewards\n",
    "    # Join the query (prompt) + response (generated tokens)\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    # Compute the reward for each of the texts (query + response)\n",
    "    # shape: A list of dictionaries with two keys: POSITIVE and NEGATIVE. We are interested in the POSITIVE score. This will be our reward.\n",
    "    # List of lists [[][][][]]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs) # [{'label': 'NEGATIVE', 'score': -2.335047960281372}, {'label': 'POSITIVE', 'score': 2.557039737701416}]\n",
    "    \n",
    "    # The reward for each text is the score (logit) corresponding to the POSITIVE class. \n",
    "    # Shape: A list of scalars, one for each generated response. \n",
    "    # It means we assign the reward to the whole response (not to each token).\n",
    "    # We pick the NEGATIVE score to train the model\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    \n",
    "    #### Phase 1 + Phase 2: calculate the logprobs and then run the PPO update\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfaf033-9882-47cb-824f-1f6acffa80f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"gpt2-imdb-pos-v2\", push_to_hub=False)\n",
    "tokenizer.save_pretrained(\"gpt2-imdb-pos-v2\", push_to_hub=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning)",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
