{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proximal Policy Optimization</h1>\n",
    "    \n",
    "In Lectures we have seen the concepts behind Reinforcement Learning and how the various algorthims work to maximise some objective. We've seen basic versions learn in gridworlds with look-up tables and more modern version learn control tasks with MLPs.<br>\n",
    "In this lab we are taking things even further and introducing a very popular and widely used policy gradient algroithm, <a href=\"https://arxiv.org/pdf/1707.06347.pdf\">Proximal Policy Optimization</a>. This algorthims is the culmulation of many improvements in policy gradient algorithms and contains many tricks to help quickly and stably train RL agents on a range of possible tasks. <br>\n",
    "In this lab we'll talk through the different improvements that have been made over basic Actor Critic algorithms and then we'll look at what makes PPO special. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1632973086697,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "MuHbe8YyJKvr"
   },
   "outputs": [],
   "source": [
    "# Uncomment to install the procgen environments!\n",
    "# !pip install procgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1632973110350,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "b1P9AkmzJDrL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "from procgen import ProcgenEnv\n",
    "import gym\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Categorical\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1632973087875,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "msANO4BXJtSb"
   },
   "outputs": [],
   "source": [
    "device = torch.device(0 if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/PROCGEN.png\" width=\"750\" align=\"center\">\n",
    "<br>\n",
    "In the lectures we introduced the <a href=\"https://openai.com/blog/procgen-benchmark/\">Procgen</a> training environments, which were created to test the generalization ability of Reinforcement Learning.<br>\n",
    "However, we never trained an RL agent to learn from these environments!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1999,
     "status": "ok",
     "timestamp": 1632973089866,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "GX2jtVh7JDrO"
   },
   "outputs": [],
   "source": [
    "# Procgen uses OpenAI gym and can create parallel instances of the same game, allowing us to collect many rollouts \n",
    "# at once, meaning we can train much faster and smoother!\n",
    "\n",
    "# The name of the Procgen game\n",
    "env_name = \"coinrun\"\n",
    "# How many levels we wish to train on\n",
    "num_levels = 20\n",
    "# Hard or easy levels (see paper for the difference)\n",
    "dist_mode = \"easy\"\n",
    "# The number of parallel instances of the game we wish to create \n",
    "num_envs = 64\n",
    "# Create the environment instance\n",
    "envs = ProcgenEnv(num_envs=num_envs, env_name=env_name, start_level=0,\n",
    "                  num_levels=num_levels, distribution_mode=dist_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Network Architecture</h3>\n",
    "We are taking the Large IMPALA Architecture (without LSTM) from the <a href=\"https://arxiv.org/pdf/1802.01561.pdf\">Importance Weighted Actor-Learner Architectures</a> Paper which outlines a method to train RL agents in a Scalable and Distributed way.\n",
    "Note that we are NOT using that method here, we are simply using the Network.<br>\n",
    "This Network is Deep, but is still \"light-weight\" enough for RL, it also has very large residual skip connects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1632973089867,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "R89reL1YJDrP"
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, 3, 1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = self.relu(x_in)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x_out = self.conv2(x)\n",
    "        return x_in + x_out\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, block_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, block_channels, 3, 1, 1)\n",
    "        self.max_pool = nn.MaxPool2d(3, 2, 1)\n",
    "        self.res1 = ResBlock(block_channels)\n",
    "        self.res2 = ResBlock(block_channels)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = self.conv1(x_in)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.res1(x)\n",
    "        return self.res2(x)\n",
    "\n",
    "\n",
    "class ImpalaCnn64(nn.Module):\n",
    "    def __init__(self, in_channels, num_outputs, base_channels=16):\n",
    "        super(ImpalaCnn64, self).__init__()\n",
    "        self.conv_block1 = ConvBlock(in_channels, base_channels)\n",
    "        self.conv_block2 = ConvBlock(base_channels, 2*base_channels)\n",
    "        self.conv_block3 = ConvBlock(2*base_channels, 2*base_channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(8*8*2*base_channels, 256)\n",
    "        self.actor = nn.Linear(256, num_outputs)\n",
    "        self.critic = nn.Linear(256, 1)\n",
    "        self.SM = nn.Softmax(1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        self.state = self.relu(self.fc1(x))\n",
    "\n",
    "        value = self.critic(self.state)\n",
    "        self.prob = self.SM(self.actor(self.state))\n",
    "        dist = Categorical(probs=self.prob)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset Buffer</h3>\n",
    "Just like we saw in Deep Q learning we will use a buffer to store all of the environment interactions. However, unlike DQNs all the data in the buffer will be collected by the same Policy. So we will completly refill the buffer after every training update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1632973089868,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "qJ6WBbvtJDrQ"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, data_names, buffer_size, mini_batch_size, device):\n",
    "        self.data_keys = data_names\n",
    "        self.data_dict = {}\n",
    "        self.buffer_size = buffer_size\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a deque for each data type with set max length\n",
    "        for name in self.data_keys:\n",
    "            self.data_dict[name] = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def buffer_full(self):\n",
    "        return len(self.data_dict[self.data_keys[0]]) == self.buffer_size\n",
    "\n",
    "    def data_log(self, data_name, data):\n",
    "        # split tensor along batch into a list of individual datapoints\n",
    "        data = data.cpu().split(1)\n",
    "        # Extend the deque for data type, deque will handle popping old data to maintain buffer size\n",
    "        self.data_dict[data_name].extend(data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_size = len(self.data_dict[self.data_keys[0]])\n",
    "        batch_size = batch_size - batch_size % self.mini_batch_size\n",
    "\n",
    "        ids = np.random.permutation(batch_size)\n",
    "        ids = np.split(ids, batch_size // self.mini_batch_size)\n",
    "        for i in range(len(ids)):\n",
    "            batch_dict = {}\n",
    "            for name in self.data_keys:\n",
    "                c = [self.data_dict[name][j] for j in ids[i]]\n",
    "                batch_dict[name] = torch.cat(c).to(self.device)\n",
    "            batch_dict[\"batch_size\"] = len(ids[i])\n",
    "            yield batch_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict[self.data_keys[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Helper Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1632973089869,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "KAQztEwnJDrR"
   },
   "outputs": [],
   "source": [
    "# Procgen returns a dictionary as the state, this fuction converts the rbg images [0, 255] into a tensor [0, 1]\n",
    "def state_to_tensor(state_dict, device):\n",
    "    if type(state_dict) is dict:\n",
    "        state = torch.FloatTensor(state_dict['rgb'].transpose((0, 3, 1, 2))).to(device)\n",
    "    else:\n",
    "        state = torch.FloatTensor(state_dict.transpose((2, 0, 1))).unsqueeze(0).to(device)\n",
    "\n",
    "    return state / 256  # Put tensor on gpu before divide - much faster\n",
    "\n",
    "\n",
    "# Convert the Image Tensor to a uint8 image for saving\n",
    "def tensor_to_unit8(tesnor):\n",
    "    numpy_float = tesnor.squeeze(0).cpu().numpy().transpose((1, 2, 0))\n",
    "    return (numpy_float * 255).astype(np.uint8)\n",
    "\n",
    "# To test the agent we loop through all the training levels and an equivelant number of unseen levels\n",
    "# Note this is not optimal as the training will wait untill this is done before continuing.\n",
    "# With more training levels the time it takes to test will increase!\n",
    "# Testing is usually done in a seperate process using the current saved checkpoint of the Policy parameters \n",
    "# (see IMPALA paper for a \"full on\" distributed method)\n",
    "def run_tests(dist_mode, env_name, num_levels, train_test=\"train\"):\n",
    "    if train_test == \"train\":\n",
    "        start_level = 0\n",
    "    else:\n",
    "        start_level = num_levels\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(num_levels):\n",
    "        env = gym.make(\"procgen:procgen-\" + env_name + \"-v0\", \n",
    "                       start_level=start_level + i, num_levels=1, distribution_mode=dist_mode)\n",
    "        scores.append(test_agent(env))\n",
    "        \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Tests Policy once on the given environment\n",
    "def test_agent(env, log_states=False):\n",
    "    start_state = env.reset()\n",
    "    state = state_to_tensor(start_state, device)\n",
    "    \n",
    "    if log_states:\n",
    "        states_logger = [tensor_to_unit8(state)]\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            dist, _ = rl_model(state)  # Forward pass of actor-critic model\n",
    "            action = dist.sample().item()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = state_to_tensor(next_state, device)\n",
    "            if log_states:\n",
    "                states_logger.append(tensor_to_unit8(state))\n",
    "                \n",
    "    if log_states:\n",
    "        return total_reward, states_logger\n",
    "    else:\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PPO Clipped Surrogate Objective </h3>\n",
    "PPO follows on from another algorithm called Trust Region Policy Optimization (TRPO) where they ask the question, <em>\"how much should we trust this optimization step</em>?\" RL is notoriously \"noisy\" and the wrong step could collapse our policy! PPO uses an approximation of the \"Trust Region\" step in TRPO (which can be difficult to compute) based on some heuristics (created using their experience of implementing RL algorithms) to perform a \"safe\" update step.<br>\n",
    "\n",
    "The total combined optimization objective that we wish to <b>Maximise</b> ($L^{CLIP}$) is as follows:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[min \\left(r_t(\\theta)\\hat{\\mathbf{A}}_t, \\:clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{\\mathbf{A}}_t\\right)] \n",
    "\\end{equation*}\n",
    "\n",
    "Where:\n",
    "\\begin{equation*}\n",
    "r_t(\\theta) = \\frac{\\pi_\\theta(a_t, s_t)}{\\pi_{\\theta_{old}}(a_t, s_t)}\n",
    "\\end{equation*}\n",
    "<br>\n",
    "Also: $\\hat{\\mathbf{A}}_t$ is the Advantage (more on this soon) and $\\epsilon$ is the clipping parameter.<br>\n",
    "\n",
    "A visual representation of this objective is seen below.\n",
    "\n",
    "<img src=\"../data/PPO_Clipping.png\" width=\"750\" align=\"center\">\n",
    "\n",
    "<br>\n",
    "The best way to understand the objective is to see it implemented! Read the comments bellow to see how each part works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_loss(new_dist, actions, old_log_probs, advantages, clip_param):\n",
    "    ########### Policy Gradient update for actor with clipping - PPO #############\n",
    "    \n",
    "    # 1. Find the new probability \n",
    "    # Work out the probability (log probability) that the agent will NOW take\n",
    "    # the action it took during the rollout\n",
    "    # We assume there has been some optimisation steps between when the action was taken and now so the\n",
    "    # probability has probably changed\n",
    "    new_log_probs = new_dist.log_prob(actions)\n",
    "    \n",
    "    # 2. Find the ratio of new to old - r_t(theta)\n",
    "    # Calculate the ratio of new/old action probability (remember we have log probabilities here)\n",
    "    # log(new_prob) - log(old_prob) = log(new_prob/old_prob)\n",
    "    # exp(log(new_prob/old_prob)) = new_prob/old_prob\n",
    "    # We use the ratio of new/old action probabilities (not just the log probability of the action like in\n",
    "    # vanilla policy gradients) so that if there is a large difference between the probabilities then we can\n",
    "    # take a larger/smaller update step\n",
    "    # EG: If we want to decrease the probability of taking an action but the new action probability\n",
    "    # is now higher than it was before we can take a larger update step to correct this\n",
    "    ratio = (new_log_probs - old_log_probs).exp()\n",
    "\n",
    "    # 3. Calculate the ratio * advantage - the first term in the MIN statement\n",
    "    # We want to MAXIMISE the (Advantage * Ratio)\n",
    "    # If the advantage is positive this corresponds to INCREASING the probability of taking that action\n",
    "    # If the advantage is negative this corresponds to DECREASING the probability of taking that action\n",
    "    surr1 = ratio * advantages\n",
    "\n",
    "    # 4. Calculate the (clipped ratio) * advantage - the second term in the MIN statement\n",
    "    # PPO goes a bit further, if we simply update update using the Advantage * Ratio we will sometimes\n",
    "    # get very large or very small policy updates when we don't want them\n",
    "    #\n",
    "    # EG1: If we want to increase the probability of taking an action but the new action probability\n",
    "    # is now higher than it was before we will take a larger step, however if the action probability is\n",
    "    # already higher we don't need to keep increasing it (large output values can create instabilities).\n",
    "    #\n",
    "    # EG2: You can also consider the opposite case where we want to decrease the action probability\n",
    "    # but the probability has already decreased, in this case we will take a smaller step than before,\n",
    "    # which is also not desirable as it will slow down the \"removal\" (decreasing the probability)\n",
    "    # of \"bad\" actions from our policy.\n",
    "    surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
    "    \n",
    "    # 5. Take the minimum of the two \"surrogate\" losses\n",
    "    # PPO therefore clips the upper bound of the ratio when the advantage is positive\n",
    "    # and clips the lower bound of the ratio when the advantage is negative so our steps are not too large\n",
    "    # or too small when necessary, it does this by using a neat trick of simply taking the MIN of two \"surrogate\"\n",
    "    # losses which chooses which loss to use!\n",
    "    actor_loss = torch.min(surr1, surr2)\n",
    "    \n",
    "    # 6. Return the Expectation over the batch\n",
    "    return actor_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_critic_loss(new_value, old_value, returns, clip_param):\n",
    "    ########### Value Function update for critic with clipping #############\n",
    "    \n",
    "    # To help stabalise the training of the value function we can do a similar thing as the clipped objective\n",
    "    # for PPO - Note: this is NOT nessisary but does help!\n",
    "        \n",
    "    # 1. MSE/L2 loss on the current value and the returns\n",
    "    vf_loss1 = (new_value - returns).pow(2.)\n",
    "    \n",
    "    # 2. MSE/L2 loss on the clipped value and the returns\n",
    "    # Here we create an \"approximation\" of the new value (aka the current value) by finding the difference\n",
    "    # between the \"new\" and \"old\" value and adding a clipped amount back to the old value\n",
    "    vpredclipped = old_value + torch.clamp(new_value - old_value, -clip_param, clip_param)\n",
    "    # Note that we ONLY backprop through the new value\n",
    "    vf_loss2 = (vpredclipped - returns).pow(2.)\n",
    "    \n",
    "    # 3. Take the MAX between the two losses\n",
    "    # This trick has the effect of only updating the current value DIRECTLY if is it WORSE (higher error)\n",
    "    # than the old value. \n",
    "    # If the old value was worse then the \"approximation\" will be worse and we update\n",
    "    # the new value only a little bit!\n",
    "    critic_loss = torch.max(vf_loss1, vf_loss2)\n",
    "    \n",
    "    # 4. Return the Expectation over the batch\n",
    "    return critic_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generalised Advantage Estimate</h3>\n",
    "Another Improvement that implementation of PPO often use is the Generalised Advantage Estimate (GAE) from the paper\n",
    "<a href=\"https://arxiv.org/pdf/1506.02438.pdf\">High Dimensional Continuous Control Using Generalized Advantage Estimation</a> (This is also a great paper for understanding policy optimisation algorithms and RL in general).<br>\n",
    "GAE creates an estimate of the Advantage that is less noisy be combining Advantage you get by using the returns approximation from TD learning and \"real\" returns update.<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{A}}_t^{GAE(\\gamma, \\tau)} = \\sum_{l=0}^{\\infty}{(\\gamma\\tau)^l\\delta^V_{t+1}}\n",
    "\\end{equation*}\n",
    "<br>\n",
    "Where $\\delta^V_{t+1}$ is our TD update (and also the an approximation of the Advantage):\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "\\delta^V_{t+1} =  r_t + \\gamma\\mathbf{V}(s_{t+1})-\\mathbf{V}(s_t)\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\\begin{equation*}\n",
    "\\mathbf{R}_t \\: \\tilde{=}  \\: r_t + \\gamma\\mathbf{V}(s_{t+1})\n",
    "\\end{equation*}\n",
    "<br>\n",
    "And $\\gamma$ is the decay rate of future reward, $0 < \\gamma < 1$,  and $\\tau$ is the decay rate of the sum of $\\delta^V_{t+1}$, $0 < \\tau < 1$.<br>\n",
    "\n",
    "To get a better idea of what $\\tau$ does lets look at two cases outlined in the paper, when $\\tau = 0$ and $\\tau = 1$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{A}}_t^{GAE(\\gamma, 0)} = r_t + \\gamma\\mathbf{V}(s_{t+1})-\\mathbf{V}(s_t)\n",
    "\\end{equation*}\n",
    "<br>\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{A}}_t^{GAE(\\gamma, 1)} = \\sum_{l=0}^{\\infty}{\\gamma^lr_{t+l}}-\\mathbf{V}(s_t)\n",
    "\\end{equation*}\n",
    "\n",
    "You can see that $\\tau$ controls the mixing between a basic advantage calculation (which can be noisy) and an advantage calculation using the Value and rewards as an appromimation of the returns (which is biased). <br>\n",
    "To seperate out different levels within a single environment so that we don't calculate the GAE over the WHOLE sequence we use \"masks\" (1 or 0) to \"zero\" the GAE and start again. More on the training environments later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1632973089868,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "qt9HAuquJDrQ"
   },
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.999, tau=0.95):\n",
    "    # Similar to calculating the returns we can start at the end of the sequence and go backwards\n",
    "    gae = 0\n",
    "    returns = deque()\n",
    "    gae_logger = deque()\n",
    "\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        # Calculate the current delta value\n",
    "        delta = rewards[step] + gamma * next_value * masks[step] - values[step]\n",
    "        \n",
    "        # The GAE is the decaying sum of these delta values\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        \n",
    "        # Get the new next value\n",
    "        next_value = values[step]\n",
    "        \n",
    "        # If we add the value back to the GAE we get a TD approximation for the returns\n",
    "        # which we can use to train the Value function\n",
    "        returns.appendleft(gae + values[step])\n",
    "        gae_logger.appendleft(gae)\n",
    "\n",
    "    return returns, gae_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Update Loop</h3>\n",
    "Using the loss functions we created before we will now train our actor/critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1632978544806,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "JYiIKpp8JDrS"
   },
   "outputs": [],
   "source": [
    "def ppo_update(data_buffer, ppo_epochs, clip_param):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for data_batch in data_buffer:\n",
    "            # Forward pass of input state observationsequence\n",
    "            new_dist, new_value = rl_model(data_batch[\"states\"])\n",
    "\n",
    "            # Most Policy gradient algorithms include a small \"Entropy bonus\" to increases the \"entropy\" of \n",
    "            # the action distribution, aka the \"randomness\"\n",
    "            # This ensures that the actor does not converge to taking the same action everytime and\n",
    "            # maintains some ability for \"exploration\" of the policy\n",
    "            \n",
    "            # Determine expectation over the batch of the action distribution entropy\n",
    "            entropy = new_dist.entropy().mean()\n",
    "\n",
    "            actor_loss = ppo_loss(new_dist, data_batch[\"actions\"], data_batch[\"log_probs\"], data_batch[\"advantages\"],\n",
    "                                  clip_param)\n",
    "\n",
    "            critic_loss = clipped_critic_loss(new_value, data_batch[\"values\"], data_batch[\"returns\"], clip_param)\n",
    "\n",
    "            # These techniques allow us to do multiple epochs of our data without huge update steps throwing off our\n",
    "            # policy/value function (gradient explosion etc).\n",
    "            # It can also help prevent \"over-fitting\" to a single batch of observations etc, \n",
    "            # RL boot-straps itself and the noisy \"ground truth\" targets (if you can call them that) will\n",
    "            # shift overtime and we need to make sure our actor-critic can quickly adapt, over-fitting to a\n",
    "            # single batch of observations will prevent that\n",
    "            agent_loss = critic_loss - actor_loss - 0.01 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            agent_loss.backward()\n",
    "            # Clip gradient norm to further prevent large updates\n",
    "            nn.utils.clip_grad_norm_(rl_model.parameters(), 40)\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1632973114796,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "SroP0A8AJDrS"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters, PPO has many many hyperparameters\n",
    "lr = 5e-4\n",
    "# How many minibatchs (therefore optimization steps) we want per epoch \n",
    "num_mini_batch = 8\n",
    "# Total number of steps during the rollout phase \n",
    "num_steps = 256 \n",
    "# Number of Epochs for training\n",
    "ppo_epochs = 3\n",
    "\n",
    "# PPO parameters\n",
    "gamma = 0.999\n",
    "tau = 0.95\n",
    "clip_param = 0.2\n",
    "\n",
    "# Training parameters\n",
    "max_frames = 10e6\n",
    "frames_seen = 0\n",
    "rollouts = 0\n",
    "\n",
    "# Score loggers\n",
    "test_score_logger = []\n",
    "train_score_logger = []\n",
    "frames_logger = []\n",
    "\n",
    "# Set the size of the FIFO databuffer to the total number of steps for a single batch of rollouts\n",
    "# so that every episode the buffer has completely reset\n",
    "buffer_size = num_steps * num_envs\n",
    "# Calculate the size of each minibatch  - usually very big - 2048!\n",
    "mini_batch_size = buffer_size // num_mini_batch\n",
    "\n",
    "# Define the data we wish to collect for the databuffer\n",
    "data_names = [\"states\", \"actions\", \"log_probs\", \"values\", \"returns\", \"advantages\"]\n",
    "data_buffer = ReplayBuffer(data_names, buffer_size, mini_batch_size, device)\n",
    "\n",
    "# Create the actor critic Model and optimizer\n",
    "rl_model = ImpalaCnn64(3, 15).to(device)\n",
    "optimizer = optim.Adam(rl_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Main Training Loop</h3>\n",
    "To understand the structure of the Rollout/Training Cycle it is important to know how the Procgen environments work and how the rollout/training structure works.\n",
    "<br>\n",
    "We have created a number of environments that will run in seperate processes on your computer. With every environment step you will get a dictionary that defines the current state. Within the dictionary is a batch of images that are the current game frames for all environments. The Procgen environments will run continuously, that is, for every environment as soon as one game/level is completed a new one starts. Therefore we do not use a \"done\" signal to determine when to stop as the level in one environment may be compleated before another.\n",
    "<br><br>\n",
    "By instead stopping after a number of timesteps we have another problem, we may have cut a rollout in half, stopping before it is over! Because of this we won't know if we could have gotten a Reward and therefore don't know what the true Returns would be. However, we are using a TD approximate of the Returns, that is we are using the Value function as an approximation of the Returns and so do not need to complete the level every time in order to train, the algorithm just uses it's best guess. As long as, on average, we fully complete the level within the rollout we can still train. Most of the levels in the different Procgen games can be completed within 256 steps and so this is the max  number of steps we use! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uuRixotKJDrT",
    "outputId": "64c5a540-08a7-4a52-9b58-dcecf06f361a"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "while frames_seen < max_frames:\n",
    "    rl_model.train()\n",
    "    # Initialise state\n",
    "    start_state = envs.reset()\n",
    "    state = state_to_tensor(start_state, device)\n",
    "\n",
    "    # Create data loggers - deques a bit faster than lists\n",
    "    log_probs = deque()\n",
    "    values = deque()\n",
    "    states = deque()\n",
    "    actions = deque()\n",
    "    rewards = deque()\n",
    "    masks = deque()\n",
    "    step = 0\n",
    "    done = np.zeros(num_envs)\n",
    "\n",
    "    print(\"Rollout!\")\n",
    "    with torch.no_grad():  # Don't need computational graph for roll-outs\n",
    "        while step < num_steps:\n",
    "            #  Masks so we can separate out multiple games in the same environment\n",
    "            dist, value = rl_model(state)  # Forward pass of actor-critic model\n",
    "            action = dist.sample()  # Sample action from distribution\n",
    "\n",
    "            # Take the next step in the env\n",
    "            next_state, reward, done, env_info = envs.step(action.cpu().numpy())\n",
    "\n",
    "            # Work out reward, we are just setting the reward to be either\n",
    "            # -1, 1 or 0 for negative, positive or no reward\n",
    "            reward = torch.FloatTensor(((reward > 0).astype(\"float64\") - (reward < 0).astype(\"float64\")))\n",
    "\n",
    "            # Log data\n",
    "            log_prob = dist.log_prob(action)\n",
    "            log_probs.append(log_prob)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            values.append(value)\n",
    "            rewards.append(reward.unsqueeze(1).to(device))\n",
    "            current_mask = torch.FloatTensor(1 - done).unsqueeze(1).to(device)\n",
    "            masks.append(current_mask)\n",
    "\n",
    "            state = state_to_tensor(next_state, device)\n",
    "            step += 1\n",
    "\n",
    "        # Get value at time step T+1\n",
    "        _, next_value = rl_model(state)\n",
    "        # Calculate the returns/gae\n",
    "        returns, advantage = compute_gae(next_value, rewards, masks, values, gamma=gamma, tau=tau)\n",
    "\n",
    "        data_buffer.data_log(\"states\", torch.cat(list(states)))\n",
    "        data_buffer.data_log(\"actions\", torch.cat(list(actions)))\n",
    "        data_buffer.data_log(\"returns\", torch.cat(list(returns)))\n",
    "        data_buffer.data_log(\"log_probs\", torch.cat(list(log_probs)))\n",
    "        data_buffer.data_log(\"values\", torch.cat(list(values)))\n",
    "\n",
    "        advantage = torch.cat(list(advantage)).squeeze(1)\n",
    "        # Normalising the Advantage helps stabalise training!\n",
    "        data_buffer.data_log(\"advantages\", (advantage - advantage.mean()) / (advantage.std() + 1e-8))\n",
    "        \n",
    "        # Update the frames counter\n",
    "        # We normaly base how long to train for by counting the number of \"environment interactions\"\n",
    "        # In our case we can simply counte how many game frames we have received from the environment\n",
    "        frames_seen += advantage.shape[0]\n",
    "    \n",
    "    # We train after every batch of rollouts\n",
    "    # With the stabalisation techniques in PPO we can \"safely\" take many steps with a single\n",
    "    # batch of rollouts, therefore we usualy train with the data over multiple epochs whereas basic\n",
    "    # actor critic methods only use one epoch.\n",
    "    print(\"Training\")\n",
    "    ppo_update(data_buffer, ppo_epochs, clip_param)\n",
    "    rollouts += 1\n",
    "\n",
    "    if rollouts % 10 == 0:\n",
    "        print(\"Testing\")\n",
    "        rl_model.eval()\n",
    "        test_score = run_tests(dist_mode, env_name, num_levels, train_test=\"test\")\n",
    "        train_score = run_tests(dist_mode, env_name, num_levels, train_test=\"train\")\n",
    "\n",
    "        test_score_logger.append(test_score)\n",
    "        train_score_logger.append(train_score)\n",
    "        frames_logger.append(frames_seen)\n",
    "        print(\"Trained on %d Frames, Test/Train Scores [%d/%d]\" \n",
    "            %(frames_seen, test_score, train_score))\n",
    "\n",
    "        time_to_end = ((time.time() - start_time) / frames_seen) * (max_frames - frames_seen)\n",
    "        print(\"Time to end: %dh:%dm\" % (time_to_end // 3600, (time_to_end % 3600) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1632978567759,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06313774588804829868"
     },
     "user_tz": -600
    },
    "id": "V13p-x8dJDrU",
    "outputId": "7c6ace18-c4e1-4b60-b531-808aebf13d17"
   },
   "outputs": [],
   "source": [
    "plt.plot(frames_logger, test_score_logger)\n",
    "plt.plot(frames_logger, train_score_logger)\n",
    "plt.legend([\"Testing\", \"Training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBU07cr0Gy9I"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"procgen:procgen-\" + env_name + \"-v0\", start_level=200, num_levels=1, distribution_mode=dist_mode)\n",
    "score, states = test_agent(env, True)\n",
    "imageio.mimsave(env_name + \"_\" + dist_mode + \"_rollout.gif\", states, fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Procgen_PPO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
