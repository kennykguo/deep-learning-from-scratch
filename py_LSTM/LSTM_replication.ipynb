{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "803525d9-d13f-499e-b771-afcae11022db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b0d6f66-ac20-4696-991d-6a1ca0bfa4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/validation.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97256cc7-0a35-4b94-8fcc-970ab0309388",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data))\n",
    "V = vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9e5a487-c085-4ebe-aec7-02b743e34e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19349777"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "637b25a7-8250-408a-816a-08de66f506a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 19349777 characters, 101 unique.\n"
     ]
    }
   ],
   "source": [
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dad4b912-037e-49eb-8da9-fe2acb27a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_to_ix {'Â': 0, 'm': 1, '2': 2, 'I': 3, 'C': 4, \"'\": 5, 'q': 6, '\\n': 7, 'R': 8, 'P': 9, 'c': 10, 'E': 11, '?': 12, 'M': 13, 'V': 14, 'O': 15, ' ': 16, 'k': 17, '4': 18, '3': 19, 'l': 20, 'i': 21, '1': 22, 'T': 23, 'â': 24, 'N': 25, 'r': 26, 'Ã': 27, 'y': 28, '0': 29, 'D': 30, '´': 31, '7': 32, '€': 33, '“': 34, 'a': 35, 'e': 36, '9': 37, '*': 38, 'U': 39, 'p': 40, 'z': 41, ';': 42, 'Q': 43, 'u': 44, '<': 45, 'Š': 46, 'X': 47, 'n': 48, 'J': 49, 'h': 50, 'w': 51, 'L': 52, '˜': 53, 'Ž': 54, 't': 55, '\"': 56, ')': 57, '.': 58, 'b': 59, ',': 60, '-': 61, '‹': 62, '$': 63, '/': 64, 'f': 65, 'j': 66, 'Y': 67, 'x': 68, '!': 69, '”': 70, 's': 71, 'B': 72, 'K': 73, 'v': 74, 'A': 75, 'ð': 76, ':': 77, '5': 78, 'G': 79, '&': 80, '+': 81, '©': 82, '™': 83, 'd': 84, 'S': 85, '6': 86, '#': 87, 'H': 88, 'œ': 89, 'Ÿ': 90, 'W': 91, '8': 92, '(': 93, 'g': 94, '±': 95, '¦': 96, '\\xad': 97, 'o': 98, 'Z': 99, 'F': 100}\n",
      "ix_to_char {0: 'Â', 1: 'm', 2: '2', 3: 'I', 4: 'C', 5: \"'\", 6: 'q', 7: '\\n', 8: 'R', 9: 'P', 10: 'c', 11: 'E', 12: '?', 13: 'M', 14: 'V', 15: 'O', 16: ' ', 17: 'k', 18: '4', 19: '3', 20: 'l', 21: 'i', 22: '1', 23: 'T', 24: 'â', 25: 'N', 26: 'r', 27: 'Ã', 28: 'y', 29: '0', 30: 'D', 31: '´', 32: '7', 33: '€', 34: '“', 35: 'a', 36: 'e', 37: '9', 38: '*', 39: 'U', 40: 'p', 41: 'z', 42: ';', 43: 'Q', 44: 'u', 45: '<', 46: 'Š', 47: 'X', 48: 'n', 49: 'J', 50: 'h', 51: 'w', 52: 'L', 53: '˜', 54: 'Ž', 55: 't', 56: '\"', 57: ')', 58: '.', 59: 'b', 60: ',', 61: '-', 62: '‹', 63: '$', 64: '/', 65: 'f', 66: 'j', 67: 'Y', 68: 'x', 69: '!', 70: '”', 71: 's', 72: 'B', 73: 'K', 74: 'v', 75: 'A', 76: 'ð', 77: ':', 78: '5', 79: 'G', 80: '&', 81: '+', 82: '©', 83: '™', 84: 'd', 85: 'S', 86: '6', 87: '#', 88: 'H', 89: 'œ', 90: 'Ÿ', 91: 'W', 92: '8', 93: '(', 94: 'g', 95: '±', 96: '¦', 97: '\\xad', 98: 'o', 99: 'Z', 100: 'F'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i, ch in enumerate(chars)}\n",
    "print('char_to_ix', char_to_ix)\n",
    "print('ix_to_char', ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49d73ca7-b3ec-4236-909e-89717624e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of hidden state vectors; applies to h and c.\n",
    "H = hidden_size = 100\n",
    "seq_length = 16 # number of steps to unroll the LSTM for\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e58fbe51-4327-4e43-9fc0-a35add62af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input x is concatenated with state h\n",
    "# The joined vector is used to feed into most blocks within the LSTM cell\n",
    "HV = H + V # (100 + 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53b63f29-db16-4d3c-82ec-c6e569bb47da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dae113e6-e21a-45c8-98a6-4ce6d35ad7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DATA = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f9daf5d-fe1a-4658-94c5-18062b243741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters/weights -- these are shared among all steps\n",
    "# Inputs are characters one-hot encoded in a vocab-sized vector. (1, vocab_size)\n",
    "# Dimensions: H = hidden_size, V = vocab_size, HV = hidden_size + vocab_size\n",
    "\n",
    "Wf = np.random.randn(H, HV) * 0.01 # (100, 165)\n",
    "bf = np.zeros((H, 1)) # (100, 1)\n",
    "Wi = np.random.randn(H, HV) * 0.01 # (100, 165)\n",
    "bi = np.zeros((H, 1)) # (100, 1)\n",
    "Wcc = np.random.randn(H, HV) * 0.01 # (100, 165)\n",
    "bcc = np.zeros((H, 1)) # (100, 1)\n",
    "Wo = np.random.randn(H, HV) * 0.01 # (100, 165)\n",
    "bo = np.zeros((H, 1)) # (100, 1)\n",
    "Wy = np.random.randn(V, H) * 0.01 # (65, 100)\n",
    "by = np.zeros((V, 1)) # (100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa8c9c14-4d53-446c-a9ae-0465c7fda7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Computes sigmoid function.\n",
    "\n",
    "    z: array of input values.\n",
    "\n",
    "    Returns array of outputs, sigmoid(z).\n",
    "    \"\"\"\n",
    "    # Note: this version of sigmoid tries to avoid overflows in the computation\n",
    "    # of e^(-z), by using an alternative formulation when z is negative, to get\n",
    "    # 0. e^z / (1+e^z) is equivalent to the definition of sigmoid, but we won't\n",
    "    # get e^(-z) to overflow when z is very negative.\n",
    "    # Since both the x and y arguments to np.where are evaluated by Python, we\n",
    "    # may still get overflow warnings for large z elements; therefore we ignore\n",
    "    # warnings during this computation.\n",
    "    with np.errstate(over='ignore', invalid='ignore'):\n",
    "        return np.where(z >= 0,\n",
    "                        1 / (1 + np.exp(-z)),\n",
    "                        np.exp(z) / (1 + np.exp(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4fd07c7e-dd95-4fe6-8bfa-e0a381e951e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev, cprev):\n",
    "    \"\"\"Runs forward and backward passes through the RNN.\n",
    "      inputs, targets: Lists of integers. For some i, inputs[i] is the input\n",
    "                       character (encoded as an index into the ix_to_char map)\n",
    "                       and targets[i] is the corresponding next character in the\n",
    "                       training data (similarly encoded).\n",
    "      hprev: Hx1 array of initial hidden state (column vector)\n",
    "      cprev: Hx1 array of initial hidden state (column vector)\n",
    "      returns: loss, gradients on model parameters, and last hidden states\n",
    "    \"\"\"\n",
    "    # Caches that keep values computed in the forward pass at each time step, to be reused in the backward pass.\n",
    "    # Initialize as dictionaries to be indexed with time steps\n",
    "    xs, xhs, ys, ps, hs, cs, fgs, igs, ccs, ogs = (\n",
    "            {}, {}, {}, {}, {}, {}, {}, {}, {}, {})\n",
    "\n",
    "    # Initial incoming states (from time step = -1)\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    cs[-1] = np.copy(cprev)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # Forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        # Input at time step t is xs[t]. Prepare a one-hot encoded vector of\n",
    "        # shape (V, 1). inputs[t] is the index where the 1 goes.\n",
    "        xs[t] = np.zeros((V, 1)) # (65, 1) \n",
    "        xs[t][inputs[t]] = 1 # (65, 1) -> column vector\n",
    "\n",
    "        # hprev and xs[t] are column vector; stack them together into a \"taller\"\n",
    "        # column vector - first the elements of x, then h.\n",
    "        xhs[t] = np.vstack((xs[t], hs[t-1])) # (165, 1) -> column vector\n",
    "\n",
    "        # Gates f, i and o\n",
    "        fgs[t] = sigmoid(np.dot(Wf, xhs[t]) + bf) # (100, 165) @ (165, 1) = (100, 1)\n",
    "        igs[t] = sigmoid(np.dot(Wi, xhs[t]) + bi) # (100, 165) @ (165, 1) = (100, 1)\n",
    "        ogs[t] = sigmoid(np.dot(Wo, xhs[t]) + bo) # (100, 165) @ (165, 1) = (100, 1)\n",
    "\n",
    "        # Candidate cc\n",
    "        ccs[t] = np.tanh(np.dot(Wcc, xhs[t]) + bcc) # (100, 165) @ (165, 1) = (100, 1)\n",
    "\n",
    "        # This step's h and c\n",
    "        cs[t] = fgs[t] * cs[t-1] + igs[t] * ccs[t] # (100, 1) * (100, 1) + (100, 1) * (100, 1)\n",
    "        hs[t] = np.tanh(cs[t]) * ogs[t] # (100, 1)\n",
    "\n",
    "        # Convert to output\n",
    "        ys[t] = np.dot(Wy, hs[t]) + by # (65, 100) @ (100, 1) = (65, 1)\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # (65, 1)\n",
    "\n",
    "        # Cross-entropy loss.\n",
    "        loss += -np.log(ps[t][targets[t], 0]) # (65, 1)\n",
    "\n",
    "    # Initialize gradients of all weights/biases to 0.\n",
    "    # We only need to compute 5 matrices in backpropogation!\n",
    "    dWf = np.zeros_like(Wf)\n",
    "    dbf = np.zeros_like(bf)\n",
    "    \n",
    "    dWi = np.zeros_like(Wi)\n",
    "    dbi = np.zeros_like(bi)\n",
    "    \n",
    "    dWcc = np.zeros_like(Wcc)\n",
    "    dbcc = np.zeros_like(bcc)\n",
    "    \n",
    "    dWo = np.zeros_like(Wo)\n",
    "    dbo = np.zeros_like(bo)\n",
    "    \n",
    "    dWy = np.zeros_like(Wy)\n",
    "    dby = np.zeros_like(by)\n",
    "\n",
    "    # Incoming gradients for h and c; for backwards loop step these represent\n",
    "    # dh[t] and dc[t]; we do truncated BPTT, so assume they are 0 initially.\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dcnext = np.zeros_like(cs[0])\n",
    "    \n",
    "    # The backwards pass iterates over the input sequence backwards.\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backprop through the gradients of loss and softmax.\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1\n",
    "\n",
    "        # Compute gradients for the Wy and by parameters.\n",
    "        dWy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "\n",
    "        # Backprop through the fully-connected layer (Wy, by) to h. Also add up\n",
    "        # the incoming gradient for h from the next cell.\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "\n",
    "        # Backprop through multiplication with output gate; here \"dtanh\" means\n",
    "        # the gradient at the output of tanh.\n",
    "        dctanh = ogs[t] * dh\n",
    "        # Backprop through the tanh function; since cs[t] branches in two\n",
    "        # directions we add dcnext too.\n",
    "        dc = dctanh * (1 - np.tanh(cs[t]) ** 2) + dcnext\n",
    "\n",
    "        # Backprop through multiplication with the tanh; here \"dhogs\" means\n",
    "        # the gradient at the output of the sigmoid of the output gate. Then\n",
    "        # backprop through the sigmoid itself (ogs[t] is the sigmoid output).\n",
    "        dhogs = dh * np.tanh(cs[t])\n",
    "        dho = dhogs * ogs[t] * (1 - ogs[t])\n",
    "\n",
    "        # Compute gradients for the output gate parameters.\n",
    "        dWo += np.dot(dho, xhs[t].T)\n",
    "        dbo += dho\n",
    "\n",
    "        # Backprop dho to the xh input.\n",
    "        dxh_from_o = np.dot(Wo.T, dho)\n",
    "\n",
    "        # Backprop through the forget gate: sigmoid and elementwise mul.\n",
    "        dhf = cs[t-1] * dc * fgs[t] * (1 - fgs[t])\n",
    "        dWf += np.dot(dhf, xhs[t].T)\n",
    "        dbf += dhf\n",
    "        dxh_from_f = np.dot(Wf.T, dhf)\n",
    "\n",
    "        # Backprop through the input gate: sigmoid and elementwise mul.\n",
    "        dhi = ccs[t] * dc * igs[t] * (1 - igs[t])\n",
    "        dWi += np.dot(dhi, xhs[t].T)\n",
    "        dbi += dhi\n",
    "        dxh_from_i = np.dot(Wi.T, dhi)\n",
    "\n",
    "        dhcc = igs[t] * dc * (1 - ccs[t] ** 2)\n",
    "        dWcc += np.dot(dhcc, xhs[t].T)\n",
    "        dbcc += dhcc\n",
    "        dxh_from_cc = np.dot(Wcc.T, dhcc)\n",
    "\n",
    "        # Combine all contributions to dxh, and extract the gradient for the\n",
    "        # h part to propagate backwards as dhnext.\n",
    "        dxh = dxh_from_o + dxh_from_f + dxh_from_i + dxh_from_cc\n",
    "        dhnext = dxh[V:, :]\n",
    "\n",
    "        # dcnext from dc and the forget gate.\n",
    "        dcnext = fgs[t] * dc\n",
    "\n",
    "    # Gradient clipping to the range [-5, 5].\n",
    "    for dparam in [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "    return (loss, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
    "            hs[len(inputs)-1], cs[len(inputs)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fca46a3b-8d53-4ec2-a408-7e846261f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, c, seed_ix, n):\n",
    "    \"\"\"Sample a sequence of integers from the model.\n",
    "\n",
    "    Runs the LSTM in forward mode for n steps; seed_ix is the seed letter for\n",
    "    the first time step, h and c are the memory state. Returns a sequence of\n",
    "    letters produced by the model (indices).\n",
    "    \"\"\"\n",
    "    x = np.zeros((V, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "\n",
    "    for t in range(n):\n",
    "        # Run the forward pass only.\n",
    "        xh = np.vstack((x, h))\n",
    "        fg = sigmoid(np.dot(Wf, xh) + bf)\n",
    "        ig = sigmoid(np.dot(Wi, xh) + bi)\n",
    "        og = sigmoid(np.dot(Wo, xh) + bo)\n",
    "        cc = np.tanh(np.dot(Wcc, xh) + bcc)\n",
    "        c = fg * c + ig * cc\n",
    "        h = np.tanh(c) * og\n",
    "        y = np.dot(Wy, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "        # Sample from the distribution produced by softmax.\n",
    "        ix = np.random.choice(range(V), p=p.ravel())\n",
    "        x = np.zeros((V, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e39df23-5a9d-45a6-8dc1-b6d8f63df451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradCheck(inputs, targets, hprev, cprev):\n",
    "    global Wf, Wi, bf, bi, Wcc, bcc, Wo, bo, Wy, by\n",
    "    num_checks, delta = 10, 1e-5\n",
    "    (_, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
    "     _, _) = lossFun(inputs, targets, hprev, cprev)\n",
    "    for param, dparam, name in zip(\n",
    "            [Wf, bf, Wi, bi, Wcc, bcc, Wo, bo, Wy, by],\n",
    "            [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby],\n",
    "            ['Wf', 'bf', 'Wi', 'bi', 'Wcc', 'bcc', 'Wo', 'bo', 'Wy', 'by']):\n",
    "        assert dparam.shape == param.shape\n",
    "        print(name)\n",
    "        for i in range(num_checks):\n",
    "            ri = np.random.randint(0, param.size)\n",
    "            old_val = param.flat[ri]\n",
    "            param.flat[ri] = old_val + delta\n",
    "            numloss0 = lossFun(inputs, targets, hprev, cprev)[0]\n",
    "            param.flat[ri] = old_val - delta\n",
    "            numloss1 = lossFun(inputs, targets, hprev, cprev)[0]\n",
    "            param.flat[ri] = old_val # reset\n",
    "            grad_analytic = dparam.flat[ri]\n",
    "            grad_numerical = (numloss0 - numloss1) / (2 * delta)\n",
    "            if grad_numerical + grad_analytic == 0:\n",
    "                rel_error = 0\n",
    "            else:\n",
    "                rel_error = (abs(grad_analytic - grad_numerical) /\n",
    "                             abs(grad_numerical + grad_analytic))\n",
    "            print('%s, %s => %e' % (grad_numerical, grad_analytic, rel_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127fbfa5-07a4-4833-aa01-aada52bff9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " kŠŸ#f0jX)KOX¦PD˜<f,Oœ/<pyl&Gc2CâŽ€QX(K&rk.Š,'su!K6˜O‹8mpexo\"t4Q,V(Y±©v”-oRCTfH.˜¦1*vŽXIE!#±aHf.âBV+g6˜j™+e¦swxc?©.8uSÃŠ#:ld4Ksðcg/MEw\n",
      "n8”!Uv€ð”œM2#Jy™.yjZJÃ&0my1I38?P2m,m-ÂSemPŠp:A”&+ycNœt ÃjgB;\"\n",
      "jâT“ \n",
      "----\n",
      "iter 0 (p=0), loss 73.841930\n",
      "iter 200 (p=3200), loss 68.078371\n",
      "iter 400 (p=6400), loss 61.791523\n",
      "iter 600 (p=9600), loss 56.126380\n",
      "iter 800 (p=12800), loss 51.381767\n",
      "----\n",
      " . Tht aloogd Fos. She purlid to it to gaike mot foll ball soot all relpetola, alt. L to very thiy loveds, to he ca 'not bit alled tim ttie tot namg lammd the ander ailt, sange of and all. Tam ans vely \n",
      "----\n",
      "iter 1000 (p=16000), loss 47.197665\n",
      "iter 1200 (p=19200), loss 43.757579\n",
      "iter 1400 (p=22400), loss 40.694380\n",
      "iter 1600 (p=25600), loss 38.098688\n",
      "iter 1800 (p=28800), loss 36.109759\n",
      "----\n",
      "  But a now bould in him kimn the wut him smikedn remhen wister has pVme. Timty hemmed theret hw the and the cakrow tild fart put moy bech. Timcy thos was dald bot hamy and him trainke momed dik mV. He \n",
      "----\n",
      "iter 2000 (p=32000), loss 34.510898\n",
      "iter 2200 (p=35200), loss 33.160549\n",
      "iter 2400 (p=38400), loss 31.708361\n",
      "iter 2600 (p=41600), loss 30.541802\n",
      "iter 2800 (p=44800), loss 29.423138\n",
      "----\n",
      " hit ett tray, Timmy on it map oLars trude papping was momer too cari tows shen. \n",
      "\n",
      "Žhan it was a lrounrt. He wanged. She was nobhen mopetned and that the foull. He was mais, \"\"Ded Miked herze to see un \n",
      "----\n",
      "iter 3000 (p=48000), loss 28.904127\n",
      "iter 3200 (p=51200), loss 27.847688\n",
      "iter 3400 (p=54400), loss 27.241693\n",
      "iter 3600 (p=57600), loss 26.303838\n",
      "iter 3800 (p=60800), loss 26.264440\n",
      "----\n",
      " mily one day!naRe. But becw thaid on, y\"Thais kencited every neellore fared there went were hin man und to plry back cale.\"\n",
      "\"Once upon a wife. She kent hind dexd.\"\n",
      "\"Once upon to coteburery's becale. S \n",
      "----\n",
      "iter 4000 (p=64000), loss 25.684021\n",
      "iter 4200 (p=67200), loss 25.089394\n",
      "iter 4400 (p=70400), loss 24.755384\n",
      "iter 4600 (p=73600), loss 24.397641\n",
      "iter 4800 (p=76800), loss 23.960035\n",
      "----\n",
      " en it ackide.\n",
      "\n",
      "\n",
      "\n",
      "Lily's to sware toud his primed Lily loved to garld happy spifed togethers and har go play whit she snemand with he some toing whating dock. Hill day. Spoke that. Bound he mould Lily  \n",
      "----\n",
      "iter 5000 (p=80000), loss 23.498081\n",
      "iter 5200 (p=83200), loss 23.335943\n"
     ]
    }
   ],
   "source": [
    "def basicGradCheck():\n",
    "    inputs = [char_to_ix[ch] for ch in data[:seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[1:seq_length+1]]\n",
    "    hprev = np.random.randn(H, 1)\n",
    "    cprev = np.random.randn(H, 1)\n",
    "    gradCheck(inputs, targets, hprev, cprev)\n",
    "\n",
    "# Uncomment this to run gradient checking instead of training\n",
    "#basicGradCheck()\n",
    "#sys.exit()\n",
    "\n",
    "# n is the iteration counter; p is the input sequence pointer, at the beginning\n",
    "# of each step it points at the sequence in the input that will be used for\n",
    "# training this iteration.\n",
    "n, p = 0, 0\n",
    "\n",
    "# Memory variables for Adagrad.\n",
    "mWf = np.zeros_like(Wf)\n",
    "mbf = np.zeros_like(bf)\n",
    "mWi = np.zeros_like(Wi)\n",
    "mbi = np.zeros_like(bi)\n",
    "mWcc = np.zeros_like(Wcc)\n",
    "mbcc = np.zeros_like(bcc)\n",
    "mWo = np.zeros_like(Wo)\n",
    "mbo = np.zeros_like(bo)\n",
    "mWy = np.zeros_like(Wy)\n",
    "mby = np.zeros_like(by)\n",
    "smooth_loss = -np.log(1.0/V) * seq_length\n",
    "\n",
    "while p < MAX_DATA:\n",
    "    # Prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        # Reset RNN memory\n",
    "        hprev = np.zeros((H, 1))\n",
    "        cprev = np.zeros((H, 1))\n",
    "        p = 0 # go from start of data\n",
    "\n",
    "    # In each step we unroll the RNN for seq_length cells, and present it with\n",
    "    # seq_length inputs and seq_length target outputs to learn.\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # Sample from the model now and then.\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, cprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "    # Forward seq_length characters through the RNN and fetch gradient.\n",
    "    (loss, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
    "     hprev, cprev) = lossFun(inputs, targets, hprev, cprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 200 == 0:\n",
    "        print('iter %d (p=%d), loss %f' % (n, p, smooth_loss))\n",
    "\n",
    "    # Perform parameter update with Adagrad.\n",
    "    for param, dparam, mem in zip(\n",
    "            [Wf, bf, Wi, bi, Wcc, bcc, Wo, bo, Wy, by],\n",
    "            [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby],\n",
    "            [mWf, mbf, mWi, mbi, mWcc, mbcc, mWo, mbo, mWy, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "    p += seq_length\n",
    "    n += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
