{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "803525d9-d13f-499e-b771-afcae11022db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b0d6f66-ac20-4696-991d-6a1ca0bfa4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/validation.txt'\n",
    "with open(filename, 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97256cc7-0a35-4b94-8fcc-970ab0309388",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data))\n",
    "V = vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9e5a487-c085-4ebe-aec7-02b743e34e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19349777"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "637b25a7-8250-408a-816a-08de66f506a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 19349777 characters, 101 unique.\n"
     ]
    }
   ],
   "source": [
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dad4b912-037e-49eb-8da9-fe2acb27a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_to_ix {'Â': 0, 'm': 1, '2': 2, 'I': 3, 'C': 4, \"'\": 5, 'q': 6, '\\n': 7, 'R': 8, 'P': 9, 'c': 10, 'E': 11, '?': 12, 'M': 13, 'V': 14, 'O': 15, ' ': 16, 'k': 17, '4': 18, '3': 19, 'l': 20, 'i': 21, '1': 22, 'T': 23, 'â': 24, 'N': 25, 'r': 26, 'Ã': 27, 'y': 28, '0': 29, 'D': 30, '´': 31, '7': 32, '€': 33, '“': 34, 'a': 35, 'e': 36, '9': 37, '*': 38, 'U': 39, 'p': 40, 'z': 41, ';': 42, 'Q': 43, 'u': 44, '<': 45, 'Š': 46, 'X': 47, 'n': 48, 'J': 49, 'h': 50, 'w': 51, 'L': 52, '˜': 53, 'Ž': 54, 't': 55, '\"': 56, ')': 57, '.': 58, 'b': 59, ',': 60, '-': 61, '‹': 62, '$': 63, '/': 64, 'f': 65, 'j': 66, 'Y': 67, 'x': 68, '!': 69, '”': 70, 's': 71, 'B': 72, 'K': 73, 'v': 74, 'A': 75, 'ð': 76, ':': 77, '5': 78, 'G': 79, '&': 80, '+': 81, '©': 82, '™': 83, 'd': 84, 'S': 85, '6': 86, '#': 87, 'H': 88, 'œ': 89, 'Ÿ': 90, 'W': 91, '8': 92, '(': 93, 'g': 94, '±': 95, '¦': 96, '\\xad': 97, 'o': 98, 'Z': 99, 'F': 100}\n",
      "ix_to_char {0: 'Â', 1: 'm', 2: '2', 3: 'I', 4: 'C', 5: \"'\", 6: 'q', 7: '\\n', 8: 'R', 9: 'P', 10: 'c', 11: 'E', 12: '?', 13: 'M', 14: 'V', 15: 'O', 16: ' ', 17: 'k', 18: '4', 19: '3', 20: 'l', 21: 'i', 22: '1', 23: 'T', 24: 'â', 25: 'N', 26: 'r', 27: 'Ã', 28: 'y', 29: '0', 30: 'D', 31: '´', 32: '7', 33: '€', 34: '“', 35: 'a', 36: 'e', 37: '9', 38: '*', 39: 'U', 40: 'p', 41: 'z', 42: ';', 43: 'Q', 44: 'u', 45: '<', 46: 'Š', 47: 'X', 48: 'n', 49: 'J', 50: 'h', 51: 'w', 52: 'L', 53: '˜', 54: 'Ž', 55: 't', 56: '\"', 57: ')', 58: '.', 59: 'b', 60: ',', 61: '-', 62: '‹', 63: '$', 64: '/', 65: 'f', 66: 'j', 67: 'Y', 68: 'x', 69: '!', 70: '”', 71: 's', 72: 'B', 73: 'K', 74: 'v', 75: 'A', 76: 'ð', 77: ':', 78: '5', 79: 'G', 80: '&', 81: '+', 82: '©', 83: '™', 84: 'd', 85: 'S', 86: '6', 87: '#', 88: 'H', 89: 'œ', 90: 'Ÿ', 91: 'W', 92: '8', 93: '(', 94: 'g', 95: '±', 96: '¦', 97: '\\xad', 98: 'o', 99: 'Z', 100: 'F'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i, ch in enumerate(chars)}\n",
    "print('char_to_ix', char_to_ix)\n",
    "print('ix_to_char', ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49d73ca7-b3ec-4236-909e-89717624e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of hidden state vectors; applies to h and c.\n",
    "H = hidden_size = 100\n",
    "seq_length = 16 # number of steps to unroll the LSTM for\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e58fbe51-4327-4e43-9fc0-a35add62af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input x is concatenated with state h\n",
    "# The joined vector is used to feed into most blocks within the LSTM cell\n",
    "HV = H + V # (100 + 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53b63f29-db16-4d3c-82ec-c6e569bb47da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dae113e6-e21a-45c8-98a6-4ce6d35ad7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DATA = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f9daf5d-fe1a-4658-94c5-18062b243741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters/weights -- these are shared among all steps\n",
    "# Inputs are characters one-hot encoded in a vocab-sized vector. (1, vocab_size)\n",
    "# Dimensions: H = hidden_size, V = vocab_size, HV = hidden_size + vocab_size\n",
    "\n",
    "Wf = np.random.randn(H, HV) * 0.01 # (100, 165)\n",
    "bf = np.zeros((H, 1)) # (100, 1)\n",
    "Wi = np.random.randn(H, HV) * 0.01 # (100, 165)\n",
    "bi = np.zeros((H, 1)) # (100, 1)\n",
    "Wcc = np.random.randn(H, HV) * 0.01 # (100, 165)\n",
    "bcc = np.zeros((H, 1)) # (100, 1)\n",
    "Wo = np.random.randn(H, HV) * 0.01 # (100, 165)\n",
    "bo = np.zeros((H, 1)) # (100, 1)\n",
    "Wy = np.random.randn(V, H) * 0.01 # (65, 100)\n",
    "by = np.zeros((V, 1)) # (100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa8c9c14-4d53-446c-a9ae-0465c7fda7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Computes sigmoid function.\n",
    "\n",
    "    z: array of input values.\n",
    "\n",
    "    Returns array of outputs, sigmoid(z).\n",
    "    \"\"\"\n",
    "    # Note: this version of sigmoid tries to avoid overflows in the computation\n",
    "    # of e^(-z), by using an alternative formulation when z is negative, to get\n",
    "    # 0. e^z / (1+e^z) is equivalent to the definition of sigmoid, but we won't\n",
    "    # get e^(-z) to overflow when z is very negative.\n",
    "    # Since both the x and y arguments to np.where are evaluated by Python, we\n",
    "    # may still get overflow warnings for large z elements; therefore we ignore\n",
    "    # warnings during this computation.\n",
    "    with np.errstate(over='ignore', invalid='ignore'):\n",
    "        return np.where(z >= 0,\n",
    "                        1 / (1 + np.exp(-z)),\n",
    "                        np.exp(z) / (1 + np.exp(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4fd07c7e-dd95-4fe6-8bfa-e0a381e951e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev, cprev):\n",
    "    \"\"\"Runs forward and backward passes through the RNN.\n",
    "      inputs, targets: Lists of integers. For some i, inputs[i] is the input\n",
    "                       character (encoded as an index into the ix_to_char map)\n",
    "                       and targets[i] is the corresponding next character in the\n",
    "                       training data (similarly encoded).\n",
    "      hprev: Hx1 array of initial hidden state (column vector)\n",
    "      cprev: Hx1 array of initial hidden state (column vector)\n",
    "      returns: loss, gradients on model parameters, and last hidden states\n",
    "    \"\"\"\n",
    "    # Caches that keep values computed in the forward pass at each time step, to be reused in the backward pass.\n",
    "    # Initialize as dictionaries to be indexed with time steps\n",
    "    xs, xhs, ys, ps, hs, cs, fgs, igs, ccs, ogs = (\n",
    "            {}, {}, {}, {}, {}, {}, {}, {}, {}, {})\n",
    "\n",
    "    # Initial incoming states (from time step = -1)\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    cs[-1] = np.copy(cprev)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # Forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        # Input at time step t is xs[t]. Prepare a one-hot encoded vector of\n",
    "        # shape (V, 1). inputs[t] is the index where the 1 goes.\n",
    "        xs[t] = np.zeros((V, 1)) # (65, 1) \n",
    "        xs[t][inputs[t]] = 1 # (65, 1) -> column vector\n",
    "\n",
    "        # hprev and xs[t] are column vector; stack them together into a \"taller\"\n",
    "        # column vector - first the elements of x, then h.\n",
    "        xhs[t] = np.vstack((xs[t], hs[t-1])) # (165, 1) -> column vector\n",
    "\n",
    "        # Gates f, i and o\n",
    "        fgs[t] = sigmoid(np.dot(Wf, xhs[t]) + bf) # (100, 165) @ (165, 1) = (100, 1)\n",
    "        igs[t] = sigmoid(np.dot(Wi, xhs[t]) + bi) # (100, 165) @ (165, 1) = (100, 1)\n",
    "        ogs[t] = sigmoid(np.dot(Wo, xhs[t]) + bo) # (100, 165) @ (165, 1) = (100, 1)\n",
    "\n",
    "        # Candidate cc\n",
    "        ccs[t] = np.tanh(np.dot(Wcc, xhs[t]) + bcc) # (100, 165) @ (165, 1) = (100, 1)\n",
    "\n",
    "        # This step's h and c\n",
    "        cs[t] = fgs[t] * cs[t-1] + igs[t] * ccs[t] # (100, 1) * (100, 1) + (100, 1) * (100, 1)\n",
    "        hs[t] = np.tanh(cs[t]) * ogs[t] # (100, 1)\n",
    "\n",
    "        # Convert to output\n",
    "        ys[t] = np.dot(Wy, hs[t]) + by # (65, 100) @ (100, 1) = (65, 1)\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # (65, 1)\n",
    "\n",
    "        # Cross-entropy loss.\n",
    "        loss += -np.log(ps[t][targets[t], 0]) # (65, 1)\n",
    "\n",
    "    # Initialize gradients of all weights/biases to 0.\n",
    "    # We only need to compute 5 matrices in backpropogation!\n",
    "    dWf = np.zeros_like(Wf)\n",
    "    dbf = np.zeros_like(bf)\n",
    "    \n",
    "    dWi = np.zeros_like(Wi)\n",
    "    dbi = np.zeros_like(bi)\n",
    "    \n",
    "    dWcc = np.zeros_like(Wcc)\n",
    "    dbcc = np.zeros_like(bcc)\n",
    "    \n",
    "    dWo = np.zeros_like(Wo)\n",
    "    dbo = np.zeros_like(bo)\n",
    "    \n",
    "    dWy = np.zeros_like(Wy)\n",
    "    dby = np.zeros_like(by)\n",
    "\n",
    "    # Incoming gradients for h and c; for backwards loop step these represent\n",
    "    # dh[t] and dc[t]; we do truncated BPTT, so assume they are 0 initially.\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dcnext = np.zeros_like(cs[0])\n",
    "    \n",
    "    # The backwards pass iterates over the input sequence backwards.\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backprop through the gradients of loss and softmax.\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1\n",
    "\n",
    "        # Compute gradients for the Wy and by parameters.\n",
    "        dWy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "\n",
    "        # Backprop through the fully-connected layer (Wy, by) to h. Also add up\n",
    "        # the incoming gradient for h from the next cell.\n",
    "        dh = np.dot(Wy.T, dy) + dhnext\n",
    "\n",
    "        # Backprop through multiplication with output gate; here \"dtanh\" means\n",
    "        # the gradient at the output of tanh.\n",
    "        dctanh = ogs[t] * dh\n",
    "        # Backprop through the tanh function; since cs[t] branches in two\n",
    "        # directions we add dcnext too.\n",
    "        dc = dctanh * (1 - np.tanh(cs[t]) ** 2) + dcnext\n",
    "\n",
    "        # Backprop through multiplication with the tanh; here \"dhogs\" means\n",
    "        # the gradient at the output of the sigmoid of the output gate. Then\n",
    "        # backprop through the sigmoid itself (ogs[t] is the sigmoid output).\n",
    "        dhogs = dh * np.tanh(cs[t])\n",
    "        dho = dhogs * ogs[t] * (1 - ogs[t])\n",
    "\n",
    "        # Compute gradients for the output gate parameters.\n",
    "        dWo += np.dot(dho, xhs[t].T)\n",
    "        dbo += dho\n",
    "\n",
    "        # Backprop dho to the xh input.\n",
    "        dxh_from_o = np.dot(Wo.T, dho)\n",
    "\n",
    "        # Backprop through the forget gate: sigmoid and elementwise mul.\n",
    "        dhf = cs[t-1] * dc * fgs[t] * (1 - fgs[t])\n",
    "        dWf += np.dot(dhf, xhs[t].T)\n",
    "        dbf += dhf\n",
    "        dxh_from_f = np.dot(Wf.T, dhf)\n",
    "\n",
    "        # Backprop through the input gate: sigmoid and elementwise mul.\n",
    "        dhi = ccs[t] * dc * igs[t] * (1 - igs[t])\n",
    "        dWi += np.dot(dhi, xhs[t].T)\n",
    "        dbi += dhi\n",
    "        dxh_from_i = np.dot(Wi.T, dhi)\n",
    "\n",
    "        dhcc = igs[t] * dc * (1 - ccs[t] ** 2)\n",
    "        dWcc += np.dot(dhcc, xhs[t].T)\n",
    "        dbcc += dhcc\n",
    "        dxh_from_cc = np.dot(Wcc.T, dhcc)\n",
    "\n",
    "        # Combine all contributions to dxh, and extract the gradient for the\n",
    "        # h part to propagate backwards as dhnext.\n",
    "        dxh = dxh_from_o + dxh_from_f + dxh_from_i + dxh_from_cc\n",
    "        dhnext = dxh[V:, :]\n",
    "\n",
    "        # dcnext from dc and the forget gate.\n",
    "        dcnext = fgs[t] * dc\n",
    "\n",
    "    # Gradient clipping to the range [-5, 5].\n",
    "    for dparam in [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "    return (loss, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
    "            hs[len(inputs)-1], cs[len(inputs)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fca46a3b-8d53-4ec2-a408-7e846261f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, c, seed_ix, n):\n",
    "    \"\"\"Sample a sequence of integers from the model.\n",
    "\n",
    "    Runs the LSTM in forward mode for n steps; seed_ix is the seed letter for\n",
    "    the first time step, h and c are the memory state. Returns a sequence of\n",
    "    letters produced by the model (indices).\n",
    "    \"\"\"\n",
    "    x = np.zeros((V, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "\n",
    "    for t in range(n):\n",
    "        # Run the forward pass only.\n",
    "        xh = np.vstack((x, h))\n",
    "        fg = sigmoid(np.dot(Wf, xh) + bf)\n",
    "        ig = sigmoid(np.dot(Wi, xh) + bi)\n",
    "        og = sigmoid(np.dot(Wo, xh) + bo)\n",
    "        cc = np.tanh(np.dot(Wcc, xh) + bcc)\n",
    "        c = fg * c + ig * cc\n",
    "        h = np.tanh(c) * og\n",
    "        y = np.dot(Wy, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "        # Sample from the distribution produced by softmax.\n",
    "        ix = np.random.choice(range(V), p=p.ravel())\n",
    "        x = np.zeros((V, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e39df23-5a9d-45a6-8dc1-b6d8f63df451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradCheck(inputs, targets, hprev, cprev):\n",
    "    global Wf, Wi, bf, bi, Wcc, bcc, Wo, bo, Wy, by\n",
    "    num_checks, delta = 10, 1e-5\n",
    "    (_, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
    "     _, _) = lossFun(inputs, targets, hprev, cprev)\n",
    "    for param, dparam, name in zip(\n",
    "            [Wf, bf, Wi, bi, Wcc, bcc, Wo, bo, Wy, by],\n",
    "            [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby],\n",
    "            ['Wf', 'bf', 'Wi', 'bi', 'Wcc', 'bcc', 'Wo', 'bo', 'Wy', 'by']):\n",
    "        assert dparam.shape == param.shape\n",
    "        print(name)\n",
    "        for i in range(num_checks):\n",
    "            ri = np.random.randint(0, param.size)\n",
    "            old_val = param.flat[ri]\n",
    "            param.flat[ri] = old_val + delta\n",
    "            numloss0 = lossFun(inputs, targets, hprev, cprev)[0]\n",
    "            param.flat[ri] = old_val - delta\n",
    "            numloss1 = lossFun(inputs, targets, hprev, cprev)[0]\n",
    "            param.flat[ri] = old_val # reset\n",
    "            grad_analytic = dparam.flat[ri]\n",
    "            grad_numerical = (numloss0 - numloss1) / (2 * delta)\n",
    "            if grad_numerical + grad_analytic == 0:\n",
    "                rel_error = 0\n",
    "            else:\n",
    "                rel_error = (abs(grad_analytic - grad_numerical) /\n",
    "                             abs(grad_numerical + grad_analytic))\n",
    "            print('%s, %s => %e' % (grad_numerical, grad_analytic, rel_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "127fbfa5-07a4-4833-aa01-aada52bff9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " kŠŸ#f0jX)KOX¦PD˜<f,Oœ/<pyl&Gc2CâŽ€QX(K&rk.Š,'su!K6˜O‹8mpexo\"t4Q,V(Y±©v”-oRCTfH.˜¦1*vŽXIE!#±aHf.âBV+g6˜j™+e¦swxc?©.8uSÃŠ#:ld4Ksðcg/MEw\n",
      "n8”!Uv€ð”œM2#Jy™.yjZJÃ&0my1I38?P2m,m-ÂSemPŠp:A”&+ycNœt ÃjgB;\"\n",
      "jâT“ \n",
      "----\n",
      "iter 0 (p=0), loss 73.841930\n",
      "iter 200 (p=3200), loss 68.078371\n",
      "iter 400 (p=6400), loss 61.791523\n",
      "iter 600 (p=9600), loss 56.126380\n",
      "iter 800 (p=12800), loss 51.381767\n",
      "----\n",
      " . Tht aloogd Fos. She purlid to it to gaike mot foll ball soot all relpetola, alt. L to very thiy loveds, to he ca 'not bit alled tim ttie tot namg lammd the ander ailt, sange of and all. Tam ans vely \n",
      "----\n",
      "iter 1000 (p=16000), loss 47.197665\n",
      "iter 1200 (p=19200), loss 43.757579\n",
      "iter 1400 (p=22400), loss 40.694380\n",
      "iter 1600 (p=25600), loss 38.098688\n",
      "iter 1800 (p=28800), loss 36.109759\n",
      "----\n",
      "  But a now bould in him kimn the wut him smikedn remhen wister has pVme. Timty hemmed theret hw the and the cakrow tild fart put moy bech. Timcy thos was dald bot hamy and him trainke momed dik mV. He \n",
      "----\n",
      "iter 2000 (p=32000), loss 34.510898\n",
      "iter 2200 (p=35200), loss 33.160549\n",
      "iter 2400 (p=38400), loss 31.708361\n",
      "iter 2600 (p=41600), loss 30.541802\n",
      "iter 2800 (p=44800), loss 29.423138\n",
      "----\n",
      " hit ett tray, Timmy on it map oLars trude papping was momer too cari tows shen. \n",
      "\n",
      "Žhan it was a lrounrt. He wanged. She was nobhen mopetned and that the foull. He was mais, \"\"Ded Miked herze to see un \n",
      "----\n",
      "iter 3000 (p=48000), loss 28.904127\n",
      "iter 3200 (p=51200), loss 27.847688\n",
      "iter 3400 (p=54400), loss 27.241693\n",
      "iter 3600 (p=57600), loss 26.303838\n",
      "iter 3800 (p=60800), loss 26.264440\n",
      "----\n",
      " mily one day!naRe. But becw thaid on, y\"Thais kencited every neellore fared there went were hin man und to plry back cale.\"\n",
      "\"Once upon a wife. She kent hind dexd.\"\n",
      "\"Once upon to coteburery's becale. S \n",
      "----\n",
      "iter 4000 (p=64000), loss 25.684021\n",
      "iter 4200 (p=67200), loss 25.089394\n",
      "iter 4400 (p=70400), loss 24.755384\n",
      "iter 4600 (p=73600), loss 24.397641\n",
      "iter 4800 (p=76800), loss 23.960035\n",
      "----\n",
      " en it ackide.\n",
      "\n",
      "\n",
      "\n",
      "Lily's to sware toud his primed Lily loved to garld happy spifed togethers and har go play whit she snemand with he some toing whating dock. Hill day. Spoke that. Bound he mould Lily  \n",
      "----\n",
      "iter 5000 (p=80000), loss 23.498081\n",
      "iter 5200 (p=83200), loss 23.335943\n",
      "iter 5400 (p=86400), loss 23.155597\n",
      "iter 5600 (p=89600), loss 22.920370\n",
      "iter 5800 (p=92800), loss 22.767909\n",
      "----\n",
      " o the roron a geadoladede?\"\" She played to mot smilk rothappy.\"\n",
      "\"Once upon a time, there was they were fell a pucpd beaghis, Sual mum, she fonghs liddise and sas, \"\"Ilyy. Cacken ap.h it to mack. One d \n",
      "----\n",
      "iter 6000 (p=96000), loss 22.699477\n",
      "iter 6200 (p=99200), loss 22.653226\n",
      "iter 6400 (p=102400), loss 22.552299\n",
      "iter 6600 (p=105600), loss 22.317096\n",
      "iter 6800 (p=108800), loss 22.265999\n",
      "----\n",
      " it pousnes, all thin that him to do with her she fele wasca the ppant and her froc and fould\n",
      "'s ald it was dres.\"\n",
      "\"Onie upongally aske.\"\" Lily fever to gew ligple somer mom lead with san bis fourd and \n",
      "----\n",
      "iter 7000 (p=112000), loss 22.389764\n",
      "iter 7200 (p=115200), loss 22.097299\n",
      "iter 7400 (p=118400), loss 21.715010\n",
      "iter 7600 (p=121600), loss 21.763507\n",
      "iter 7800 (p=124800), loss 21.688168\n",
      "----\n",
      " some to slowentiely aid the for. She was a big fover that he round chings the bornd sto the dralpeparfed the corlave over toom?\"\n",
      "\"Once upon a time, there was an elove tllievtes friend, pvound thy gog  \n",
      "----\n",
      "iter 8000 (p=128000), loss 21.276521\n",
      "iter 8200 (p=131200), loss 21.187212\n",
      "iter 8400 (p=134400), loss 21.265802\n",
      "iter 8600 (p=137600), loss 21.864366\n",
      "iter 8800 (p=140800), loss 22.059427\n",
      "----\n",
      "  that thein picimes.\n",
      "\n",
      "Annt a sporsail time, Timile s\",.\n",
      "\n",
      "They that he seaplas.\n",
      "\"\"\n",
      "Lot very shen. They hed to to junce sees came whem did her cor ofneased and did help was noved the stunt. She shill! S \n",
      "----\n",
      "iter 9000 (p=144000), loss 22.174745\n",
      "iter 9200 (p=147200), loss 22.514650\n",
      "iter 9400 (p=150400), loss 22.666419\n",
      "iter 9600 (p=153600), loss 22.508821\n",
      "iter 9800 (p=156800), loss 22.533240\n",
      "----\n",
      " ca. She said. Ben beving. He pyoud. She sees. It had stelcile you. She hidsed the anry.\n",
      "\n",
      "\"\"Yes, and it foos, up. He kisk.\n",
      "\n",
      "\"\"Weas koow min whatre. The stelpas with yenoo feal. It was loa to.\n",
      "\n",
      "\"\"Yomo,  \n",
      "----\n",
      "iter 10000 (p=160000), loss 22.733991\n",
      "iter 10200 (p=163200), loss 22.489230\n",
      "iter 10400 (p=166400), loss 22.236286\n",
      "iter 10600 (p=169600), loss 22.217550\n",
      "iter 10800 (p=172800), loss 22.295238\n",
      "----\n",
      " ed anwad, the cace. They give clomisen. \"\"They sods a look pread, and they not was but is afeing. One day, Wame dohe. They got youir. Em. They! They went the funny picalred ins crammang?\"\"\n",
      "\n",
      "Lily andm. \n",
      "----\n",
      "iter 11000 (p=176000), loss 22.408374\n",
      "iter 11200 (p=179200), loss 22.244335\n",
      "iter 11400 (p=182400), loss 22.200779\n",
      "iter 11600 (p=185600), loss 22.305127\n",
      "iter 11800 (p=188800), loss 22.335818\n",
      "----\n",
      " Sar whore was push.\n",
      "\n",
      "The awny and said. \"\"Bun a stow cooly.\n",
      "\n",
      "\"\"Nou, Max and did nice withing rad a smelilled to seir onf said, \"\"Frian do playing you mace and try parply. Thing on it. They huld withe  \n",
      "----\n",
      "iter 12000 (p=192000), loss 22.167320\n",
      "iter 12200 (p=195200), loss 22.185908\n",
      "iter 12400 (p=198400), loss 22.110613\n",
      "iter 12600 (p=201600), loss 22.009130\n",
      "iter 12800 (p=204800), loss 21.888114\n",
      "----\n",
      " ide to things,\"\" very ith the cking. It wouge and my brank carsed youn, shayeatad. They a butt. He was porajpur. You bema mushe\"! He dedeefe and many apgumemot awaying. You ald he canezy the hard. \"\"C \n",
      "----\n",
      "iter 13000 (p=208000), loss 22.112821\n",
      "iter 13200 (p=211200), loss 22.095623\n",
      "iter 13400 (p=214400), loss 21.746914\n",
      "iter 13600 (p=217600), loss 20.902198\n",
      "iter 13800 (p=220800), loss 21.254740\n",
      "----\n",
      " trens gegfors to namete fallede and mioks Lily bing to be the storn, whire stive and say to pattmy. Them was Lily and saw her my sealt make airoon, Timmy was nop to Lily. It was tadidy on! She ligrong \n",
      "----\n",
      "iter 14000 (p=224000), loss 21.295319\n",
      "iter 14200 (p=227200), loss 21.154514\n",
      "iter 14400 (p=230400), loss 20.794226\n",
      "iter 14600 (p=233600), loss 20.909600\n",
      "iter 14800 (p=236800), loss 20.580291\n",
      "----\n",
      "  warted to othing her faded Timmy and said.\n",
      "\n",
      "One day, Lily asked ard nom smiel. \"\"WhhSot said, \n",
      "\n",
      "Lily was a taych a fun. Lily friends mas it'g thet decately her that they zourde's mommy. She lithles a \n",
      "----\n",
      "iter 15000 (p=240000), loss 20.529905\n",
      "iter 15200 (p=243200), loss 20.585656\n",
      "iter 15400 (p=246400), loss 20.353797\n",
      "iter 15600 (p=249600), loss 20.644252\n",
      "iter 15800 (p=252800), loss 20.435930\n",
      "----\n",
      " tes with winging in the room nown:s a little bard anipmered the bead they den on onean best the breil wro fill. Thened.\"\n",
      "\"Onca upon a time, there was a little girl nimed to sed hec they porse him same \n",
      "----\n",
      "iter 16000 (p=256000), loss 20.197678\n",
      "iter 16200 (p=259200), loss 20.227170\n",
      "iter 16400 (p=262400), loss 19.786889\n",
      "iter 16600 (p=265600), loss 19.982192\n",
      "iter 16800 (p=268800), loss 19.909269\n",
      "----\n",
      " ark\n",
      "\"\"\n",
      "Mhe felt realized got farice on a big all moo unoon, Sue lave if the bish what them in the little soff with her and playing for love, Lily parked for to the hamay and axawore to gogeful.\"\n",
      "\"Once \n",
      "----\n",
      "iter 17000 (p=272000), loss 20.016093\n",
      "iter 17200 (p=275200), loss 19.908264\n",
      "iter 17400 (p=278400), loss 19.730042\n",
      "iter 17600 (p=281600), loss 20.017354\n",
      "iter 17800 (p=284800), loss 20.039379\n",
      "----\n",
      "  He plowelcive wint trar. He still but ot were it and same could. Lily asall scelly like the din. Tears was sometring. Suddenls, pancy it for tower all wat swook the chingirr towed thes help you agaci \n",
      "----\n",
      "iter 18000 (p=288000), loss 20.314212\n",
      "iter 18200 (p=291200), loss 20.188780\n",
      "iter 18400 (p=294400), loss 20.175405\n",
      "iter 18600 (p=297600), loss 20.012542\n",
      "iter 18800 (p=300800), loss 19.999115\n",
      "----\n",
      " aclow toys firht and hurts swy ale in a named to hing and hows to help and corians. \n",
      "\n",
      "From than we, One day, Lily loved and stank and very choug took her!\"\" Lily net to him in hir fruggle the lick dag \n",
      "----\n",
      "iter 19000 (p=304000), loss 19.843465\n",
      "iter 19200 (p=307200), loss 19.786591\n",
      "iter 19400 (p=310400), loss 19.581287\n",
      "iter 19600 (p=313600), loss 19.507675\n",
      "iter 19800 (p=316800), loss 19.506519\n",
      "----\n",
      " s mom asked him, veniwed. Tim could arbades. The fially, he mady sag the car. She the prall. \"Oshe putted friendsoun some his toys oon. One day, she sliden he putsed to away. But then, \"\"Ot, sue that  \n",
      "----\n",
      "iter 20000 (p=320000), loss 19.323507\n",
      "iter 20200 (p=323200), loss 19.432943\n",
      "iter 20400 (p=326400), loss 19.651968\n",
      "iter 20600 (p=329600), loss 19.646657\n",
      "iter 20800 (p=332800), loss 19.747742\n",
      "----\n",
      " nlyazues, mily. You clow tame dress. The cante caccoll got for help share again.\"\"\n",
      "\n",
      "Timmy was very man dank and loved to Tweare I pully was not good thmy was docelow the friend to bake too a little be \n",
      "----\n",
      "iter 21000 (p=336000), loss 19.830674\n",
      "iter 21200 (p=339200), loss 19.516366\n",
      "iter 21400 (p=342400), loss 19.314789\n",
      "iter 21600 (p=345600), loss 19.349341\n",
      "iter 21800 (p=348800), loss 19.273473\n",
      "----\n",
      " Timmy want to Besh to for siftret in ohe pon affe ausain. It was prothere and get dy. Hif to the ent.\"\"\n",
      "\n",
      "\"\"If was zathers and learned his toy. But they saw a big big finders and help ortist hisped to  \n",
      "----\n",
      "iter 22000 (p=352000), loss 19.178038\n",
      "iter 22200 (p=355200), loss 18.967152\n",
      "iter 22400 (p=358400), loss 19.083712\n",
      "iter 22600 (p=361600), loss 19.197179\n",
      "iter 22800 (p=364800), loss 19.883482\n",
      "----\n",
      " ttle boy named Matinning all pack alro beeco in a bittle gooy. What buth a biffelmharo help to prick. Jay and share are they slows lots to we had. Lily lave the earner weo feither darg their took the  \n",
      "----\n",
      "iter 23000 (p=368000), loss 20.542850\n",
      "iter 23200 (p=371200), loss 20.921284\n",
      "iter 23400 (p=374400), loss 21.410760\n",
      "iter 23600 (p=377600), loss 22.006023\n",
      "iter 23800 (p=380800), loss 21.951397\n",
      "----\n",
      " ed, he tried that keat woth the propen at the homed important fus porbte mallow the macy and inso a kabke he loved to kear buck stemeiengus. Sudly sat pup and fuls a loa ony uretthes runs\"\" somethin t \n",
      "----\n",
      "iter 24000 (p=384000), loss 22.322537\n",
      "iter 24200 (p=387200), loss 22.515076\n",
      "iter 24400 (p=390400), loss 22.637606\n",
      "iter 24600 (p=393600), loss 22.321978\n",
      "iter 24800 (p=396800), loss 22.415978\n",
      "----\n",
      "  to be and had tile.\n",
      "\n",
      "But he was wa hided to go,\"\"\n",
      "\n",
      "Timmy learnd, Mommy who loved to lave but up syertunt's wanted trechired. One day, she saw a becated in the doimect it. I'l mathel! Iid un and stonk \n",
      "----\n",
      "iter 25000 (p=400000), loss 22.054103\n",
      "iter 25200 (p=403200), loss 22.033732\n",
      "iter 25400 (p=406400), loss 21.953081\n",
      "iter 25600 (p=409600), loss 22.118383\n",
      "iter 25800 (p=412800), loss 22.120543\n",
      "----\n",
      " efoul €own hougs in the preting said,\"\" Sail. Jou'l \n",
      "\n",
      "J\n",
      "Sul, Miting coniied and shoughore. The und excuther was witt cole brive and togetice any crupars, they so mot thas shome fans too beftrow to str \n",
      "----\n",
      "iter 26000 (p=416000), loss 22.239008\n",
      "iter 26200 (p=419200), loss 22.293198\n",
      "iter 26400 (p=422400), loss 22.385142\n",
      "iter 26600 (p=425600), loss 22.503056\n",
      "iter 26800 (p=428800), loss 22.351578\n",
      "----\n",
      " eant, \"\"That we greet is and the dow feeling to day and spees. \n",
      "\n",
      "Ofaided inaa smou, home wame the toym. So she said, \"\"Then storn look oats.\"\n",
      "\"Once upon a time restone the nageshad herse. The buttend, \n",
      "----\n",
      "iter 27000 (p=432000), loss 22.105446\n",
      "iter 27200 (p=435200), loss 22.068175\n",
      "iter 27400 (p=438400), loss 22.014742\n",
      "iter 27600 (p=441600), loss 22.218113\n",
      "iter 27800 (p=444800), loss 22.096212\n",
      "----\n",
      " ill. \n",
      "\n",
      "He was the botturf lay, the but was very happy and say he camed hims. The day in the ling on to be he dy a little munar anymores ways! Som said it wanted to hrave boy weret out if the vooking.  \n",
      "----\n",
      "iter 28000 (p=448000), loss 22.138215\n",
      "iter 28200 (p=451200), loss 21.993742\n",
      "iter 28400 (p=454400), loss 22.072741\n",
      "iter 28600 (p=457600), loss 21.999985\n",
      "iter 28800 (p=460800), loss 21.715689\n",
      "----\n",
      " was colling litine yourchavuy the barben. When deagher sneind. Mayb's and shoaging. Jue, the tank for and going.\"\n",
      "\"Once upon a time, there was a lighlo: hurt. €I Liry wrone pary. Soy. Whe poled and al \n",
      "----\n",
      "iter 29000 (p=464000), loss 21.845082\n",
      "iter 29200 (p=467200), loss 21.658769\n",
      "iter 29400 (p=470400), loss 21.563692\n",
      "iter 29600 (p=473600), loss 21.391199\n",
      "iter 29800 (p=476800), loss 21.152991\n",
      "----\n",
      " slile of it with somethere. Ued the birded. The bird lotme excited to as the so. All gettinut are. She fly away be fust one found afouc story all the parching decided to have so haved, smilined feellu \n",
      "----\n",
      "iter 30000 (p=480000), loss 21.204439\n",
      "iter 30200 (p=483200), loss 21.261114\n",
      "iter 30400 (p=486400), loss 21.215509\n",
      "iter 30600 (p=489600), loss 21.169694\n",
      "iter 30800 (p=492800), loss 21.134310\n",
      "----\n",
      " \" Saidenhy was very preme. The hird!\n",
      "\n",
      "Sutt then scared: Rat was very encatimeride. She was clame join with go it of the Bight of the painy get seamwing to play with she hiud down the toon and scared w \n",
      "----\n",
      "iter 31000 (p=496000), loss 21.081650\n",
      "iter 31200 (p=499200), loss 20.911446\n",
      "iter 31400 (p=502400), loss 20.955768\n",
      "iter 31600 (p=505600), loss 21.116195\n",
      "iter 31800 (p=508800), loss 20.999287\n",
      "----\n",
      " t it with all the green shu rinning mililed belong, but's make in! Saratle same game. But he keed.\n",
      "\n",
      "One day, Bluy fice to fell pupping alst it big tur babam they put they were she found geren with it. \n",
      "----\n",
      "iter 32000 (p=512000), loss 20.771033\n",
      "iter 32200 (p=515200), loss 20.763994\n",
      "iter 32400 (p=518400), loss 20.698376\n",
      "iter 32600 (p=521600), loss 20.965512\n",
      "iter 32800 (p=524800), loss 20.829371\n",
      "----\n",
      " hat, not. You smile cothorf again. \n",
      "\n",
      "When she stitwy ato clean to hifs to ches from his Blacause beacter back. Jane went in the mining and the can of to play of around - gight. \n",
      "\n",
      "He waststard. It was  \n",
      "----\n",
      "iter 33000 (p=528000), loss 20.692215\n",
      "iter 33200 (p=531200), loss 20.971928\n",
      "iter 33400 (p=534400), loss 21.198114\n",
      "iter 33600 (p=537600), loss 21.380567\n",
      "iter 33800 (p=540800), loss 21.372151\n",
      "----\n",
      " y one baby have fun loget!\"\" said the now maben evee!sOup to go. Summy Jan said, \"\"We wrong together and said very grent enibble wone they felt be outhowed hal afourd to lay, had just off of the littl \n",
      "----\n",
      "iter 34000 (p=544000), loss 21.490073\n",
      "iter 34200 (p=547200), loss 21.497142\n",
      "iter 34400 (p=550400), loss 21.231941\n",
      "iter 34600 (p=553600), loss 21.119183\n",
      "iter 34800 (p=556800), loss 21.220425\n",
      "----\n",
      "  surly. He eeciased and said, \"\"Mo. He yellyes and when the boty and a so hip boy said he noticed hepp and the lived, yishornise from shase the laddy. Ks bot that can were tried to stayser. Samaw with \n",
      "----\n",
      "iter 35000 (p=560000), loss 21.307613\n",
      "iter 35200 (p=563200), loss 21.451635\n",
      "iter 35400 (p=566400), loss 21.191682\n",
      "iter 35600 (p=569600), loss 21.365575\n",
      "iter 35800 (p=572800), loss 21.331975\n",
      "----\n",
      " em, shis imers. They alsact tees ot. He was so happy the trind had going and chough up to the intule, he loved to happene. The peaple with at find had lots and had a. Sthestes said Max saf of the wott \n",
      "----\n",
      "iter 36000 (p=576000), loss 21.107856\n",
      "iter 36200 (p=579200), loss 21.103805\n",
      "iter 36400 (p=582400), loss 21.154471\n",
      "iter 36600 (p=585600), loss 21.318870\n",
      "iter 36800 (p=588800), loss 21.467946\n",
      "----\n",
      "  to smilibus roming before! It was hip toded had could. It're, he had her minall the stair, \"\"Beca, some sornd Jebout himing some gick room and he decided to love to pess what but it cound a muns!\"\" P \n",
      "----\n",
      "iter 37000 (p=592000), loss 21.300518\n",
      "iter 37200 (p=595200), loss 20.999449\n",
      "iter 37400 (p=598400), loss 20.730499\n",
      "iter 37600 (p=601600), loss 20.699276\n",
      "iter 37800 (p=604800), loss 20.041778\n",
      "----\n",
      " ca said the deepyy cololel. \"\"Ow dayele benated by is bug her afuped ailecause sun. \n",
      "\n",
      "\"\"Mes was so dacky sunged of the pank. Jains bear wanted to playoun and help you found to do sad yumiture a little \n",
      "----\n",
      "iter 38000 (p=608000), loss 19.890340\n",
      "iter 38200 (p=611200), loss 19.519721\n",
      "iter 38400 (p=614400), loss 19.306487\n",
      "iter 38600 (p=617600), loss 19.090339\n",
      "iter 38800 (p=620800), loss 18.940928\n",
      "----\n",
      " e day, Jenna lited. It was hoostard a forent a flary, Timmy will forg a civel atponester.\"\n",
      "\"Once upon a time, there was every watche, its in the mossiened to gere a hagves. \"\"Buy that's walk felt spep \n",
      "----\n",
      "iter 39000 (p=624000), loss 19.032293\n",
      "iter 39200 (p=627200), loss 18.861658\n",
      "iter 39400 (p=630400), loss 18.614432\n",
      "iter 39600 (p=633600), loss 18.415171\n",
      "iter 39800 (p=636800), loss 18.355634\n",
      "----\n",
      " y lioks poom wetter the bepf home od the book wone was so hrow to helptwa aroudd it to pell of fun three. Let's mos off!\"\n",
      "\"One day, she heary fent a my away in the bird, your.\n",
      "\n",
      "Lecy med of bunt. \"\"Tha \n",
      "----\n",
      "iter 40000 (p=640000), loss 18.444741\n",
      "iter 40200 (p=643200), loss 18.685136\n",
      "iter 40400 (p=646400), loss 18.551158\n",
      "iter 40600 (p=649600), loss 18.609029\n",
      "iter 40800 (p=652800), loss 18.590822\n",
      "----\n",
      " r food. it that day, the mor at it pune,. Flow, Mam need free tagetcalby, she warked and they cam for her eniin. Tom ard her mom and they creat and thought to play and said, \n",
      "\n",
      "Lily, Sue would it's to  \n",
      "----\n",
      "iter 41000 (p=656000), loss 18.587022\n",
      "iter 41200 (p=659200), loss 18.533142\n",
      "iter 41400 (p=662400), loss 18.433885\n",
      "iter 41600 (p=665600), loss 18.406057\n",
      "iter 41800 (p=668800), loss 18.208346\n",
      "----\n",
      "  in the munk, that was a flowering op the baggret you?\"\" Sue thanked a tooqustay. Her mom said, \"\"You ale woml by boing, all day boy.\"\"\n",
      "\n",
      "Tim flow. It banone to play a beait tout?\n",
      "\n",
      "Mommy rade on the be \n",
      "----\n",
      "iter 42000 (p=672000), loss 18.403261\n",
      "iter 42200 (p=675200), loss 18.632027\n",
      "iter 42400 (p=678400), loss 18.823460\n",
      "iter 42600 (p=681600), loss 19.209420\n",
      "iter 42800 (p=684800), loss 19.306951\n",
      "----\n",
      "  this prack this bif mester alaid. It flave the nam of the great were plant harm. From that,\"\" Lily moved him go his wimbed id and dask paint come aland with the bepsor with the could closer, \"\"Do not \n",
      "----\n",
      "iter 43000 (p=688000), loss 19.515441\n",
      "iter 43200 (p=691200), loss 19.708403\n",
      "iter 43400 (p=694400), loss 19.861044\n",
      "iter 43600 (p=697600), loss 19.958439\n",
      "iter 43800 (p=700800), loss 19.927802\n",
      "----\n",
      " eens and g€ve tull wing too, the coon. Juck said notectes and the sey frem. She stecken asume, funfurs hads. The all Bebure outive. One day, he ground. John was happy one the ate shentul? \n",
      "\n",
      "Sudn't mam \n",
      "----\n",
      "iter 44000 (p=704000), loss 20.116429\n",
      "iter 44200 (p=707200), loss 20.184797\n",
      "iter 44400 (p=710400), loss 20.206494\n",
      "iter 44600 (p=713600), loss 20.038741\n",
      "iter 44800 (p=716800), loss 20.166188\n",
      "----\n",
      "  he raghed his friends smiles faretly gave the baoure the sofnenicsed a helpas. \n",
      "\n",
      "The bird wanted to grome.\n",
      "\n",
      "Fl, oneed for the fiss all the doplocapeedep. She had was oneard. The flet gest. He said, â \n",
      "----\n",
      "iter 45000 (p=720000), loss 20.431723\n",
      "iter 45200 (p=723200), loss 20.175428\n",
      "iter 45400 (p=726400), loss 20.293311\n",
      "iter 45600 (p=729600), loss 20.208468\n",
      "iter 45800 (p=732800), loss 20.345886\n",
      "----\n",
      "  up the zood. \"\"Do look of her invite the him. \n",
      "\n",
      "My feel who lott oceve the melcue of mading a slooky. While they badd her. Wo Gristers put surllo, the pet whs dapky and street man had a teamaot tems, \n",
      "----\n",
      "iter 46000 (p=736000), loss 20.199394\n",
      "iter 46200 (p=739200), loss 20.179942\n",
      "iter 46400 (p=742400), loss 19.889584\n",
      "iter 46600 (p=745600), loss 19.722764\n",
      "iter 46800 (p=748800), loss 19.867138\n",
      "----\n",
      " d to a tame away. It felt laught. \n",
      "\n",
      "Dug, always relows in the suct. \n",
      "\n",
      "Joy looking. He had so had called but as checised her playing sofeinul,\"\n",
      "\"Once there were the athy to brippon, she delted sime, an \n",
      "----\n",
      "iter 47000 (p=752000), loss 19.743056\n",
      "iter 47200 (p=755200), loss 19.727941\n",
      "iter 47400 (p=758400), loss 19.680327\n",
      "iter 47600 (p=761600), loss 19.615478\n",
      "iter 47800 (p=764800), loss 19.992177\n",
      "----\n",
      " er leade he starked over a gave the timevors icer of the do so invined thfme was windiquy.\n",
      "\n",
      "The pirplet was very wayze e*chers! Then. He gail to see down and the dows.\n",
      "\n",
      "His mom, pooken pot. Anster and \n",
      "----\n",
      "iter 48000 (p=768000), loss 19.984603\n",
      "iter 48200 (p=771200), loss 19.955828\n",
      "iter 48400 (p=774400), loss 19.815835\n",
      "iter 48600 (p=777600), loss 19.787577\n",
      "iter 48800 (p=780800), loss 19.589007\n",
      "----\n",
      "  sfleic awly to the man upy far his brighin's hunt it. \n",
      "\n",
      "When a little birdey and thankid to day on the itco fant!\n",
      "\n",
      "So he camed Tummy realing ailut the scalled iver. They loved time! He zan wanted to  \n",
      "----\n",
      "iter 49000 (p=784000), loss 19.539145\n",
      "iter 49200 (p=787200), loss 19.398390\n",
      "iter 49400 (p=790400), loss 19.483641\n",
      "iter 49600 (p=793600), loss 19.890077\n",
      "iter 49800 (p=796800), loss 19.656904\n",
      "----\n",
      " went on every ove outside and theare the time in ome. From then undy around ailure!\"\"\n",
      "\n",
      "Every gree to was loft. Allione.\"\n",
      "\"Oided in the went stopped about for the beat torring and seen was wrong the co \n",
      "----\n",
      "iter 50000 (p=800000), loss 19.863977\n",
      "iter 50200 (p=803200), loss 19.917437\n",
      "iter 50400 (p=806400), loss 20.037060\n",
      "iter 50600 (p=809600), loss 19.988846\n",
      "iter 50800 (p=812800), loss 19.824930\n",
      "----\n",
      " n to sofem smized, dasceran the kappy onle horrigh. So he went bitmer, ran into the boudf. He welcausely sooped her and bug never so.\n",
      "\n",
      "One day Tom was to the shered, Jus. Whene op the squeing. But it  \n",
      "----\n",
      "iter 51000 (p=816000), loss 19.903392\n",
      "iter 51200 (p=819200), loss 20.227939\n",
      "iter 51400 (p=822400), loss 19.919461\n",
      "iter 51600 (p=825600), loss 20.020753\n",
      "iter 51800 (p=828800), loss 19.915240\n",
      "----\n",
      "  pickle.\"\n",
      "\"Once upon a time, there was a oft them. They who got the drappont listee back ever before cheenst! \"\"Aft, do you, blaok on what \"\"When ole ran in thourtly bisted hand. Every dass coth sad b \n",
      "----\n",
      "iter 52000 (p=832000), loss 19.670649\n",
      "iter 52200 (p=835200), loss 19.615094\n",
      "iter 52400 (p=838400), loss 19.902797\n",
      "iter 52600 (p=841600), loss 19.728277\n",
      "iter 52800 (p=844800), loss 19.803559\n",
      "----\n",
      " eft smife it friend.\"\n",
      "\"Once upon arometime the yabbbous that there wouldn't! The bizn the kild enchis arver of her lice around the rauch grabbe to smer driakled morging to pet and soeptand him how. Th \n",
      "----\n",
      "iter 53000 (p=848000), loss 19.995218\n",
      "iter 53200 (p=851200), loss 20.013602\n",
      "iter 53400 (p=854400), loss 19.942249\n",
      "iter 53600 (p=857600), loss 19.728824\n",
      "iter 53800 (p=860800), loss 19.683750\n",
      "----\n",
      " ed od and girl who happy to gire of the frugged, \"\"That then help and thouteff to to him a wot! Enn was so hapsily. She was very bester, and he was spant to have the fat to explore, but he could zeach \n",
      "----\n",
      "iter 54000 (p=864000), loss 19.451293\n",
      "iter 54200 (p=867200), loss 19.530982\n",
      "iter 54400 (p=870400), loss 19.278500\n",
      "iter 54600 (p=873600), loss 19.126852\n",
      "iter 54800 (p=876800), loss 19.127755\n",
      "----\n",
      " of hed armed her mom cleas. She saw something sawp and scared shinding!\"\n",
      "\"Once upon a time, reanger and cat was like a grsprets and he wereved to slew her his upen were worning what the doust is beel  \n",
      "----\n",
      "iter 55000 (p=880000), loss 19.088480\n",
      "iter 55200 (p=883200), loss 19.210484\n",
      "iter 55400 (p=886400), loss 19.320403\n",
      "iter 55600 (p=889600), loss 19.664169\n",
      "iter 55800 (p=892800), loss 19.428604\n",
      "----\n",
      " a huge coltern make it to that he was so just be a brgate to bepver becate. And Sally asked it always fasted playing aroon and follot her mom and Tim was amiing the stras and foll. Dadly were she chab \n",
      "----\n",
      "iter 56000 (p=896000), loss 19.643562\n",
      "iter 56200 (p=899200), loss 19.575341\n",
      "iter 56400 (p=902400), loss 19.348703\n",
      "iter 56600 (p=905600), loss 19.158779\n",
      "iter 56800 (p=908800), loss 18.700599\n",
      "----\n",
      " zy to the felt just up that â€œCan the pen and the sunch and it, gig his felt his walicced ever houte togethers to be hight the jadd and kept ole that for she tops hug ins loved the big!\"\n",
      "\"The miness! \n",
      "----\n",
      "iter 57000 (p=912000), loss 18.632393\n",
      "iter 57200 (p=915200), loss 18.319575\n",
      "iter 57400 (p=918400), loss 18.378225\n",
      "iter 57600 (p=921600), loss 18.175549\n",
      "iter 57800 (p=924800), loss 18.022309\n",
      "----\n",
      " lomed and smiled who loved on her friends but a with his mom togethel. She cais wore! Tim was so heart from the bird said, \"\"It's not mine to her. One tiris to play. It wanted to play and put time, he \n",
      "----\n",
      "iter 58000 (p=928000), loss 18.124856\n",
      "iter 58200 (p=931200), loss 18.227428\n",
      "iter 58400 (p=934400), loss 18.002735\n",
      "iter 58600 (p=937600), loss 17.998444\n",
      "iter 58800 (p=940800), loss 17.790024\n",
      "----\n",
      "  said ne, so. I.\n",
      "\n",
      "Rane stior yerew, But a longly toy dow. Billy and Lily learned to s,ea hagped idea to mom. They can safly gady in his do showly fat bucker wish. The fored his ball lay. They saw a ck \n",
      "----\n",
      "iter 59000 (p=944000), loss 18.018390\n",
      "iter 59200 (p=947200), loss 18.343839\n",
      "iter 59400 (p=950400), loss 18.125815\n",
      "iter 59600 (p=953600), loss 18.174801\n",
      "iter 59800 (p=956800), loss 17.907590\n",
      "----\n",
      "  mom caurely!\"\" Lug saared the flust flew her. Timmy lived a vorting troim ip and coors with a while of.\"\" Timmy's car wanted so hel. They were clems.\"\n",
      "\"Once upon a time, there was a little girl wrong \n",
      "----\n",
      "iter 60000 (p=960000), loss 17.801078\n",
      "iter 60200 (p=963200), loss 17.561257\n",
      "iter 60400 (p=966400), loss 17.638890\n",
      "iter 60600 (p=969600), loss 17.586664\n",
      "iter 60800 (p=972800), loss 17.776989\n",
      "----\n",
      " ed of now happy it one with his aniggetbor. She had a big and tough it up and he stayt mom started that oidey. The toy named Lily let his chose and paucely atay on friends. One day, Lily went her less \n",
      "----\n",
      "iter 61000 (p=976000), loss 17.899178\n",
      "iter 61200 (p=979200), loss 18.305640\n",
      "iter 61400 (p=982400), loss 18.572612\n",
      "iter 61600 (p=985600), loss 18.905901\n",
      "iter 61800 (p=988800), loss 19.061043\n",
      "----\n",
      " appy makeany home, sualhane to mong very great things brownack the very happy fan to thinght on the his or down everyioon she worked to go with his mom need botries worl her friends and they wook his  \n",
      "----\n",
      "iter 62000 (p=992000), loss 19.235893\n",
      "iter 62200 (p=995200), loss 19.556775\n",
      "iter 62400 (p=998400), loss 19.544196\n"
     ]
    }
   ],
   "source": [
    "def basicGradCheck():\n",
    "    inputs = [char_to_ix[ch] for ch in data[:seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[1:seq_length+1]]\n",
    "    hprev = np.random.randn(H, 1)\n",
    "    cprev = np.random.randn(H, 1)\n",
    "    gradCheck(inputs, targets, hprev, cprev)\n",
    "\n",
    "# Uncomment this to run gradient checking instead of training\n",
    "#basicGradCheck()\n",
    "#sys.exit()\n",
    "\n",
    "# n is the iteration counter; p is the input sequence pointer, at the beginning\n",
    "# of each step it points at the sequence in the input that will be used for\n",
    "# training this iteration.\n",
    "n, p = 0, 0\n",
    "\n",
    "# Memory variables for Adagrad.\n",
    "mWf = np.zeros_like(Wf)\n",
    "mbf = np.zeros_like(bf)\n",
    "mWi = np.zeros_like(Wi)\n",
    "mbi = np.zeros_like(bi)\n",
    "mWcc = np.zeros_like(Wcc)\n",
    "mbcc = np.zeros_like(bcc)\n",
    "mWo = np.zeros_like(Wo)\n",
    "mbo = np.zeros_like(bo)\n",
    "mWy = np.zeros_like(Wy)\n",
    "mby = np.zeros_like(by)\n",
    "smooth_loss = -np.log(1.0/V) * seq_length\n",
    "\n",
    "while p < MAX_DATA:\n",
    "    # Prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        # Reset RNN memory\n",
    "        hprev = np.zeros((H, 1))\n",
    "        cprev = np.zeros((H, 1))\n",
    "        p = 0 # go from start of data\n",
    "\n",
    "    # In each step we unroll the RNN for seq_length cells, and present it with\n",
    "    # seq_length inputs and seq_length target outputs to learn.\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # Sample from the model now and then.\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, cprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "    # Forward seq_length characters through the RNN and fetch gradient.\n",
    "    (loss, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
    "     hprev, cprev) = lossFun(inputs, targets, hprev, cprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 200 == 0:\n",
    "        print('iter %d (p=%d), loss %f' % (n, p, smooth_loss))\n",
    "\n",
    "    # Perform parameter update with Adagrad.\n",
    "    for param, dparam, mem in zip(\n",
    "            [Wf, bf, Wi, bi, Wcc, bcc, Wo, bo, Wy, by],\n",
    "            [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby],\n",
    "            [mWf, mbf, mWi, mbi, mWcc, mbcc, mWo, mbo, mWy, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "    p += seq_length\n",
    "    n += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
