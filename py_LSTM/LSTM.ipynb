{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff91fc80-0bc4-471f-91e9-6c51ac49657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d27627-2fa1-4c6e-bfad-dd87852ae629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
      "Character to index mapping: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "Vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(f'Character to index mapping: {itos}')\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Shuffle the words\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f1a8cf47-b550-404c-8e7b-8df895e1f819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes - X: torch.Size([208135, 8]), Y: torch.Size([208135, 8])\n",
      "Development data shapes - X: torch.Size([52028, 8]), Y: torch.Size([52028, 8])\n",
      "Training batches shape - X: torch.Size([32, 6504, 8]), Y: torch.Size([32, 6504, 8])\n",
      "Development batches shape - X: torch.Size([32, 1625, 8]), Y: torch.Size([32, 1625, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def encode_words(words):\n",
    "    encoded = []\n",
    "    for w in words:\n",
    "        encoded.extend([stoi[ch] for ch in '.' + w + '.'])\n",
    "    return encoded\n",
    "\n",
    "encoded = encode_words(words)\n",
    "\n",
    "def create_pairs(seq, block_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(0, len(seq) - block_size):\n",
    "        X.append(seq[i:i+block_size])\n",
    "        Y.append(seq[i+1:i+block_size+1])\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "    return X, Y\n",
    "\n",
    "n = len(encoded)\n",
    "n1 = int(0.8 * n)\n",
    "\n",
    "train_seq = encoded[:n1]\n",
    "dev_seq = encoded[n1:]\n",
    "\n",
    "block_size = 8\n",
    "\n",
    "Xtr, Ytr = create_pairs(train_seq, block_size)\n",
    "Xdev, Ydev = create_pairs(dev_seq, block_size)\n",
    "\n",
    "print(f'Training data shapes - X: {Xtr.shape}, Y: {Ytr.shape}')\n",
    "print(f'Development data shapes - X: {Xdev.shape}, Y: {Ydev.shape}')\n",
    "\n",
    "def split_into_batches(X, Y, batch_size):\n",
    "    num_batches = X.size(0) // batch_size\n",
    "    X = X[:num_batches * batch_size]\n",
    "    Y = Y[:num_batches * batch_size]\n",
    "    \n",
    "    X = X.view(batch_size, -1, X.size(-1))\n",
    "    Y = Y.view(batch_size, -1, Y.size(-1))\n",
    "    return X, Y\n",
    "\n",
    "Xtr_batched, Ytr_batched = split_into_batches(Xtr, Ytr, batch_size)\n",
    "Xdev_batched, Ydev_batched = split_into_batches(Xdev, Ydev, batch_size)\n",
    "\n",
    "print(f'Training batches shape - X: {Xtr_batched.shape}, Y: {Ytr_batched.shape}')\n",
    "print(f'Development batches shape - X: {Xdev_batched.shape}, Y: {Ydev_batched.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "aff50ce0-b584-4dfd-89f0-96b8fd1db1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 208135, 27])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_Xtr = F.one_hot(Xtr, 27)\n",
    "one_hot_Xtr = one_hot_Xtr.view(8, -1, 27)\n",
    "one_hot_Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a48fc655-fb4e-4ea8-b9d1-398c26fb170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "32\n",
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 32, 27]), torch.Size([8, 32]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "hidden_size = 30\n",
    "# Get a batch of random numbers\n",
    "batch = torch.randint(0, one_hot_Xtr.shape[1], (batch_size,))\n",
    "Xb = one_hot_Xtr[:, batch, :] # (8, 32, 27)\n",
    "Yb = Ytr[batch]\n",
    "Yb = Yb.view(8, -1)\n",
    "time_steps, batch_size, input_size = Xb.shape\n",
    "print(time_steps)\n",
    "print(batch_size)\n",
    "print(input_size)\n",
    "Xb.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7fddc288-34bc-4c65-bb6e-fa74c5799979",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb = Xb.type(torch.float32)\n",
    "Yb = Yb.type(torch.int64)\n",
    "\n",
    "# Parameters\n",
    "Fvh = torch.randn(vocab_size, hidden_size)\n",
    "i1vh = torch.randn(vocab_size, hidden_size)\n",
    "i2vh = torch.randn(vocab_size, hidden_size)\n",
    "Ovh = torch.randn(vocab_size, hidden_size)\n",
    "\n",
    "Fhh = torch.randn(hidden_size, hidden_size)\n",
    "i1hh = torch.randn(hidden_size, hidden_size)\n",
    "i2hh = torch.randn(hidden_size, hidden_size)\n",
    "Ohh = torch.randn(hidden_size, hidden_size)\n",
    "\n",
    "bias1 = torch.zeros(hidden_size)\n",
    "bias2 = torch.zeros(hidden_size)\n",
    "bias3 = torch.zeros(hidden_size)\n",
    "bias4 = torch.zeros(hidden_size)\n",
    "\n",
    "output_matrix = torch.randn(hidden_size, vocab_size)\n",
    "\n",
    "# Storage\n",
    "preact1 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "preact2 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "preact3 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "preact4 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "\n",
    "act1 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "act2 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "act3 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "act4 = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "\n",
    "Cin = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "Cout = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "Ctout = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "\n",
    "Hin = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "Hout = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "\n",
    "logits = torch.zeros((time_steps, batch_size, vocab_size))\n",
    "\n",
    "c0 = torch.zeros(batch_size, hidden_size)\n",
    "h0 = torch.zeros((batch_size, hidden_size))\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "# To update\n",
    "dFvh = torch.zeros(vocab_size, hidden_size) / 0.0\n",
    "di1vh = torch.zeros(vocab_size, hidden_size) / 0.0\n",
    "di2vh = torch.zeros(vocab_size, hidden_size) / 0.0\n",
    "dOvh = torch.zeros(vocab_size, hidden_size) / 0.0\n",
    "\n",
    "dFhh = torch.zeros(hidden_size, hidden_size)/ 0.0\n",
    "di1hh = torch.zeros(hidden_size, hidden_size)/ 0.0\n",
    "di2hh = torch.zeros(hidden_size, hidden_size)/ 0.0\n",
    "dOhh = torch.zeros(hidden_size, hidden_size)/ 0.0\n",
    "\n",
    "dbias1 = torch.zeros(hidden_size) # (30)\n",
    "dbias2 = torch.zeros(hidden_size) # (30)\n",
    "dbias3 = torch.zeros(hidden_size) # (30)\n",
    "dbias4 = torch.zeros(hidden_size) # (30)\n",
    "\n",
    "doutput_matrix = torch.randn(hidden_size, vocab_size) # (30, 27)\n",
    "\n",
    "\n",
    "# Placeholders (indexed with t)\n",
    "dlogits = torch.zeros((time_steps, batch_size, vocab_size)) # (30, 27)\n",
    "dhidden1 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dhidden2 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dtotal = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "\n",
    "dpreact1 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30) \n",
    "dpreact2 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dpreact3 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dpreact4 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "\n",
    "dact1 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30) \n",
    "dact2 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dact3 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dact4 = torch.zeros((time_steps, batch_size, hidden_size)) # (32, 30)\n",
    "\n",
    "dC = torch.zeros((time_steps, batch_size, hidden_size)) # (32, 30)\n",
    "dCt = torch.zeros((time_steps, batch_size, hidden_size)) # (32, 30)\n",
    "dHin = torch.zeros((time_steps, batch_size, hidden_size)) # (32, 30)\n",
    "dHout = torch.zeros((time_steps, batch_size, hidden_size)) # (32, 30)\n",
    "dlogits = torch.zeros((time_steps, batch_size, vocab_size)) # (32, 27)\n",
    "\n",
    "dc0 = torch.zeros(batch_size, hidden_size)\n",
    "dh0 = torch.zeros((batch_size, hidden_size))\n",
    "dcn = torch.zeros(batch_size, hidden_size)\n",
    "dhn = torch.zeros((batch_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a1b15766-4d25-43b2-ae0a-dfd12d510093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5591)\n",
      "tensor(0.5620)\n",
      "tensor(0.5519)\n",
      "tensor(0.5470)\n",
      "tensor(0.5595)\n",
      "tensor(0.5681)\n",
      "tensor(0.5561)\n",
      "tensor(0.5641)\n",
      "tensor(0.5650)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Backpropogate all preactivations\u001b[39;00m\n\u001b[1;32m    107\u001b[0m dpreact1[t] \u001b[38;5;241m=\u001b[39m dact1[t] \u001b[38;5;241m*\u001b[39m act1[t] \u001b[38;5;241m*\u001b[39m ( \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m act1[t])\n\u001b[0;32m--> 108\u001b[0m dpreact2[t] \u001b[38;5;241m=\u001b[39m dact2[t] \u001b[38;5;241m*\u001b[39m act2[t] \u001b[38;5;241m*\u001b[39m ( \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mact2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    109\u001b[0m dpreact3[t] \u001b[38;5;241m=\u001b[39m dact3[t] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(dpreact3[t])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    110\u001b[0m dpreact4[t] \u001b[38;5;241m=\u001b[39m dact4[t] \u001b[38;5;241m*\u001b[39m act4[t] \u001b[38;5;241m*\u001b[39m ( \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m act4[t])\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/_tensor.py:966\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "hidden_size = 30\n",
    "lr = 0.1\n",
    "time_steps = 8\n",
    "batch_size = 32\n",
    "nput_size = 27\n",
    "Xb.shape, Yb.shape\n",
    "iterations = 0\n",
    "h0 = torch.zeros((batch_size, hidden_size))\n",
    "c0 = torch.zeros((batch_size, hidden_size))\n",
    "\n",
    "while iterations < 2000:\n",
    "    # Get a batch of random numbers into correct shape\n",
    "    batch = torch.randint(0, one_hot_Xtr.shape[1], (batch_size,))\n",
    "    Xb = one_hot_Xtr[:, batch, :] # (8, 32, 27)\n",
    "    Xb = Xb / 1.0\n",
    "    Yb = Ytr[batch]\n",
    "    Yb = Yb.view(8, -1)\n",
    "\n",
    "    # Forward propagation\n",
    "    for t in range(time_steps):\n",
    "        # if c0 is None: c0 = np.zeros((b,d))\n",
    "        # if h0 is None: h0 = np.zeros((b,d))\n",
    "        if t == 0:\n",
    "            Hin[t] = h0\n",
    "            Cin[t] = c0\n",
    "        else:\n",
    "            Hin[t] = Hout[t-1]\n",
    "            Cin[t] = Cout[t-1]\n",
    "        loss = 0\n",
    "        \n",
    "        preact1[t] = Xb[t] @ Fvh + Hin[t] @ Fhh + bias1 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "        preact2[t] = Xb[t] @ i1vh + Hin[t] @ i1hh * bias2 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "        preact3[t] = Xb[t] @ i2vh + Hin[t] @ i2hh + bias3 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "        preact4[t] = Xb[t] @ Ovh + Hin[t] @ Ohh + bias4 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "        \n",
    "        act1[t] = torch.sigmoid(preact1[t]) # (32, 30)\n",
    "        act2[t] = torch.sigmoid(preact2[t]) # (32, 30)\n",
    "        act3[t] = torch.tanh(preact3[t]) # (32, 30)\n",
    "        act4[t] = torch.sigmoid(preact4[t]) # (32, 30)\n",
    "        \n",
    "        Cout[t] = act1[t] * Cin[t] + act2[t] * act3[t] # (32, 30)\n",
    "        if t < time_steps -1: Cin[t+1] = Cout[t]\n",
    "        Ctout[t] = torch.tanh(Cout[t]) # (32, 30)\n",
    "        Hout[t] = Ct[t] * act4[t] # (32, 30)\n",
    "        if t < time_steps -1: Hin[t+1] = Hout[t] \n",
    "        \n",
    "        \n",
    "        logits[t] = Hout[t] @ output_matrix # (32, 27)\n",
    "        counts = logits.exp()\n",
    "        counts_sum = counts.sum(1, keepdims=True)\n",
    "        counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "        probs = counts * counts_sum_inv\n",
    "        logprobs = probs.log()\n",
    "        loss += -logprobs[t][torch.arange(32), Yb].mean()\n",
    "\n",
    "    if (iterations % 100 == 0):\n",
    "        print (loss / time_steps)\n",
    "    \n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "    # Backward pass\n",
    "    for t in reversed(range(time_steps)):\n",
    "    \n",
    "        # Backpropogate cross entropy\n",
    "        dlogits[t] = F.softmax(logits[t], 1)\n",
    "        dlogits[t][torch.arange(batch_size), Yb] -= 1\n",
    "        dlogits /= n\n",
    "        \n",
    "        # Backpropogate dHout\n",
    "        # t = 7, 6, 5, 4, 3, 2, 1, 0\n",
    "        if (t < time_steps-1):\n",
    "            # dHout of a previous time step, must add dHin of the next time step\n",
    "            # dHout of one time step, becomes the input for the next time step\n",
    "            # Hout[t] = Hin[t+1]\n",
    "            dHout[t] = dHin[t+1]\n",
    "            dHout[t] =  dlogits[t] @ output_matrix.T + dHin[t+1] # (32, 27) @ (27, 30) = (32, 30) dHt on paper derivation\n",
    "        else: # t = 7\n",
    "            dHout[t] =  dlogits[t] @ output_matrix.T + dhn\n",
    "    \n",
    "        # Backpropogate output matrix\n",
    "        doutput_matrix = dlogits[t].T @ dHout[t] # (32, 27)\n",
    "        \n",
    "        # Backpropogate dact3 (output gate activations)\n",
    "        dact3[t] = dHout[t] * Ctout[t] # (32, 27) * (32, 27) = (32, 27)\n",
    "        \n",
    "        # Backpropogate dC (current cell state)\n",
    "        dC[t] = dHout[t] * act4[t] * (1 - torch.tanh(Cout[t])**2) # (32, 27) * (32, 27) * (32, 27) = (32, 27)\n",
    "        \n",
    "        # Backpropogate act1 and previous cell state\n",
    "        if t > 0:\n",
    "            # Forget gate activations\n",
    "            # Last cell activations\n",
    "            dact1[t] = dC[t] * Cout[t-1] # (32, 27) * (32, 27) = (32, 27)\n",
    "            dC[t-1] = dC[t] * act1[t]\n",
    "        else:\n",
    "            dact1[t] = dC[t] * c0\n",
    "            dc0 = dC[t] * act1[t]\n",
    "        \n",
    "        # Backpropogate i1 activations\n",
    "        dact2[t] = dC[t] * act3[t]\n",
    "        \n",
    "        # Backpropogate i2 activations\n",
    "        dact3[t] = dC[t] * act2[t]\n",
    "        \n",
    "        # Backpropogate all preactivations\n",
    "        dpreact1[t] = dact1[t] * act1[t] * ( 1- act1[t])\n",
    "        dpreact2[t] = dact2[t] * act2[t] * ( 1- act2[t])\n",
    "        dpreact3[t] = dact3[t] * (1 - torch.tanh(dpreact3[t])**2)\n",
    "        dpreact4[t] = dact4[t] * act4[t] * ( 1- act4[t])\n",
    "        \n",
    "        # Backpropogate gates\n",
    "        dFvh = Xb[t].T @ dpreact1[t] # (27, 32) (32, 30) = (27, 30)\n",
    "        dFhh = Hin[t].T @ dpreact1[t] \n",
    "        di1vh = Xb[t].T @ dpreact2[t]\n",
    "        di1hh = Hin[t].T @ dpreact2[t]\n",
    "        di2vh = Xb[t].T @ dpreact3[t]\n",
    "        di2hh = Hin[t].T @ dpreact3[t]\n",
    "        dOvh = Xb[t].T @ dpreact4[t]\n",
    "        dOhh = Hin[t].T @ dpreact4[t]\n",
    "        \n",
    "        # Backpropogate prevh\n",
    "        dHin[t] = dpreact1[t] @ Fhh.T + dpreact2[t] @ i1hh.T + dpreact3[t] @ i2hh.T + dpreact4[t] @ Ohh.T\n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Update parameters using gradients\n",
    "    Fvh -= lr * dFvh\n",
    "    i1vh -= lr * di1vh\n",
    "    i2vh -= lr * di2vh\n",
    "    Ovh -= lr * dOvh\n",
    "    \n",
    "    Fhh -= lr * dFhh\n",
    "    i1hh -= lr * di1hh\n",
    "    i2hh -= lr * di2hh\n",
    "    Ohh -= lr * dOhh\n",
    "    \n",
    "    bias1 -= lr * dbias1\n",
    "    bias2 -= lr * dbias2\n",
    "    bias3 -= lr * dbias3\n",
    "    bias4 -= lr * dbias4\n",
    "    h0 = Hout[-1]\n",
    "    c0 = Cout[-1]\n",
    "\n",
    "    iterations+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4c78234a-e277-4118-bb33-bea7f86b2fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aamjaomjmjfbffstsxmddzmjmsmejomjrpmldsszmddajkdzr.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
    "    return e_x / torch.sum(e_x, dim=1, keepdim=True)\n",
    "\n",
    "def sample_model(start_vector, Fvh, Fhh, i1vh, i1hh, i2vh, i2hh, Ovh, Ohh, bias1, bias2, bias3, bias4, output_matrix):\n",
    "    \"\"\"\n",
    "    Generate a sequence of samples from the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_vector: Initial one-hot encoded vector to start the generation (torch tensor of shape (1, input_size))\n",
    "    - Other parameters: Model weights and biases\n",
    "    \n",
    "    Returns:\n",
    "    - Generated sequence (list of integers)\n",
    "    \"\"\"\n",
    "    input_size = start_vector.shape[1]\n",
    "    hidden_size = Fvh.shape[1]\n",
    "    \n",
    "    # Initialize hidden and cell states\n",
    "    h = torch.zeros((1, hidden_size))\n",
    "    c = torch.zeros((1, hidden_size))\n",
    "    \n",
    "    # Initialize the generated sequence with the index of the start vector\n",
    "    generated_sequence = [torch.argmax(start_vector).item()]\n",
    "    \n",
    "    # Loop until '.' is generated or the maximum sequence length is reached\n",
    "    while generated_sequence[-1] != 0 and len(generated_sequence) <= max_length:\n",
    "        x = start_vector  # Use the provided start vector as input\n",
    "        \n",
    "        preact1 = x @ Fvh + h @ Fhh + bias1\n",
    "        preact2 = x @ i1vh + h @ i1hh + bias2\n",
    "        preact3 = x @ i2vh + h @ i2hh + bias3\n",
    "        preact4 = x @ Ovh + h @ Ohh + bias4\n",
    "        \n",
    "        act1 = torch.sigmoid(preact1)\n",
    "        act2 = torch.sigmoid(preact2)\n",
    "        act3 = torch.tanh(preact3)\n",
    "        act4 = torch.sigmoid(preact4)\n",
    "        \n",
    "        c = act1 * c + act2 * act3\n",
    "        h = torch.tanh(c) * act4\n",
    "        \n",
    "        logits = h @ output_matrix\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        # Sample from the probability distribution to get the next input\n",
    "        next_index = torch.multinomial(probs.squeeze(), 1).item()\n",
    "        \n",
    "        generated_sequence.append(next_index)\n",
    "        \n",
    "        # Update the start vector with the one-hot encoded representation of the next index\n",
    "        start_vector = torch.zeros((1, input_size))\n",
    "        start_vector[0, next_index] = 1\n",
    "        \n",
    "    return [itos[ch] for ch in generated_sequence]\n",
    "\n",
    "# Example usage\n",
    "input_size = 27  # Size of the one-hot encoded vector\n",
    "start_index = 1  # Index representing the starting point (e.g., 'a')\n",
    "start_vector = torch.zeros((1, input_size))\n",
    "start_vector[0, start_index] = 1  # One-hot encode the starting point\n",
    "\n",
    "# Assume Fvh, Fhh, i1vh, i1hh, i2vh, i2hh, Ovh, Ohh, bias1, bias2, bias3, bias4, and output_matrix are already defined\n",
    "\n",
    "max_length = 100  # Maximum length of the generated sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7f6d2bc2-0b8b-4014-a382-629827d57d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alkbsetbrmrokmommssmrwqudkmjsnzjtomsjmdhdrijowwhh.\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = sample_model(start_vector, Fvh, Fhh, i1vh, i1hh, i2vh, i2hh, Ovh, Ohh, bias1, bias2, bias3, bias4, output_matrix)\n",
    "print(''.join(generated_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ed7ba90a-df0e-4d3b-9d69-85b15d3fa3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = torch.exp(x - torch.max(x, axis=1, keepdims=True))\n",
    "    return e_x / torch.sum(e_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "08e6a1b8-134e-4758-b5d4-8eaf562fb6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2775)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "\n",
    "loss = 0\n",
    "for t in range(time_steps):\n",
    "    if c0 is None: c0 = np.zeros((b,d))\n",
    "    if h0 is None: h0 = np.zeros((b,d))\n",
    "    Hin[0] = h0\n",
    "    Cin[0] =  c0\n",
    "    \n",
    "    preact1[t] = Xb[t] @ Fvh + Hin[t] @ Fhh + bias1 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "    preact2[t] = Xb[t] @ i1vh + Hin[t] @ i1hh * bias2 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "    preact3[t] = Xb[t] @ i2vh + Hin[t] @ i2hh + bias3 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "    preact4[t] = Xb[t] @ Ovh + Hin[t] @ Ohh + bias4 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "    \n",
    "    act1[t] = torch.sigmoid(preact1[t]) # (32, 30)\n",
    "    act2[t] = torch.sigmoid(preact2[t]) # (32, 30)\n",
    "    act3[t] = torch.tanh(preact3[t]) # (32, 30)\n",
    "    act4[t] = torch.sigmoid(preact4[t]) # (32, 30)\n",
    "    \n",
    "    Cout[t] = act1[t] * Cin[t] + act2[t] * act3[t] # (32, 30)\n",
    "    if t < time_steps -1: Cin[t+1] = Cout[t]\n",
    "    Ctout[t] = torch.tanh(Cout[t]) # (32, 30)\n",
    "    Hout[t] = Ct[t] * act4[t] # (32, 30)\n",
    "    if t < time_steps -1: Hin[t+1] = Hout[t] \n",
    "    \n",
    "    \n",
    "    logits[t] = Hout[t] @ output_matrix # (32, 27)\n",
    "    counts = logits.exp()\n",
    "    counts_sum = counts.sum(1, keepdims=True)\n",
    "    counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "    probs = counts * counts_sum_inv\n",
    "    logprobs = probs.log()\n",
    "    loss += -logprobs[t][torch.arange(32), Yb].mean()\n",
    "\n",
    "print (loss / time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6adee8b7-3d88-40d2-8315-2130a017c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in reversed(range(time_steps)):\n",
    "    \n",
    "    # Backpropogate cross entropy\n",
    "    dlogits[t] = F.softmax(logits[t], 1)\n",
    "    dlogits[t][torch.arange(batch_size), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    \n",
    "    # Backpropogate dHout\n",
    "    # t = 7, 6, 5, 4, 3, 2, 1, 0\n",
    "    if (t < time_steps-1):\n",
    "        # dHout of a previous time step, must add dHin of the next time step\n",
    "        # dHout of one time step, becomes the input for the next time step\n",
    "        # Hout[t] = Hin[t+1]\n",
    "        dHout[t] = dHin[t+1]\n",
    "        dHout[t] =  dlogits[t] @ output_matrix.T + dHin[t+1] # (32, 27) @ (27, 30) = (32, 30) dHt on paper derivation\n",
    "    else: # t = 7\n",
    "        dHout[t] =  dlogits[t] @ output_matrix.T + dhn\n",
    "\n",
    "    # Backpropogate output matrix\n",
    "    doutput_matrix = dlogits[t].T @ dHout[t] # (32, 27)\n",
    "    \n",
    "    # Backpropogate dact3 (output gate activations)\n",
    "    dact3[t] = dHout[t] * Ctout[t] # (32, 27) * (32, 27) = (32, 27)\n",
    "    \n",
    "    # Backpropogate dC (current cell state)\n",
    "    dC[t] = dHout[t] * act4[t] * (1 - torch.tanh(Cout[t])**2) # (32, 27) * (32, 27) * (32, 27) = (32, 27)\n",
    "    \n",
    "    # Backpropogate act1 and previous cell state\n",
    "    if t > 0:\n",
    "        # Forget gate activations\n",
    "        # Last cell activations\n",
    "        dact1[t] = dC[t] * Cout[t-1] # (32, 27) * (32, 27) = (32, 27)\n",
    "        dC[t-1] = dC[t] * act1[t]\n",
    "    else:\n",
    "        dact1[t] = dC[t] * c0\n",
    "        dc0 = dC[t] * act1[t]\n",
    "    \n",
    "    # Backpropogate i1 activations\n",
    "    dact2[t] = dC[t] * act3[t]\n",
    "    \n",
    "    # Backpropogate i2 activations\n",
    "    dact3[t] = dC[t] * act2[t]\n",
    "    \n",
    "    # Backpropogate all preactivations\n",
    "    dpreact1[t] = dact1[t] * act1[t] * ( 1- act1[t])\n",
    "    dpreact2[t] = dact2[t] * act2[t] * ( 1- act2[t])\n",
    "    dpreact3[t] = dact3[t] * (1 - torch.tanh(dpreact3[t])**2)\n",
    "    dpreact4[t] = dact4[t] * act4[t] * ( 1- act4[t])\n",
    "    \n",
    "    # Backpropogate gates\n",
    "    dFvh = Xb[t].T @ dpreact1[t] # (27, 32) (32, 30) = (27, 30)\n",
    "    dFhh = Hin[t].T @ dpreact1[t] \n",
    "    di1vh = Xb[t].T @ dpreact2[t]\n",
    "    di1hh = Hin[t].T @ dpreact2[t]\n",
    "    di2vh = Xb[t].T @ dpreact3[t]\n",
    "    di2hh = Hin[t].T @ dpreact3[t]\n",
    "    dOvh = Xb[t].T @ dpreact4[t]\n",
    "    dOhh = Hin[t].T @ dpreact4[t]\n",
    "    \n",
    "    # Backpropogate prevh\n",
    "    dHin[t] = dpreact1[t] @ Fhh.T + dpreact2[t] @ i1hh.T + dpreact3[t] @ i2hh.T + dpreact4[t] @ Ohh.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f40874a0-9a19-4b09-b930-ad19850b86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 0.5\n",
    "\n",
    "# Update parameters using gradients\n",
    "Fvh -= lr * dFvh\n",
    "i1vh -= lr * di1vh\n",
    "i2vh -= lr * di2vh\n",
    "Ovh -= lr * dOvh\n",
    "\n",
    "Fhh -= lr * dFhh\n",
    "i1hh -= lr * di1hh\n",
    "i2hh -= lr * di2hh\n",
    "Ohh -= lr * dOhh\n",
    "\n",
    "bias1 -= lr * dbias1\n",
    "bias2 -= lr * dbias2\n",
    "bias3 -= lr * dbias3\n",
    "bias4 -= lr * dbias4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
