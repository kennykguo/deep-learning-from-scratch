{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a2a30e-5b4d-4c06-81d9-878f4b21afa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/kenny/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Download NLTK model data (you need to do this once)\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d614b5d-907b-41a8-a6c7-4a805fd3fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a96b5c-8a7e-44d8-ad05-6d0aad0d6b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 68647 sentences.\n"
     ]
    }
   ],
   "source": [
    "with open('../data/reddit-comments-2015-08.csv', 'r', newline='', encoding='utf-8') as f:\n",
    "    # Initalize a reader object\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    # Skip the header row\n",
    "    # next(reader)  \n",
    "    # Split full comments into sentences  - [nltk.sent_tokenize(x[0].lower()) for x in reader] - for the paragraph x[0] from the csv file, make it lowercase and tokenize all sentence\n",
    "    # For all pararaphs in the csv file. * operator unpacks the list into individual sentences, and creates a single iterable\n",
    "    # sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(str(x).lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    # Replace all sentence x in sentences with the start token, sentence body, and text token\"\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print (f\"Parsed {len(sentences)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0dc1ea-0b43-43cf-803d-2ec7f03d7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd63d95c-ba97-4582-acfc-0bbcfe288aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [[word for word in sentence if word not in {'[', ']', '(', ')'}] for sentence in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053435a4-26a2-452c-925a-92938f19ae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SENTENCE_START', 'a', 'dishonest', 'seller', \"isn\\\\'t\", 'going', 'to', 'run', 'the', 'check', 'in', 'the', 'first', 'place', '.', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "# # List of lists\n",
    "print(tokenized_sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9883e9de-4bc0-4c24-8a26-f7ac00b411c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "\n",
    "for i in range (len(tokenized_sentences)):\n",
    "    for word1, word2, in zip(tokenized_sentences[i], tokenized_sentences[i][1:]):\n",
    "        # Create a tuple\n",
    "        bigram = (word1, word2)\n",
    "        # Index into the dictionary, update it by one\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed66e52b-3cc6-460f-808e-b4900d26fa04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', 'SENTENCE_END'), 47244),\n",
       " (('SENTENCE_START', '``'), 9738),\n",
       " ((\"''\", 'SENTENCE_END'), 7573),\n",
       " (('SENTENCE_START', 'i'), 6466),\n",
       " ((\"'\", 'SENTENCE_END'), 5956),\n",
       " ((',', 'and'), 5707),\n",
       " (('.', \"''\"), 4960),\n",
       " ((',', 'but'), 4942),\n",
       " (('*', '*'), 4696),\n",
       " (('of', 'the'), 4317),\n",
       " (('in', 'the'), 4082),\n",
       " (('?', 'SENTENCE_END'), 3919),\n",
       " (('.', \"'\"), 3412),\n",
       " (('it', \"'s\"), 3313),\n",
       " ((',', 'i'), 3176),\n",
       " (('if', 'you'), 2899),\n",
       " (('do', \"n't\"), 2748),\n",
       " (('&', 'gt'), 2573),\n",
       " (('gt', ';'), 2573),\n",
       " (('to', 'be'), 2485),\n",
       " (('http', ':'), 2354),\n",
       " (('i', \"'m\"), 2336),\n",
       " (('SENTENCE_START', 'it'), 2273),\n",
       " (('SENTENCE_START', 'the'), 2240),\n",
       " (('``', 'SENTENCE_END'), 2116),\n",
       " (('to', 'the'), 2106),\n",
       " (('on', 'the'), 2059),\n",
       " (('``', 'i'), 1996),\n",
       " (('SENTENCE_START', \"'\"), 1870),\n",
       " (('!', 'SENTENCE_END'), 1804),\n",
       " (('is', 'a'), 1693),\n",
       " (('SENTENCE_START', 'if'), 1634),\n",
       " (('https', ':'), 1629),\n",
       " ((',', 'the'), 1599),\n",
       " (('and', 'i'), 1568),\n",
       " (('you', 'can'), 1563),\n",
       " (('amp', ';'), 1535),\n",
       " (('for', 'the'), 1529),\n",
       " (('you', 'have'), 1510),\n",
       " (('SENTENCE_START', 'you'), 1504),\n",
       " (('but', 'i'), 1503),\n",
       " (('&', 'amp'), 1483),\n",
       " ((',', 'it'), 1462),\n",
       " (('and', 'the'), 1423),\n",
       " (('in', 'a'), 1405),\n",
       " ((',', 'you'), 1393),\n",
       " (('i', 'was'), 1378),\n",
       " (('i', 'do'), 1375),\n",
       " (('it', 'is'), 1371),\n",
       " (('with', 'the'), 1330),\n",
       " (('i', 'think'), 1327),\n",
       " (('it', '.'), 1320),\n",
       " (('it', 'was'), 1319),\n",
       " (('have', 'a'), 1307),\n",
       " (('it\\\\', \"'s\"), 1306),\n",
       " (('i', 'have'), 1306),\n",
       " (('you', \"'re\"), 1254),\n",
       " ((',', 'so'), 1251),\n",
       " (('a', 'lot'), 1233),\n",
       " (('i', 'am'), 1181),\n",
       " (('for', 'a'), 1166),\n",
       " (('this', 'is'), 1129),\n",
       " (('SENTENCE_START', 'but'), 1113),\n",
       " (('have', 'to'), 1112),\n",
       " (('want', 'to'), 1097),\n",
       " (('the', 'same'), 1093),\n",
       " (('to', 'get'), 1092),\n",
       " (('you', 'are'), 1092),\n",
       " (('going', 'to'), 1083),\n",
       " (('at', 'the'), 1059),\n",
       " (('that', \"'s\"), 1052),\n",
       " (('would', 'be'), 1027),\n",
       " (('as', 'a'), 1021),\n",
       " (('does', \"n't\"), 1020),\n",
       " (('with', 'a'), 995),\n",
       " (('to', 'do'), 994),\n",
       " (('SENTENCE_START', '*'), 992),\n",
       " (('i', \"'ve\"), 983),\n",
       " ((\"'s\", 'a'), 978),\n",
       " (('SENTENCE_START', 'and'), 975),\n",
       " (('of', 'a'), 969),\n",
       " ((',', 'or'), 957),\n",
       " (('SENTENCE_START', 'they'), 952),\n",
       " (('i', 'would'), 939),\n",
       " (('is', 'the'), 906),\n",
       " (('i\\\\', \"'m\"), 895),\n",
       " (('they', 'are'), 880),\n",
       " (('that', 'the'), 847),\n",
       " (('from', 'the'), 835),\n",
       " (('SENTENCE_START', 'this'), 825),\n",
       " (('it', ','), 820),\n",
       " ((\"'\", 'i'), 820),\n",
       " (('ca', \"n't\"), 813),\n",
       " ((\"'s\", 'not'), 811),\n",
       " (('lot', 'of'), 806),\n",
       " (('i', 'can'), 805),\n",
       " (('SENTENCE_START', 'he'), 785),\n",
       " (('need', 'to'), 780),\n",
       " (('one', 'of'), 773),\n",
       " (('be', 'a'), 767),\n",
       " (('out', 'of'), 761),\n",
       " (('so', 'i'), 750),\n",
       " (('that', 'i'), 748),\n",
       " (('and', 'it'), 746),\n",
       " (('but', 'it'), 742),\n",
       " (('did', \"n't\"), 739),\n",
       " (('all', 'the'), 739),\n",
       " (('to', 'make'), 738),\n",
       " (('will', 'be'), 738),\n",
       " (('a', 'few'), 733),\n",
       " (('there', 'is'), 732),\n",
       " (('of', 'this'), 732),\n",
       " (('SENTENCE_START', 'that'), 729),\n",
       " ((',', 'which'), 729),\n",
       " (('that', 'you'), 726),\n",
       " (('SENTENCE_START', 'so'), 718),\n",
       " (('there', 'are'), 715),\n",
       " (('to', 'a'), 708),\n",
       " (('is', 'not'), 702),\n",
       " (('\\\\n\\\\n', '*'), 697),\n",
       " (('is', \"n't\"), 696),\n",
       " ((',', 'they'), 664),\n",
       " (('SENTENCE_START', 'we'), 662),\n",
       " (('to', 'have'), 662),\n",
       " (('you', 'do'), 659),\n",
       " (('when', 'i'), 658),\n",
       " (('that', 'is'), 654),\n",
       " (('the', 'game'), 653),\n",
       " (('is', 'that'), 642),\n",
       " (('was', 'a'), 639),\n",
       " (('if', 'i'), 639),\n",
       " ((\"'m\", 'not'), 628),\n",
       " (('able', 'to'), 628),\n",
       " ((',', 'then'), 626),\n",
       " (('on', 'a'), 625),\n",
       " (('SENTENCE_START', 'please'), 619),\n",
       " ((',', 'if'), 617),\n",
       " (('the', 'other'), 614),\n",
       " (('have', 'any'), 606),\n",
       " (('\\\\n', '*'), 602),\n",
       " (('trying', 'to'), 601),\n",
       " (('a', 'good'), 600),\n",
       " (('the', 'first'), 585),\n",
       " (('you\\\\', \"'re\"), 584),\n",
       " (('SENTENCE_START', 'there'), 584),\n",
       " ((',', 'that'), 583),\n",
       " (('as', 'well'), 581),\n",
       " (('should', 'be'), 576),\n",
       " (('at', 'least'), 574),\n",
       " (('i', \"'d\"), 570),\n",
       " (('there', \"'s\"), 569),\n",
       " (('has', 'been'), 554),\n",
       " (('i', 'had'), 550),\n",
       " (('can', 'be'), 543),\n",
       " (('in', 'my'), 542),\n",
       " (('and', 'you'), 540),\n",
       " (('when', 'you'), 538),\n",
       " (('but', 'the'), 535),\n",
       " (('a', 'bit'), 534),\n",
       " (('i', 'know'), 534),\n",
       " (('have', 'been'), 531),\n",
       " (('of', 'my'), 528),\n",
       " (('and', 'this'), 526),\n",
       " (('and', 'a'), 522),\n",
       " (('\\\\n\\\\n', '&'), 521),\n",
       " (('you', 'want'), 518),\n",
       " (('they', \"'re\"), 517),\n",
       " (('if', 'it'), 516),\n",
       " (('kind', 'of'), 516),\n",
       " (('about', 'the'), 515),\n",
       " (('i', 'just'), 514),\n",
       " ((',', 'as'), 514),\n",
       " (('and', 'then'), 512),\n",
       " (('the', 'only'), 505),\n",
       " (('which', 'is'), 503),\n",
       " (('SENTENCE_START', '\\\\n\\\\ni'), 502),\n",
       " ((\"n't\", 'have'), 501),\n",
       " (('what', 'you'), 501),\n",
       " (('he', 'was'), 498),\n",
       " (('he', \"'s\"), 493),\n",
       " (('to', 'see'), 488),\n",
       " (('the', 'best'), 488),\n",
       " (('it', 'would'), 485),\n",
       " (('the', 'time'), 485),\n",
       " (('them', '.'), 485),\n",
       " (('if', 'they'), 485),\n",
       " (('?', \"''\"), 482),\n",
       " (('``', '&'), 481),\n",
       " (('a', 'little'), 480),\n",
       " (('*', 'i'), 479),\n",
       " ((',', 'not'), 479),\n",
       " (('by', 'the'), 474),\n",
       " (('i', 'did'), 472),\n",
       " (('a', 'bot'), 467),\n",
       " (('not', 'a'), 467),\n",
       " (('that', 'it'), 466),\n",
       " (('am', 'a'), 463),\n",
       " (('it', 'to'), 462),\n",
       " (('what', 'i'), 462),\n",
       " (('the', 'most'), 461),\n",
       " (('SENTENCE_START', '\\\\n\\\\n'), 460),\n",
       " (('like', 'a'), 458),\n",
       " (('to', 'me'), 458),\n",
       " (('this', 'subreddit'), 458),\n",
       " (('SENTENCE_START', 'in'), 455),\n",
       " (('that\\\\', \"'s\"), 453),\n",
       " (('the', 'moderators'), 453),\n",
       " (('to', 'go'), 452),\n",
       " (('do', 'you'), 450),\n",
       " (('that', '.'), 447),\n",
       " (('!', \"''\"), 447),\n",
       " (('me', '.'), 440),\n",
       " (('have', 'the'), 437),\n",
       " (('that', 'they'), 437),\n",
       " (('/message/compose/', '?'), 433),\n",
       " (('people', 'who'), 431),\n",
       " ((\"''\", '.'), 431),\n",
       " ((',', 'a'), 427),\n",
       " (('because', 'i'), 427),\n",
       " (('any', 'questions'), 427),\n",
       " (('more', 'than'), 425),\n",
       " ((',', 'because'), 425),\n",
       " (('contact', 'the'), 425),\n",
       " (('had', 'a'), 424),\n",
       " (('part', 'of'), 420),\n",
       " ((\"'\", '&'), 419),\n",
       " ((',', 'he'), 419),\n",
       " (('you', '.'), 417),\n",
       " (('i', 'could'), 416),\n",
       " (('bot', ','), 411),\n",
       " (('please', 'contact'), 411),\n",
       " (('SENTENCE_START', 'my'), 408),\n",
       " (('i', \"don\\\\'t\"), 408),\n",
       " (('this', 'action'), 406),\n",
       " ((\"'s\", 'the'), 405),\n",
       " (('SENTENCE_START', 'a'), 403),\n",
       " (('questions', 'or'), 402),\n",
       " (('would', \"n't\"), 400),\n",
       " ((',', 'there'), 399),\n",
       " (('as', 'the'), 399),\n",
       " (('or', 'concerns'), 399),\n",
       " (('of', 'your'), 399),\n",
       " (('and', 'that'), 399),\n",
       " (('they', 'have'), 398),\n",
       " (('concerns', '.'), 398),\n",
       " (('like', 'the'), 398),\n",
       " (('would', 'have'), 397),\n",
       " (('action', 'was'), 397),\n",
       " (('automatically', '.'), 396),\n",
       " (('for', 'me'), 395),\n",
       " (('was', 'performed'), 395),\n",
       " (('performed', 'automatically'), 395),\n",
       " (('moderators', 'of'), 393),\n",
       " (('time', '.'), 391),\n",
       " ((',', 'we'), 390),\n",
       " (('i', \"'ll\"), 390),\n",
       " (('get', 'a'), 385),\n",
       " (('i\\\\', \"'ve\"), 384),\n",
       " (('because', 'it'), 383),\n",
       " ((',', 'just'), 382),\n",
       " (('subreddit', '/message/compose/'), 381),\n",
       " (('has', 'a'), 381),\n",
       " (('in', 'your'), 381),\n",
       " (('``', 'it'), 379),\n",
       " (('--', '--'), 379),\n",
       " (('into', 'the'), 378),\n",
       " (('way', 'to'), 377),\n",
       " (('``', 'the'), 377),\n",
       " (('all', 'of'), 375),\n",
       " (('if', 'the'), 375),\n",
       " (('of', 'it'), 374),\n",
       " ((':', '//www.youtube.com/watch'), 374),\n",
       " (('//www.youtube.com/watch', '?'), 374),\n",
       " (('about', 'it'), 373),\n",
       " (('try', 'to'), 368),\n",
       " (('the', 'way'), 367),\n",
       " (('to', 'say'), 365),\n",
       " (('you', ','), 363),\n",
       " ((\"n't\", 'know'), 363),\n",
       " (('they', 'were'), 363),\n",
       " (('in', 'this'), 362),\n",
       " (('you', 'should'), 361),\n",
       " (('do', 'not'), 360),\n",
       " (('at', 'all'), 357),\n",
       " (('most', 'of'), 357),\n",
       " ((\"'s\", 'just'), 356),\n",
       " (('SENTENCE_START', 'not'), 356),\n",
       " (('of', 'them'), 355),\n",
       " (('it', 'does'), 355),\n",
       " (('even', 'if'), 355),\n",
       " (('we', 'have'), 353),\n",
       " (('as', 'i'), 351),\n",
       " (('and', 'they'), 349),\n",
       " (('SENTENCE_START', '\\\\n'), 349),\n",
       " (('that', ','), 348),\n",
       " (('be', 'able'), 347),\n",
       " (('SENTENCE_START', 'also'), 346),\n",
       " (('me', ','), 346),\n",
       " (('of', 'people'), 345),\n",
       " (('to', 'use'), 344),\n",
       " ((\"''\", ','), 344),\n",
       " (('SENTENCE_START', 'i\\\\'), 342),\n",
       " (('SENTENCE_START', 'just'), 340),\n",
       " (('SENTENCE_START', 'as'), 338),\n",
       " (('be', 'the'), 338),\n",
       " (('SENTENCE_START', 'what'), 337),\n",
       " ((\"'ve\", 'been'), 337),\n",
       " (('some', 'of'), 335),\n",
       " (('you', 'to'), 334),\n",
       " ((':', '\\\\n\\\\n'), 334),\n",
       " ((',', 'in'), 333),\n",
       " (('you', 'could'), 332),\n",
       " (('used', 'to'), 331),\n",
       " (('was', 'the'), 331),\n",
       " (('had', 'to'), 329),\n",
       " (('was', \"n't\"), 328),\n",
       " ((',', 'etc'), 327),\n",
       " (('that', 'he'), 327),\n",
       " (('because', 'of'), 326),\n",
       " (('you', 'know'), 325),\n",
       " (('but', 'you'), 324),\n",
       " (('not', 'the'), 321),\n",
       " (('they', 'do'), 320),\n",
       " (('and', 'not'), 320),\n",
       " ((',', 'even'), 320),\n",
       " (('are', \"n't\"), 320),\n",
       " (('how', 'to'), 319),\n",
       " ((\"n't\", 'think'), 318),\n",
       " (('to', 'play'), 318),\n",
       " (('it', 'will'), 318),\n",
       " (('the', 'world'), 318),\n",
       " (('you', 'need'), 317),\n",
       " (('just', 'a'), 317),\n",
       " (('*', \"''\"), 316),\n",
       " (('because', 'they'), 316),\n",
       " (('you', 'get'), 316),\n",
       " (('he', 'is'), 316),\n",
       " (('think', 'it'), 316),\n",
       " (('wo', \"n't\"), 314),\n",
       " (('*', '|'), 313),\n",
       " (('etc', '.'), 312),\n",
       " (('SENTENCE_START', 'do'), 311),\n",
       " (('SENTENCE_START', 'it\\\\'), 310),\n",
       " (('``', 'you'), 310),\n",
       " (('for', 'you'), 310),\n",
       " (('you', 'think'), 310),\n",
       " (('*', \"'\"), 309),\n",
       " (('the', 'fact'), 308),\n",
       " (('a', 'new'), 307),\n",
       " ((\"n't\", 'be'), 305),\n",
       " (('could', 'be'), 304),\n",
       " (('instead', 'of'), 303),\n",
       " (('at', 'a'), 302),\n",
       " (('to', 'take'), 302),\n",
       " (('look', 'at'), 301),\n",
       " (('on', 'my'), 300),\n",
       " ((',', 'this'), 300),\n",
       " (('up', 'to'), 300),\n",
       " (('well', ','), 299),\n",
       " (('is', 'just'), 298),\n",
       " (('you', 'will'), 297),\n",
       " (('does', 'not'), 295),\n",
       " (('though', '.'), 295),\n",
       " (('are', 'not'), 294),\n",
       " (('due', 'to'), 294),\n",
       " (('SENTENCE_START', 'when'), 294),\n",
       " (('not', 'to'), 293),\n",
       " (('are', 'the'), 292),\n",
       " (('so', 'much'), 291),\n",
       " (('sort', 'of'), 290),\n",
       " (('that', 'are'), 289),\n",
       " (('do', 'it'), 289),\n",
       " (('that', 'was'), 289),\n",
       " (('the', '``'), 288),\n",
       " (('might', 'be'), 287),\n",
       " (('SENTENCE_START', 'then'), 286),\n",
       " (('for', 'your'), 286),\n",
       " (('not', 'be'), 286),\n",
       " (('like', 'this'), 284),\n",
       " (('i', 'guess'), 284),\n",
       " (('SENTENCE_START', 'no'), 281),\n",
       " (('but', 'if'), 281),\n",
       " (('like', 'to'), 280),\n",
       " (('is', 'to'), 279),\n",
       " (('i', 'got'), 279),\n",
       " (('fact', 'that'), 278),\n",
       " (('as', 'you'), 277),\n",
       " (('like', 'that'), 275),\n",
       " (('SENTENCE_START', 'for'), 275),\n",
       " (('think', 'that'), 275),\n",
       " (('there', 'was'), 274),\n",
       " (('?', \"'\"), 274),\n",
       " (('from', 'a'), 274),\n",
       " (('this', '.'), 274),\n",
       " (('*', '\\\\n\\\\n'), 273),\n",
       " (('have', 'no'), 272),\n",
       " (('over', 'the'), 272),\n",
       " ((\"'re\", 'not'), 272),\n",
       " (('make', 'it'), 271),\n",
       " (('back', 'to'), 271),\n",
       " ((',', 'like'), 271),\n",
       " ((',', 'please'), 270),\n",
       " ((',', '``'), 270),\n",
       " (('the', 'last'), 269),\n",
       " (('too', '.'), 269),\n",
       " (('up', '.'), 269),\n",
       " (('SENTENCE_START', 'she'), 269),\n",
       " (('enough', 'to'), 268),\n",
       " (('and', 'he'), 267),\n",
       " (('!', '!'), 266),\n",
       " (('the', 'end'), 265),\n",
       " (('get', 'the'), 265),\n",
       " (('a', 'very'), 264),\n",
       " (('you', \"'ll\"), 264),\n",
       " (('when', 'they'), 263),\n",
       " (('not', 'sure'), 263),\n",
       " (('know', 'what'), 261),\n",
       " (('make', 'a'), 261),\n",
       " (('go', 'to'), 261),\n",
       " (('i', 'feel'), 260),\n",
       " (('me', 'to'), 259),\n",
       " ((';', 'i'), 259),\n",
       " (('is', 'an'), 259),\n",
       " (('we', 'are'), 259),\n",
       " (('but', 'that'), 259),\n",
       " (('of', 'their'), 258),\n",
       " (('they', 'can'), 258),\n",
       " (('to', 'your'), 257),\n",
       " (('with', 'my'), 257),\n",
       " (('to', 'keep'), 256),\n",
       " (('now', '.'), 256),\n",
       " (('time', ','), 256),\n",
       " (('the', 'right'), 255),\n",
       " (('though', ','), 255),\n",
       " (('it', 'has'), 255),\n",
       " (('and', 'have'), 254),\n",
       " (('a', 'couple'), 254),\n",
       " (('out', '.'), 254),\n",
       " (('a', '``'), 254),\n",
       " (('i', 'will'), 254),\n",
       " (('talking', 'about'), 253),\n",
       " (('than', 'the'), 252),\n",
       " (('as', 'it'), 252),\n",
       " (('to', 'you'), 252),\n",
       " (('in', 'that'), 251),\n",
       " ((',', 'is'), 250),\n",
       " (('have', \"n't\"), 250),\n",
       " (('*', ':'), 250),\n",
       " (('are', 'you'), 248),\n",
       " (('on', 'your'), 248),\n",
       " (('however', ','), 248),\n",
       " (('#', '#'), 248),\n",
       " (('right', 'now'), 248),\n",
       " (('time', 'to'), 248),\n",
       " ((',', 'though'), 247),\n",
       " (('people', 'are'), 247),\n",
       " (('there', '.'), 247),\n",
       " (('now', ','), 246),\n",
       " (('is', 'no'), 245),\n",
       " (('of', 'course'), 245),\n",
       " (('for', 'it'), 244),\n",
       " (('the', 'whole'), 244),\n",
       " (('to', 'find'), 244),\n",
       " (('it', 'in'), 243),\n",
       " (('i', 'ca'), 243),\n",
       " (('up', 'with'), 242),\n",
       " (('well', '.'), 242),\n",
       " (('a', 'great'), 241),\n",
       " ((\"n't\", 'really'), 240),\n",
       " (('SENTENCE_START', 'maybe'), 240),\n",
       " (('can', 'not'), 240),\n",
       " (('i', 'mean'), 240),\n",
       " (('i', 'want'), 240),\n",
       " (('i', 'love'), 240),\n",
       " (('them', 'to'), 239),\n",
       " (('it', 'and'), 239),\n",
       " (('are', 'a'), 239),\n",
       " (('game', '.'), 238),\n",
       " (('so', 'you'), 238),\n",
       " (('of', 'that'), 237),\n",
       " ((',', 'no'), 237),\n",
       " (('feel', 'like'), 237),\n",
       " (('said', ','), 237),\n",
       " (('SENTENCE_START', 'even'), 236),\n",
       " (('with', 'it'), 236),\n",
       " (('make', 'sure'), 236),\n",
       " (('wanted', 'to'), 236),\n",
       " (('amount', 'of'), 235),\n",
       " (('on', 'this'), 235),\n",
       " (('the', 'us'), 234),\n",
       " (('like', 'you'), 233),\n",
       " (('your', 'post'), 233),\n",
       " (('this', ','), 233),\n",
       " (('i', 'really'), 233),\n",
       " ((\"n't\", 'get'), 232),\n",
       " (('when', 'it'), 232),\n",
       " (('where', 'you'), 232),\n",
       " (('\\\\n\\\\n', \"''\"), 232),\n",
       " (('the', 'people'), 231),\n",
       " (('to', 'this'), 230),\n",
       " (('when', 'the'), 230),\n",
       " (('and', 'if'), 230),\n",
       " (('they\\\\', \"'re\"), 229),\n",
       " ((\"'ll\", 'be'), 229),\n",
       " (('we', 'can'), 229),\n",
       " (('into', 'a'), 229),\n",
       " (('because', 'you'), 228),\n",
       " (('that', 'would'), 228),\n",
       " (('SENTENCE_START', 'or'), 228),\n",
       " (('i', 'like'), 228),\n",
       " (('but', 'they'), 228),\n",
       " (('of', 'these'), 227),\n",
       " (('so', 'it'), 226),\n",
       " ((',', 'with'), 226),\n",
       " (('based', 'on'), 226),\n",
       " (('think', 'the'), 225),\n",
       " (('to', 'work'), 225),\n",
       " ((',', 'i\\\\'), 224),\n",
       " (('gon', 'na'), 223),\n",
       " (('so', 'that'), 223),\n",
       " (('yes', ','), 223),\n",
       " (('you', \"'ve\"), 223),\n",
       " (('there\\\\', \"'s\"), 222),\n",
       " (('no', 'one'), 222),\n",
       " ((\"n't\", 'want'), 222),\n",
       " (('pretty', 'much'), 222),\n",
       " (('him', '.'), 222),\n",
       " (('can', 'get'), 221),\n",
       " (('or', 'not'), 221),\n",
       " (('here', '.'), 220),\n",
       " (('SENTENCE_START', \"'the\"), 220),\n",
       " (('if', 'he'), 220),\n",
       " (('hard', 'to'), 219),\n",
       " (('SENTENCE_START', 'http'), 218),\n",
       " (('being', 'a'), 218),\n",
       " (('up', 'and'), 218),\n",
       " (('*', '\\\\n'), 218),\n",
       " (('and', 'we'), 217),\n",
       " (('SENTENCE_START', 'all'), 217),\n",
       " (('the', 'rest'), 217),\n",
       " (('SENTENCE_START', ':'), 217),\n",
       " (('rather', 'than'), 216),\n",
       " (('that', 'we'), 216),\n",
       " (('can', 'do'), 216),\n",
       " (('thank', 'you'), 215),\n",
       " ((\"n't\", 'even'), 215),\n",
       " ((',', 'it\\\\'), 215),\n",
       " (('the', 'one'), 215),\n",
       " ((':', '*'), 214),\n",
       " (('out', 'the'), 213),\n",
       " (('what', 'they'), 213),\n",
       " (('of', 'those'), 213),\n",
       " (('*', '.'), 212),\n",
       " (('is', ','), 211),\n",
       " (('because', 'the'), 211),\n",
       " (('%', 'of'), 211),\n",
       " (('do', 'with'), 210),\n",
       " (('say', 'that'), 210),\n",
       " (('as', 'much'), 210),\n",
       " (('them', ','), 209),\n",
       " (('``', '*'), 209),\n",
       " (('we', \"'re\"), 209),\n",
       " (('SENTENCE_START', 'however'), 209),\n",
       " (('looking', 'for'), 208),\n",
       " ((\"''\", 'and'), 207),\n",
       " (('yeah', ','), 206),\n",
       " ((\"'d\", 'be'), 206),\n",
       " (('!', \"'\"), 206),\n",
       " (('like', 'it'), 205),\n",
       " (('i', 'get'), 205),\n",
       " (('may', 'be'), 204),\n",
       " (('better', 'than'), 203),\n",
       " (('|', '*'), 203),\n",
       " (('``', 'this'), 202),\n",
       " (('also', ','), 202),\n",
       " (('to', 'help'), 202),\n",
       " (('of', 'what'), 202),\n",
       " (('how', 'much'), 201),\n",
       " (('up', 'the'), 201),\n",
       " (('then', 'you'), 201),\n",
       " (('end', 'of'), 201),\n",
       " (('``', 'that'), 201),\n",
       " (('to', 'my'), 200),\n",
       " (('i', 'thought'), 200),\n",
       " (('take', 'a'), 200),\n",
       " (('the', 'next'), 199),\n",
       " (('with', 'your'), 199),\n",
       " ((',', \"''\"), 198),\n",
       " (('``', 'yeah'), 198),\n",
       " (('i', 'said'), 197),\n",
       " (('of', 'his'), 197),\n",
       " (('your', 'own'), 197),\n",
       " (('again', ','), 197),\n",
       " (('``', 'well'), 197),\n",
       " (('i', 'see'), 196),\n",
       " (('but', 'not'), 196),\n",
       " (('you', 'would'), 196),\n",
       " ((',', 'what'), 196),\n",
       " (('seem', 'to'), 196),\n",
       " ((':', \"''\"), 196),\n",
       " (('through', 'the'), 195),\n",
       " (('their', 'own'), 195),\n",
       " (('SENTENCE_START', '\\\\n\\\\nthe'), 194),\n",
       " (('and', 'is'), 194),\n",
       " ((',', 'when'), 194),\n",
       " (('think', 'you'), 193),\n",
       " (('a', 'huge'), 193),\n",
       " (('then', 'i'), 193),\n",
       " (('is', 'going'), 192),\n",
       " (('all', '.'), 192),\n",
       " (('such', 'a'), 191),\n",
       " (('see', 'the'), 191),\n",
       " (('to', 'know'), 191),\n",
       " (('is', 'in'), 191),\n",
       " (('that', 'there'), 191),\n",
       " (('SENTENCE_START', 'now'), 190),\n",
       " ((\"'m\", 'a'), 190),\n",
       " (('seems', 'to'), 190),\n",
       " (('``', 'if'), 189),\n",
       " (('you', 'just'), 189),\n",
       " (('she', \"'s\"), 189),\n",
       " ((',', 'my'), 189),\n",
       " ((',', 'do'), 189),\n",
       " (('with', 'you'), 188),\n",
       " (('someone', 'who'), 188),\n",
       " (('you', \"don\\\\'t\"), 188),\n",
       " ((',', 'to'), 187),\n",
       " (('what', 'the'), 187),\n",
       " (('a', 'big'), 187),\n",
       " ((\"''\", \"'\"), 187),\n",
       " (('off', 'the'), 186),\n",
       " (('way', '.'), 185),\n",
       " (('it', 'as'), 185),\n",
       " ((',', 'your'), 185),\n",
       " (('it', 'for'), 185),\n",
       " (('get', 'to'), 184),\n",
       " (('was', 'in'), 183),\n",
       " (('you', 'ca'), 183),\n",
       " (('know', 'that'), 183),\n",
       " (('to', 'give'), 183),\n",
       " (('use', 'the'), 182),\n",
       " (('and', 'get'), 182),\n",
       " ((',', 'for'), 182),\n",
       " (('she', 'was'), 182),\n",
       " (('who', 'are'), 182),\n",
       " (('out', ','), 181),\n",
       " (('they', 'would'), 181),\n",
       " (('it', 'seems'), 181),\n",
       " (('the', 'new'), 180),\n",
       " (('long', 'as'), 180),\n",
       " (('is', 'it'), 179),\n",
       " (('needs', 'to'), 179),\n",
       " (('too', 'much'), 179),\n",
       " (('for', 'that'), 179),\n",
       " (('or', 'something'), 179),\n",
       " (('a', 'while'), 179),\n",
       " (('lots', 'of'), 179),\n",
       " (('after', 'the'), 178),\n",
       " (('you', 'were'), 178),\n",
       " (('the', 'point'), 178),\n",
       " (('what', 'is'), 178),\n",
       " (('up', ','), 178),\n",
       " (('SENTENCE_START', 'how'), 178),\n",
       " (('and', 'my'), 177),\n",
       " (('you', 'for'), 177),\n",
       " (('i', 'agree'), 177),\n",
       " (('the', 'top'), 176),\n",
       " (('or', 'a'), 176),\n",
       " (('he\\\\', \"'s\"), 176),\n",
       " (('rest', 'of'), 176),\n",
       " (('number', 'of'), 176),\n",
       " (('is', 'what'), 175),\n",
       " (('well', 'as'), 175),\n",
       " (('to', 'it'), 175),\n",
       " (('agree', 'with'), 175),\n",
       " (('SENTENCE_START', 'your'), 174),\n",
       " (('as', 'an'), 174),\n",
       " (('each', 'other'), 174),\n",
       " (('on', 'it'), 174),\n",
       " (('time', 'i'), 174),\n",
       " (('here', ','), 174),\n",
       " (('was', 'just'), 174),\n",
       " (('by', 'a'), 174),\n",
       " (('thing', '.'), 173),\n",
       " (('he', 'has'), 173),\n",
       " (('than', 'a'), 173),\n",
       " (('game', ','), 173),\n",
       " (('know', 'how'), 173),\n",
       " (('do', '.'), 171),\n",
       " (('so', ','), 171),\n",
       " (('for', 'example'), 171),\n",
       " (('of', 'all'), 171),\n",
       " (('this', 'was'), 171),\n",
       " (('is', '.'), 171),\n",
       " (('i\\\\', \"'d\"), 170),\n",
       " (('think', 'i'), 170),\n",
       " (('such', 'as'), 169),\n",
       " (('make', 'the'), 169),\n",
       " ((\"'\", '*'), 169),\n",
       " (('SENTENCE_START', 'people'), 169),\n",
       " (('again', '.'), 169),\n",
       " (('a', 'different'), 169),\n",
       " (('one', '.'), 169),\n",
       " (('SENTENCE_START', 'like'), 169),\n",
       " (('in', 'an'), 168),\n",
       " (('SENTENCE_START', 'some'), 168),\n",
       " (('there', ','), 168),\n",
       " (('is', 'pretty'), 168),\n",
       " (('but', 'he'), 168),\n",
       " (('when', 'he'), 167),\n",
       " (('they', 'will'), 167),\n",
       " (('with', 'that'), 167),\n",
       " (('something', 'like'), 167),\n",
       " (('you', 'and'), 167),\n",
       " (('thanks', 'for'), 167),\n",
       " (('having', 'a'), 166),\n",
       " ((',', 'especially'), 166),\n",
       " (('because', 'he'), 166),\n",
       " (('so', 'they'), 166),\n",
       " (('give', 'you'), 166),\n",
       " (('is', 'more'), 166),\n",
       " (('where', 'the'), 166),\n",
       " (('about', 'this'), 165),\n",
       " (('...', \"''\"), 165),\n",
       " (('like', 'i'), 165),\n",
       " (('and', 'there'), 165),\n",
       " (('we', 'were'), 165),\n",
       " ((',', '*'), 165),\n",
       " (('in', 'their'), 164),\n",
       " (('any', 'of'), 164),\n",
       " (('a', 'long'), 164),\n",
       " (('to', 'try'), 164),\n",
       " ((',', 'at'), 164),\n",
       " (('that', 'a'), 164),\n",
       " (('bit', 'of'), 164),\n",
       " ((\"'s\", 'no'), 163),\n",
       " (('the', 'problem'), 163),\n",
       " (('work', '.'), 163),\n",
       " (('people', '.'), 163),\n",
       " (('i', 'also'), 163),\n",
       " (('or', 'the'), 162),\n",
       " ((':', 'i'), 162),\n",
       " (('the', 'question'), 162),\n",
       " (('i', 'hope'), 161),\n",
       " ((\"'\", ','), 161),\n",
       " (('of', 'an'), 161),\n",
       " (('got', 'a'), 161),\n",
       " (('about', 'how'), 161),\n",
       " (('\\\\n', '&'), 161),\n",
       " (('much', 'more'), 161),\n",
       " (('on', '.'), 161),\n",
       " (('the', 'rules'), 160),\n",
       " (('i', 'still'), 160),\n",
       " (('a', 'single'), 160),\n",
       " (('for', 'this'), 160),\n",
       " (('we', 'do'), 160),\n",
       " (('a', 'better'), 160),\n",
       " (('if', 'your'), 160),\n",
       " (('have', 'an'), 160),\n",
       " ((';', 'the'), 160),\n",
       " (('be', 'in'), 159),\n",
       " (('get', 'it'), 159),\n",
       " (('as', 'they'), 159),\n",
       " (('it', 'out'), 159),\n",
       " (('if', 'we'), 158),\n",
       " (('out', 'and'), 158),\n",
       " (('SENTENCE_START', 'at'), 158),\n",
       " (('looks', 'like'), 158),\n",
       " ((\"'m\", 'sure'), 158),\n",
       " (('to', 'pay'), 158),\n",
       " (('if', 'there'), 158),\n",
       " (('end', 'up'), 157),\n",
       " (('in', 'his'), 157),\n",
       " ((',', 'she'), 157),\n",
       " (('and', 'just'), 156),\n",
       " (('it', 'a'), 156),\n",
       " (('\\\\n', \"'\"), 155),\n",
       " (('’', 's'), 155),\n",
       " (('the', 'past'), 155),\n",
       " (('the', 'case'), 155),\n",
       " (('SENTENCE_START', 'to'), 155),\n",
       " (('not', 'have'), 155),\n",
       " (('a', 'year'), 155),\n",
       " (('up', 'in'), 154),\n",
       " (('``', 'no'), 154),\n",
       " (('in', 'order'), 154),\n",
       " (('and', 'do'), 153),\n",
       " (('\\\\n\\\\n\\\\n', '*'), 153),\n",
       " (('it', 'just'), 153),\n",
       " (('has', 'to'), 153),\n",
       " (('could', 'have'), 152),\n",
       " (('with', 'this'), 152),\n",
       " (('SENTENCE_START', 'why'), 151),\n",
       " (('bunch', 'of'), 151),\n",
       " (('far', 'as'), 151),\n",
       " (('the', 'entire'), 151),\n",
       " (('so', 'many'), 151),\n",
       " (('way', ','), 151),\n",
       " (('supposed', 'to'), 151),\n",
       " (('it', 'can'), 150),\n",
       " (('know', ','), 150),\n",
       " (('any', 'other'), 150),\n",
       " (('will', 'not'), 150),\n",
       " (('do', 'the'), 149),\n",
       " (('on', 'that'), 149),\n",
       " (('SENTENCE_START', 'is'), 149),\n",
       " (('\\\\n', \"''\"), 148),\n",
       " (('a', 'bad'), 148),\n",
       " (('to', 'put'), 148),\n",
       " ((\"'s\", 'what'), 148),\n",
       " (('no', ','), 148),\n",
       " (('for', 'my'), 148),\n",
       " (('the', 'idea'), 148),\n",
       " ((',', 'since'), 148),\n",
       " (('must', 'be'), 148),\n",
       " (('you', 'might'), 147),\n",
       " (('years', 'ago'), 147),\n",
       " (('you', \"'d\"), 147),\n",
       " ((',', 'who'), 147),\n",
       " (('did', 'not'), 147),\n",
       " (('think', 'of'), 147),\n",
       " ((\"'s\", 'been'), 147),\n",
       " (('just', 'because'), 146),\n",
       " (('SENTENCE_START', 'https'), 146),\n",
       " (('they', 'did'), 146),\n",
       " (('i', 'believe'), 146),\n",
       " (('is', 'still'), 146),\n",
       " (('and', 'even'), 146),\n",
       " (('other', 'people'), 145),\n",
       " (('a', 'way'), 145),\n",
       " (('up', 'a'), 145),\n",
       " (('a', 'bunch'), 145),\n",
       " (('away', 'from'), 145),\n",
       " (('live', 'in'), 145),\n",
       " ((',', 'maybe'), 145),\n",
       " (('order', 'to'), 145),\n",
       " (('up', 'for'), 145),\n",
       " (('that', 'does'), 145),\n",
       " (('you', 'see'), 145),\n",
       " (('me', 'a'), 145),\n",
       " ((\"n't\", 'see'), 144),\n",
       " (('even', 'though'), 144),\n",
       " (('just', 'like'), 144),\n",
       " (('SENTENCE_START', 'most'), 144),\n",
       " (('who', 'is'), 144),\n",
       " ((\"'ve\", 'never'), 144),\n",
       " (('he', 'had'), 144),\n",
       " (('out', 'there'), 144),\n",
       " (('that', 'can'), 143),\n",
       " (('something', 'that'), 143),\n",
       " (('at', 'this'), 143),\n",
       " (('will', 'have'), 143),\n",
       " (('to', 'look'), 143),\n",
       " (('it', 'up'), 143),\n",
       " (('on', 'their'), 143),\n",
       " (('about', 'a'), 143),\n",
       " (('some', 'people'), 142),\n",
       " ((\"''\", 'is'), 142),\n",
       " (('that', 'this'), 142),\n",
       " (('compared', 'to'), 142),\n",
       " (('*', 'https'), 142),\n",
       " (('you', 'may'), 142),\n",
       " (('SENTENCE_START', \"'you\"), 142),\n",
       " (('what', 'it'), 142),\n",
       " (('good', '.'), 142),\n",
       " (('a', 'game'), 142),\n",
       " (('do', 'that'), 141),\n",
       " (('should', \"n't\"), 141),\n",
       " (('are', 'just'), 141),\n",
       " (('of', 'our'), 141),\n",
       " (('the', 'main'), 141),\n",
       " (('as', 'long'), 141),\n",
       " ((\"'ve\", 'seen'), 141),\n",
       " (('people', 'to'), 140),\n",
       " (('years', '.'), 140),\n",
       " (('could', \"n't\"), 140),\n",
       " (('i\\\\', \"'ll\"), 140),\n",
       " (('to', 'buy'), 140),\n",
       " (('of', 'us'), 140),\n",
       " (('all', ','), 140),\n",
       " (('...', 'i'), 140),\n",
       " (('him', ','), 140),\n",
       " (('people', ','), 139),\n",
       " (('that', 'if'), 139),\n",
       " (('for', 'some'), 139),\n",
       " (('how', 'it'), 139),\n",
       " (('and', 'in'), 139),\n",
       " (('a', 'small'), 139),\n",
       " (('to', 'start'), 139),\n",
       " (('to', 'not'), 138),\n",
       " (('to', 'think'), 138),\n",
       " (('many', 'people'), 138),\n",
       " (('100', '%'), 138),\n",
       " (('of', 'you'), 138),\n",
       " (('talk', 'about'), 138),\n",
       " (('that', 'will'), 138),\n",
       " (('with', 'them'), 137),\n",
       " (('it', 'should'), 137),\n",
       " (('to', 'that'), 137),\n",
       " (('it', 'with'), 137),\n",
       " (('that', 'people'), 137),\n",
       " (('and', 'all'), 136),\n",
       " (('he', 'did'), 136),\n",
       " (('up', 'on'), 136),\n",
       " (('it', 'on'), 136),\n",
       " (('top', 'of'), 136),\n",
       " ((\"n't\", 'like'), 136),\n",
       " (('most', 'people'), 135),\n",
       " ((',', 'while'), 135),\n",
       " (('been', 'a'), 135),\n",
       " (('can', 'see'), 135),\n",
       " ((\"n't\", 'make'), 135),\n",
       " (('you', 'did'), 135),\n",
       " (('the', 'guy'), 135),\n",
       " (('you', 'a'), 135),\n",
       " (('other', 'than'), 135),\n",
       " (('time', 'and'), 135),\n",
       " ((\"'\", \"''\"), 135),\n",
       " (('SENTENCE_START', \"'this\"), 135),\n",
       " (('couple', 'of'), 135),\n",
       " (('tend', 'to'), 135),\n",
       " (('of', '``'), 134),\n",
       " ((',', 'all'), 134),\n",
       " (('been', 'removed'), 134),\n",
       " (('out', 'to'), 134),\n",
       " (('wants', 'to'), 134),\n",
       " (('deal', 'with'), 134),\n",
       " (('not', 'that'), 134),\n",
       " (('willing', 'to'), 134),\n",
       " (('read', 'the'), 134),\n",
       " (('free', 'to'), 134),\n",
       " (('much', 'as'), 134),\n",
       " (('to', 'tell'), 133),\n",
       " (('how', 'you'), 133),\n",
       " (('which', 'i'), 133),\n",
       " ((':', 'http'), 133),\n",
       " (('my', 'own'), 133),\n",
       " (('for', 'them'), 133),\n",
       " (('find', 'a'), 133),\n",
       " (('a', 'whole'), 133),\n",
       " (('are', 'in'), 133),\n",
       " (('\\\\n', '^'), 133),\n",
       " (('the', 'more'), 133),\n",
       " (('him', 'to'), 133),\n",
       " (('this', 'thread'), 133),\n",
       " (('not', 'going'), 132),\n",
       " (('SENTENCE_START', '\\\\n\\\\nif'), 132),\n",
       " (('if', 'that'), 132),\n",
       " (('not', '.'), 132),\n",
       " (('SENTENCE_START', 'thanks'), 132),\n",
       " (('easy', 'to'), 132),\n",
       " (('since', 'i'), 132),\n",
       " (('of', 'time'), 132),\n",
       " (('and', 'your'), 131),\n",
       " (('down', 'to'), 131),\n",
       " (('the', 'original'), 131),\n",
       " (('should', 'have'), 131),\n",
       " (('and', 'other'), 131),\n",
       " (('people', 'in'), 131),\n",
       " (('know', 'the'), 131),\n",
       " (('a', 'problem'), 131),\n",
       " (('to', 'an'), 130),\n",
       " (('*', 'the'), 130),\n",
       " ((\"don\\\\'t\", 'know'), 130),\n",
       " (('it', '?'), 130),\n",
       " (('he', 'would'), 130),\n",
       " (('is', 'why'), 129),\n",
       " (('before', 'the'), 129),\n",
       " (('but', 'there'), 129),\n",
       " (('SENTENCE_START', 'because'), 129),\n",
       " (('may', 'not'), 129),\n",
       " (('it', 'comes'), 129),\n",
       " (('day', '.'), 129),\n",
       " (('thing', 'is'), 129),\n",
       " (('i', 'used'), 129),\n",
       " (('less', 'than'), 129),\n",
       " (('unless', 'you'), 129),\n",
       " (('where', 'i'), 129),\n",
       " (('with', 'an'), 129),\n",
       " ((\"'ve\", 'got'), 129),\n",
       " ((\"'s\", 'like'), 129),\n",
       " (('seems', 'like'), 128),\n",
       " (('why', 'i'), 128),\n",
       " (('is', 'one'), 128),\n",
       " (('me', 'and'), 128),\n",
       " (('is', 'very'), 128),\n",
       " (('it', 'looks'), 128),\n",
       " (('use', 'it'), 128),\n",
       " (('and', 'what'), 128),\n",
       " (('mean', ','), 127),\n",
       " (('what', 'he'), 127),\n",
       " ((\"'ve\", 'had'), 127),\n",
       " (('he', 'does'), 127),\n",
       " (('be', 'more'), 127),\n",
       " (('be', 'an'), 127),\n",
       " (('to', 'come'), 127),\n",
       " (('in', 'mind'), 127),\n",
       " (('around', 'the'), 127),\n",
       " (('i', 'saw'), 127),\n",
       " (('i', 'went'), 127),\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the first 20,000 entries as our vocabulary\n",
    "sorted(b.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c225bafb-ae40-4a0e-afdb-8142ec672457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup',\n",
       " 'future',\n",
       " 'able',\n",
       " 'drunk',\n",
       " 'hence',\n",
       " 'festival',\n",
       " 'belongs',\n",
       " 'option',\n",
       " 'cute',\n",
       " 'our',\n",
       " 'problems',\n",
       " 'visited',\n",
       " 'pile',\n",
       " 'happen',\n",
       " 'etc',\n",
       " 'accepted',\n",
       " 'absence',\n",
       " 'add',\n",
       " 'hated',\n",
       " 'bella',\n",
       " 'apparently',\n",
       " '1.',\n",
       " 'army',\n",
       " 'battlefield',\n",
       " '60',\n",
       " '|\\\\n',\n",
       " \"'right\",\n",
       " 'to=/r/explainlikeimfive',\n",
       " '95',\n",
       " 'chick',\n",
       " 'quickly',\n",
       " 'era',\n",
       " 'merely',\n",
       " 'vs',\n",
       " 'subreddits',\n",
       " 'downvoted',\n",
       " 'together',\n",
       " 'engineer',\n",
       " 'match',\n",
       " '//0fs.me/yis5stledr\\\\n\\\\nhttp',\n",
       " 'awaiting',\n",
       " 'press',\n",
       " 'queue',\n",
       " 'note',\n",
       " 'wrote',\n",
       " 'advance',\n",
       " 'nowadays',\n",
       " 'save',\n",
       " 'roads',\n",
       " 'could',\n",
       " 'dust',\n",
       " 'stairs',\n",
       " 'dice',\n",
       " 'total',\n",
       " 'lose',\n",
       " 'belong',\n",
       " 'rock',\n",
       " 'distraction',\n",
       " 'justify',\n",
       " '\\\\n\\\\nno',\n",
       " '\\\\n\\\\nwith',\n",
       " 'benner',\n",
       " 'indeed',\n",
       " 'etc.',\n",
       " 'already',\n",
       " 'compensate',\n",
       " 'during',\n",
       " 'finished',\n",
       " 'chase',\n",
       " 'reminds',\n",
       " 'can',\n",
       " 'via',\n",
       " 'guessing',\n",
       " 'taken',\n",
       " 'your',\n",
       " 'attached',\n",
       " 'level',\n",
       " 'toronto',\n",
       " '^your',\n",
       " 'across',\n",
       " 'realistically',\n",
       " 'course',\n",
       " 'impact',\n",
       " '+',\n",
       " '\\\\n\\\\nsorry',\n",
       " 'pieces',\n",
       " 'species',\n",
       " 'frowned',\n",
       " '/',\n",
       " 'piss',\n",
       " 'mentality',\n",
       " 'money',\n",
       " '80s',\n",
       " 'geforce',\n",
       " 'published',\n",
       " 'introduced',\n",
       " '\\\\n\\\\ni',\n",
       " 'vet',\n",
       " 'onto',\n",
       " 'foundation',\n",
       " 'norm',\n",
       " 'following',\n",
       " '\\\\n\\\\npeople',\n",
       " 'fashioned',\n",
       " 'training',\n",
       " \"'you\\\\\",\n",
       " 'times',\n",
       " 'incident',\n",
       " 'aim',\n",
       " \"'so\",\n",
       " '80+',\n",
       " 'forces',\n",
       " 'disappointed',\n",
       " 'such',\n",
       " 'ticket',\n",
       " 'compliment',\n",
       " 'heroes',\n",
       " 'upset',\n",
       " 'cities',\n",
       " 'collecters',\n",
       " '\\\\nlow|',\n",
       " '^^^on',\n",
       " 'specific',\n",
       " 'guns',\n",
       " 'reach',\n",
       " 'difference',\n",
       " 'benefit',\n",
       " 'counseling',\n",
       " 'nature',\n",
       " 'vault',\n",
       " 'would\\\\',\n",
       " 'periods',\n",
       " 'ya',\n",
       " \"'if\",\n",
       " 'men',\n",
       " 'corsair',\n",
       " 'decision',\n",
       " 'fully-modular',\n",
       " 'sports',\n",
       " 'free\\\\n',\n",
       " 'beer',\n",
       " 'casting',\n",
       " 'repost',\n",
       " 'january',\n",
       " 'expand',\n",
       " 'banned',\n",
       " '//www.reddit.com/r/askreddit/wiki/index',\n",
       " 'sand',\n",
       " 'rug',\n",
       " 'capture',\n",
       " 'chill',\n",
       " 'growth',\n",
       " 'explain',\n",
       " 'whom',\n",
       " 'doesnt',\n",
       " 'when',\n",
       " 'winter',\n",
       " 'sucked',\n",
       " 'know',\n",
       " 'fit',\n",
       " 'important',\n",
       " 'coverage',\n",
       " 'become',\n",
       " 'soft',\n",
       " 'dr',\n",
       " 'tempted',\n",
       " 'francisco',\n",
       " 'naturally',\n",
       " '1234-1234-1234',\n",
       " 'mall',\n",
       " 'salt',\n",
       " '//www.modojo.com/features/nintendo_3ds_friend_codes_everything_you_need_to_know',\n",
       " 'technology',\n",
       " '\\\\n\\\\nit',\n",
       " 'campus',\n",
       " '27s',\n",
       " 'certainly',\n",
       " 'lots',\n",
       " 'trigger',\n",
       " 'beach',\n",
       " \"'this\",\n",
       " '//www.reddit.com/r/askscience/wiki/quickstart/askingquestions',\n",
       " 'interacted',\n",
       " 'usa',\n",
       " '25',\n",
       " 'somehow',\n",
       " 'discover',\n",
       " '\\\\n\\\\notherwise',\n",
       " 'view',\n",
       " 'crowd',\n",
       " 'party',\n",
       " 'npc',\n",
       " 'choices',\n",
       " '^^^rhavocbot',\n",
       " 'fee',\n",
       " 'visible',\n",
       " 'equipment',\n",
       " 'reminders',\n",
       " 'command',\n",
       " 'minor',\n",
       " 'produce',\n",
       " 'self',\n",
       " 'usual',\n",
       " 'satisfied',\n",
       " 'argued',\n",
       " 'million',\n",
       " 'yadda',\n",
       " 'moderators',\n",
       " 'agency',\n",
       " 'now',\n",
       " 'copy',\n",
       " 'bare',\n",
       " 'fall',\n",
       " 'repair',\n",
       " 'u.c',\n",
       " 'subject=delete',\n",
       " 'but',\n",
       " 'assets',\n",
       " 'plans',\n",
       " 'version',\n",
       " 'roast',\n",
       " 'grown',\n",
       " 'faith',\n",
       " 'fourth',\n",
       " 'for',\n",
       " 'incentive',\n",
       " 'possible.',\n",
       " 'faq',\n",
       " 'cooldown',\n",
       " 'ads',\n",
       " 'mini',\n",
       " '/r/videos.\\\\n\\\\n',\n",
       " 'ready',\n",
       " 'giveaway',\n",
       " 'look',\n",
       " '\\\\\\\\n',\n",
       " 'him',\n",
       " 'homeless',\n",
       " 'choosing',\n",
       " 'scratch',\n",
       " 'one.\\\\n\\\\nrule',\n",
       " 'could\\\\',\n",
       " 'died',\n",
       " 'well',\n",
       " 'restraining',\n",
       " 'dog',\n",
       " 'engage',\n",
       " 'appeal',\n",
       " 'mode',\n",
       " 'cash',\n",
       " 'tactics',\n",
       " 'ball',\n",
       " 'anymore.',\n",
       " 'choose',\n",
       " 'stems',\n",
       " '\\\\n\\\\n6',\n",
       " 'p_35',\n",
       " 'sexual',\n",
       " 'concrete',\n",
       " 'sort',\n",
       " 'claiming',\n",
       " '20review',\n",
       " 'rise',\n",
       " \"'how\",\n",
       " 'purpose',\n",
       " 'hills',\n",
       " 'grid',\n",
       " 'effective',\n",
       " 'server',\n",
       " 'imaginary',\n",
       " '//www.direct2drive.com/',\n",
       " 'turret',\n",
       " 'these',\n",
       " 'sides',\n",
       " 'semi-modular',\n",
       " 'education',\n",
       " 'responses',\n",
       " 'appealing',\n",
       " 'council',\n",
       " 'deeper',\n",
       " 'to.\\\\n\\\\nif',\n",
       " '^a',\n",
       " 'plenty',\n",
       " 'it.\\\\n\\\\n',\n",
       " \"'here\",\n",
       " 'playlist',\n",
       " 'involving',\n",
       " '^^^if',\n",
       " 'budget',\n",
       " 'musician',\n",
       " 'korea',\n",
       " 'laid',\n",
       " 'interesting',\n",
       " 'obsession',\n",
       " 'adult',\n",
       " 'pvp',\n",
       " 'what',\n",
       " 'cares',\n",
       " 'admitted',\n",
       " 'conservatives',\n",
       " 'rarely',\n",
       " '75',\n",
       " 'strange',\n",
       " ',',\n",
       " 'hook',\n",
       " 'appreciate',\n",
       " 'blocksize',\n",
       " 'blow',\n",
       " 'quality',\n",
       " 'brother',\n",
       " 'liberals',\n",
       " 'feedback\\\\n',\n",
       " 'enough',\n",
       " 'subject=help',\n",
       " 'steps',\n",
       " 'fields',\n",
       " 'i7',\n",
       " 'cope',\n",
       " 'frequency',\n",
       " 'whether',\n",
       " 'sites',\n",
       " '//www.reddit.com/r/roboragi/wiki/index',\n",
       " 'using',\n",
       " 'values',\n",
       " 'sign',\n",
       " 'foster',\n",
       " '2fredd.it',\n",
       " 'barely',\n",
       " 'action',\n",
       " 'restrict_sr=on',\n",
       " 'calls',\n",
       " 'up.',\n",
       " 'assuming',\n",
       " '/r/mylittlepony/comments/1lwzub/deviantart_imgur_mirror_bot_nightmirrormoon/',\n",
       " 'haki',\n",
       " 'cardname',\n",
       " 'illness',\n",
       " 'function',\n",
       " 'wrath',\n",
       " 'opens',\n",
       " 'happened',\n",
       " 'european',\n",
       " 'since',\n",
       " '/r/asksciencediscussion',\n",
       " \"'social\",\n",
       " 'degree',\n",
       " 'timer',\n",
       " 'universe',\n",
       " 'permalink',\n",
       " 'window',\n",
       " 'say',\n",
       " 'engine',\n",
       " 'exercise',\n",
       " 'vacation',\n",
       " 'pulled',\n",
       " 'results',\n",
       " 'significant',\n",
       " 'requirements',\n",
       " 'count',\n",
       " 'lacking',\n",
       " 'comet',\n",
       " 'appears',\n",
       " 'influence',\n",
       " 'basis',\n",
       " '\\\\nyou',\n",
       " 'sentence',\n",
       " '6\\\\',\n",
       " 'articles',\n",
       " 'measure',\n",
       " 'lo',\n",
       " '//www.reddit.com/r/pricezombie/wiki/index',\n",
       " 'global',\n",
       " 'gym',\n",
       " 'ratio',\n",
       " 'everywhere',\n",
       " \"'wow\",\n",
       " \"'engineering\",\n",
       " '20of',\n",
       " 'creepy',\n",
       " 'defend',\n",
       " 'madison',\n",
       " 'message=',\n",
       " '^gosh',\n",
       " 'claimed',\n",
       " 'generated',\n",
       " 'console',\n",
       " 'congrats',\n",
       " 'radio',\n",
       " 'way.',\n",
       " '!',\n",
       " 'unique',\n",
       " 'blast',\n",
       " 'par',\n",
       " '\\\\n\\\\npersonally',\n",
       " 'miles',\n",
       " 'silver',\n",
       " 'girls',\n",
       " 'q=',\n",
       " 'vast',\n",
       " 'palestinian',\n",
       " 'everybody',\n",
       " 'winning',\n",
       " 'active',\n",
       " 'r',\n",
       " 'earthfans',\n",
       " 'demand',\n",
       " 'speakers',\n",
       " '\\\\nbut',\n",
       " 'shootings',\n",
       " 'nyc',\n",
       " 'depending',\n",
       " 'special',\n",
       " 'thing',\n",
       " 'awoken',\n",
       " '//github.com/buttscicles/tweetposter/issues',\n",
       " 'process',\n",
       " 'wage',\n",
       " 'media',\n",
       " 'voice',\n",
       " 'oblivious',\n",
       " 'pattern',\n",
       " 'thought',\n",
       " 'sense',\n",
       " 'new',\n",
       " 'jenner',\n",
       " 'suppose',\n",
       " 'existed',\n",
       " 'split',\n",
       " 'lt',\n",
       " 'night',\n",
       " 'down',\n",
       " 'gambling',\n",
       " 'buy',\n",
       " 'without',\n",
       " 'performs',\n",
       " 'subject=post+review+request',\n",
       " 'go',\n",
       " 'added',\n",
       " 'international',\n",
       " '-\\\\n\\\\nthis',\n",
       " 'anymore',\n",
       " 'stats',\n",
       " 'beneficial',\n",
       " 'transport',\n",
       " 'primary',\n",
       " 'democratic',\n",
       " 'gather',\n",
       " 'fuck',\n",
       " 'straight',\n",
       " 'ban.\\\\n\\\\n',\n",
       " 'hall',\n",
       " 'dakka',\n",
       " 'caught',\n",
       " 'flat',\n",
       " 'armor',\n",
       " 'trump',\n",
       " '\\\\n|-|-|-|-|-|',\n",
       " 'giant',\n",
       " 'y',\n",
       " '\\\\n\\\\nfirst',\n",
       " 'carry',\n",
       " 'mana',\n",
       " 'visual',\n",
       " 'fill',\n",
       " 'tends',\n",
       " 'serve',\n",
       " '^^^call',\n",
       " 'ear',\n",
       " 'support',\n",
       " 'gave',\n",
       " 'writer',\n",
       " 'submit',\n",
       " 'everyday',\n",
       " 'official',\n",
       " 'watching',\n",
       " 'so',\n",
       " 'efficient',\n",
       " 'breaks',\n",
       " 'pve',\n",
       " '^bot',\n",
       " 'requirement',\n",
       " 'summon',\n",
       " 'thinking',\n",
       " 'angle',\n",
       " 'dnc',\n",
       " 'hey',\n",
       " 'worries',\n",
       " '15',\n",
       " 'weaknesses',\n",
       " 'crate',\n",
       " 'easier',\n",
       " 'digress',\n",
       " 'small',\n",
       " 'equipped',\n",
       " 'spent',\n",
       " 'cared',\n",
       " 'aye',\n",
       " 'half',\n",
       " 'nightmare',\n",
       " 'money.',\n",
       " 'harmful',\n",
       " 'torrie',\n",
       " 'stock',\n",
       " 'replied',\n",
       " '\\\\n\\\\nwhen',\n",
       " 'aoe',\n",
       " 'songs',\n",
       " 'achieve',\n",
       " 'admittedly',\n",
       " 'smoking',\n",
       " 'highway',\n",
       " 'figuring',\n",
       " 'adding',\n",
       " 'suddenly',\n",
       " 'yet',\n",
       " 'collective',\n",
       " 'default',\n",
       " 'there',\n",
       " 'tries',\n",
       " '7th',\n",
       " 'looks',\n",
       " 'aggressive',\n",
       " 'manga',\n",
       " 'signs',\n",
       " 'launch',\n",
       " 'pitch',\n",
       " 'declare',\n",
       " 'bay',\n",
       " 'brief',\n",
       " 'uh',\n",
       " 'bass',\n",
       " 'ultimately',\n",
       " 'hard\\\\n\\\\n\\\\\\\\',\n",
       " 'dvd',\n",
       " 'shooter',\n",
       " 'pull',\n",
       " 'dodgers',\n",
       " 'home',\n",
       " 'z',\n",
       " 'earn',\n",
       " 'opponents',\n",
       " 'goes',\n",
       " \"'don\\\\'t\",\n",
       " 'personality',\n",
       " '360',\n",
       " 'removing',\n",
       " 'popular',\n",
       " 'capitalism',\n",
       " 'stack',\n",
       " 'reminded',\n",
       " \"'yep\",\n",
       " 'sword',\n",
       " '//www.tineye.com/search',\n",
       " 'alcohol',\n",
       " 'claims',\n",
       " '//offerando.info/football',\n",
       " 'urge',\n",
       " 'service',\n",
       " 'expense',\n",
       " 'hour',\n",
       " 'oil',\n",
       " 'witcher',\n",
       " 'top',\n",
       " 'modern',\n",
       " 'dollars',\n",
       " 'recently',\n",
       " 'subscribe',\n",
       " 'had',\n",
       " 'tiny',\n",
       " 'imagine',\n",
       " 'removed',\n",
       " 'effects',\n",
       " 'child',\n",
       " 'tbh',\n",
       " 'swear',\n",
       " 'steam\\\\n',\n",
       " 'allow',\n",
       " 'cultural',\n",
       " 'hilarious',\n",
       " 'contacting',\n",
       " 'rough',\n",
       " 'yesterday',\n",
       " 'writing',\n",
       " 'respectful',\n",
       " 'conservative',\n",
       " 'main',\n",
       " 'bought',\n",
       " 'rules.\\\\n\\\\n',\n",
       " 'addressing',\n",
       " 'videos',\n",
       " 'awesome',\n",
       " 'climate',\n",
       " 'spectrum',\n",
       " 'have',\n",
       " 'noting',\n",
       " 'used',\n",
       " 'villain',\n",
       " 'equate',\n",
       " 'paying',\n",
       " 'accused',\n",
       " 'camera',\n",
       " 'connection',\n",
       " 'serious',\n",
       " 'fiance',\n",
       " 'feed',\n",
       " 'out',\n",
       " '^^|',\n",
       " 'heard',\n",
       " 'came',\n",
       " 'situation',\n",
       " 'difficult',\n",
       " 'reaction',\n",
       " 'comment',\n",
       " \"'it\",\n",
       " 'dislike',\n",
       " 'including',\n",
       " 'fights',\n",
       " 'justification',\n",
       " 'feminist',\n",
       " 'hardest',\n",
       " '10',\n",
       " 'to=/r/randomactsofblowjob',\n",
       " 'causing',\n",
       " 'harder',\n",
       " 'prepare',\n",
       " 'mmr',\n",
       " 'npcs',\n",
       " '0aplease',\n",
       " 'severe',\n",
       " 'sound',\n",
       " 'hung',\n",
       " 'friendly',\n",
       " 'potter',\n",
       " 'promise',\n",
       " 'dragon',\n",
       " 'taking',\n",
       " \"'people\",\n",
       " 'still',\n",
       " 'trolls',\n",
       " 'else\\\\',\n",
       " 'mistaken',\n",
       " 'wealthy',\n",
       " 'wet',\n",
       " 'political',\n",
       " 'friday',\n",
       " 'loop',\n",
       " 'specs',\n",
       " 'troops',\n",
       " 'late',\n",
       " 'brothers',\n",
       " 'designed',\n",
       " 'strengths',\n",
       " 'genius',\n",
       " '/r/advice',\n",
       " 'og',\n",
       " 'conditions',\n",
       " 'began',\n",
       " \"'physics'\\\\n\\\\n\",\n",
       " 'forum',\n",
       " 'conclude',\n",
       " 'illegal',\n",
       " 'lf3m',\n",
       " 'loompa',\n",
       " 'suited',\n",
       " \"'computing\",\n",
       " '^/',\n",
       " 'who',\n",
       " 'listentothis',\n",
       " 'literal',\n",
       " '14',\n",
       " 'actions',\n",
       " 'closed',\n",
       " 'developers',\n",
       " 'middle',\n",
       " 'refused',\n",
       " 'danger',\n",
       " 'office',\n",
       " 'placed',\n",
       " 'butt',\n",
       " 'salary',\n",
       " 'mountain',\n",
       " 'experiences',\n",
       " 'mexican',\n",
       " 'dinner',\n",
       " 'machines',\n",
       " 'culture',\n",
       " 'ok.',\n",
       " '6',\n",
       " '18th',\n",
       " 'barracuda',\n",
       " 'talking',\n",
       " 'defeats',\n",
       " 'verified',\n",
       " 'shooting',\n",
       " 'promote',\n",
       " 'easily',\n",
       " 'abstract',\n",
       " 'lean',\n",
       " 'seeks',\n",
       " 'management',\n",
       " 'orange',\n",
       " 'wiki',\n",
       " 'human',\n",
       " 'load',\n",
       " '1st',\n",
       " 'prevent',\n",
       " 'dick',\n",
       " '2012',\n",
       " 'chief',\n",
       " 'cousin',\n",
       " 'images',\n",
       " 'v',\n",
       " 'speak',\n",
       " 'somewhere',\n",
       " 'tomorrow',\n",
       " 'picks',\n",
       " 'guild',\n",
       " 'arguments',\n",
       " 'freaking',\n",
       " 'spicy',\n",
       " 'companies',\n",
       " '//reddit.com/r/randomactsofblowjob/search',\n",
       " 'volume',\n",
       " 'equal',\n",
       " '20my',\n",
       " 'evo-series',\n",
       " 'common',\n",
       " 'sit',\n",
       " 'same',\n",
       " 'meet',\n",
       " 'mint',\n",
       " 'wave',\n",
       " 'logical',\n",
       " 'pay',\n",
       " 'bored',\n",
       " '//www.reddit.com/r/globaloffensivetrade/wiki/rules',\n",
       " 'fixed',\n",
       " 'republican',\n",
       " 'programs',\n",
       " 'incredibly',\n",
       " 'compete',\n",
       " 'hurts',\n",
       " 'cooking',\n",
       " 'pages',\n",
       " 'consider',\n",
       " 'towards',\n",
       " 'dead',\n",
       " 'additional',\n",
       " 'beliefs',\n",
       " 'chat',\n",
       " 'two',\n",
       " 'playoffs',\n",
       " 'exclusive',\n",
       " 'pipe',\n",
       " 'skill',\n",
       " 'contact',\n",
       " 'run',\n",
       " 'supported',\n",
       " 'm',\n",
       " 'non',\n",
       " '80',\n",
       " 'bug',\n",
       " '//www.reddit.com/r/remindmebot/comments/24duzp/remindmebot_info/',\n",
       " \"'to\",\n",
       " 'to=/r/globaloffensive',\n",
       " '20thread',\n",
       " 'combination',\n",
       " 'nsfwfilter=off',\n",
       " 'odd',\n",
       " 'voted',\n",
       " 'forms',\n",
       " 'french',\n",
       " 'trusted',\n",
       " 'comments',\n",
       " '.5bserious.5d_tags',\n",
       " '0d',\n",
       " 'casual',\n",
       " 'clan',\n",
       " 'shave',\n",
       " 'lined',\n",
       " 'mecha',\n",
       " 'jeg',\n",
       " 'year',\n",
       " 'strike',\n",
       " 'consult',\n",
       " 'raises',\n",
       " 'quote',\n",
       " 'super',\n",
       " 'just',\n",
       " 'weather',\n",
       " 'dropped',\n",
       " 'remember',\n",
       " 'prove',\n",
       " 'business',\n",
       " 'calories',\n",
       " 'resolution',\n",
       " 'everytime',\n",
       " 'bright',\n",
       " 'paris',\n",
       " '\\\\',\n",
       " 'busy',\n",
       " 'pt',\n",
       " 'engaged',\n",
       " 'reasonable',\n",
       " 'computer',\n",
       " '20post',\n",
       " 'currently',\n",
       " 'native',\n",
       " 'roastee',\n",
       " 'recognize',\n",
       " 'cancel',\n",
       " 'minds',\n",
       " 'located',\n",
       " 'jet',\n",
       " 'ownership',\n",
       " 'better.',\n",
       " 'housing',\n",
       " 'soon',\n",
       " 'rb',\n",
       " 'dating',\n",
       " 'change',\n",
       " 'definition',\n",
       " 'relation',\n",
       " 'baby',\n",
       " '\\\\ngoogle|',\n",
       " 'factory',\n",
       " 'drinks',\n",
       " 'caring',\n",
       " 'free',\n",
       " 'to',\n",
       " 'stand',\n",
       " 'garden',\n",
       " 'beating',\n",
       " 'solely',\n",
       " 'contain',\n",
       " 'reminder',\n",
       " 'called',\n",
       " 'fair',\n",
       " '//m.youtube.com/watch',\n",
       " 'complain',\n",
       " 'essentially',\n",
       " 'en',\n",
       " 'root',\n",
       " '^',\n",
       " 'scared',\n",
       " \"'according\",\n",
       " 'initiation',\n",
       " 'imdb',\n",
       " \"'ok\",\n",
       " 'post.\\\\n\\\\n',\n",
       " 'attack',\n",
       " 'heads',\n",
       " 'twice',\n",
       " 'absolutely',\n",
       " 'counting',\n",
       " 'adds',\n",
       " 'lol',\n",
       " 'percent',\n",
       " 'op',\n",
       " 'was',\n",
       " 'period',\n",
       " 'zedd-',\n",
       " 'causes',\n",
       " 'pavement',\n",
       " 'spread',\n",
       " '.\\\\n\\\\n\\\\n',\n",
       " 'samsung',\n",
       " 'women',\n",
       " 'loan',\n",
       " 'virginia',\n",
       " 'communicate',\n",
       " 'doc',\n",
       " 'ladder',\n",
       " 'ad',\n",
       " 'god',\n",
       " 'impossible',\n",
       " 'christ',\n",
       " 'facto',\n",
       " 'que',\n",
       " 'inch',\n",
       " 'tension',\n",
       " \"'of\",\n",
       " 'background',\n",
       " 'last',\n",
       " 'imo',\n",
       " 'thats',\n",
       " 'mental',\n",
       " 'young',\n",
       " 'tool',\n",
       " 'votes',\n",
       " '5-6',\n",
       " 'tv',\n",
       " 'gigabyte',\n",
       " 'california',\n",
       " 'ranked',\n",
       " 'eye',\n",
       " 'to=/r/videos',\n",
       " 'laws',\n",
       " 'accept',\n",
       " 'bone',\n",
       " 'battle',\n",
       " 'wish',\n",
       " 'into',\n",
       " 'concern',\n",
       " 'happy',\n",
       " 'way',\n",
       " 'you.\\\\n\\\\n',\n",
       " 'linking',\n",
       " \"'lol\",\n",
       " 'forget',\n",
       " 'stars',\n",
       " 'should',\n",
       " 'concerns',\n",
       " '16',\n",
       " 'nuts',\n",
       " '//www.reddit.com/r/askscience/search',\n",
       " 'ie',\n",
       " 'repeat',\n",
       " 'wiki_vague_titles',\n",
       " 'looked',\n",
       " 'coma',\n",
       " 'move',\n",
       " 'deserves',\n",
       " 'screwed',\n",
       " 'writers',\n",
       " 'lot',\n",
       " 'defensive',\n",
       " 'textbox',\n",
       " 'himself',\n",
       " 'valuable',\n",
       " 'poster',\n",
       " 'insecure',\n",
       " 'willing',\n",
       " \"'for\",\n",
       " '//www.reddit.com/r/havoc_bot',\n",
       " '^^^debug|',\n",
       " 'development',\n",
       " 'creating',\n",
       " 'excited',\n",
       " 'olds',\n",
       " '^message',\n",
       " 'ui',\n",
       " 'data',\n",
       " 'imply',\n",
       " 'tight',\n",
       " 'learning',\n",
       " 'obvious',\n",
       " 'participate',\n",
       " \"'while\",\n",
       " 'ugly',\n",
       " 'surrounded',\n",
       " '2014',\n",
       " 'calm',\n",
       " 'mine',\n",
       " 'south',\n",
       " 'jump',\n",
       " 'pink',\n",
       " 'asap',\n",
       " 'bringing',\n",
       " 'spending',\n",
       " 'decrease',\n",
       " 'flooded',\n",
       " 'mostly',\n",
       " 'envy',\n",
       " 'afford',\n",
       " 'everyone\\\\',\n",
       " 'manner',\n",
       " 'premium',\n",
       " 'substantial',\n",
       " 'episodes',\n",
       " 'deal',\n",
       " 'shirt',\n",
       " 'stressed',\n",
       " 'uncomfortable',\n",
       " 'application',\n",
       " '^info',\n",
       " '^\\\\\\\\',\n",
       " 'establishment',\n",
       " 'catching',\n",
       " 'screws',\n",
       " 'prices',\n",
       " 'matter',\n",
       " 'to=millennialdan',\n",
       " \"'thanks\",\n",
       " 'distance',\n",
       " 'played',\n",
       " 'idea',\n",
       " 'based',\n",
       " 'abandoned',\n",
       " 'trash',\n",
       " 'treat',\n",
       " 'continue',\n",
       " \"'what\",\n",
       " 'pace',\n",
       " 'foreign',\n",
       " 'seem',\n",
       " '^^^feedback',\n",
       " 'rehost',\n",
       " 'truth',\n",
       " 'family',\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_set = set(word for bigram, count in sorted(b.items(), key=lambda kv: -kv[1])[:30000] for word in bigram)\n",
    "word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b5fb9b-de16-43f6-ae5c-09c91a61ff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5138"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = len(word_set)\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db1305a-f192-45ff-93ef-475d0863dca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 0,\n",
       " 'future': 1,\n",
       " 'able': 2,\n",
       " 'drunk': 3,\n",
       " 'hence': 4,\n",
       " 'festival': 5,\n",
       " 'belongs': 6,\n",
       " 'option': 7,\n",
       " 'cute': 8,\n",
       " 'our': 9,\n",
       " 'problems': 10,\n",
       " 'visited': 11,\n",
       " 'pile': 12,\n",
       " 'happen': 13,\n",
       " 'etc': 14,\n",
       " 'accepted': 15,\n",
       " 'absence': 16,\n",
       " 'add': 17,\n",
       " 'hated': 18,\n",
       " 'bella': 19,\n",
       " 'apparently': 20,\n",
       " '1.': 21,\n",
       " 'army': 22,\n",
       " 'battlefield': 23,\n",
       " '60': 24,\n",
       " '|\\\\n': 25,\n",
       " \"'right\": 26,\n",
       " 'to=/r/explainlikeimfive': 27,\n",
       " '95': 28,\n",
       " 'chick': 29,\n",
       " 'quickly': 30,\n",
       " 'era': 31,\n",
       " 'merely': 32,\n",
       " 'vs': 33,\n",
       " 'subreddits': 34,\n",
       " 'downvoted': 35,\n",
       " 'together': 36,\n",
       " 'engineer': 37,\n",
       " 'match': 38,\n",
       " '//0fs.me/yis5stledr\\\\n\\\\nhttp': 39,\n",
       " 'awaiting': 40,\n",
       " 'press': 41,\n",
       " 'queue': 42,\n",
       " 'note': 43,\n",
       " 'wrote': 44,\n",
       " 'advance': 45,\n",
       " 'nowadays': 46,\n",
       " 'save': 47,\n",
       " 'roads': 48,\n",
       " 'could': 49,\n",
       " 'dust': 50,\n",
       " 'stairs': 51,\n",
       " 'dice': 52,\n",
       " 'total': 53,\n",
       " 'lose': 54,\n",
       " 'belong': 55,\n",
       " 'rock': 56,\n",
       " 'distraction': 57,\n",
       " 'justify': 58,\n",
       " '\\\\n\\\\nno': 59,\n",
       " '\\\\n\\\\nwith': 60,\n",
       " 'benner': 61,\n",
       " 'indeed': 62,\n",
       " 'etc.': 63,\n",
       " 'already': 64,\n",
       " 'compensate': 65,\n",
       " 'during': 66,\n",
       " 'finished': 67,\n",
       " 'chase': 68,\n",
       " 'reminds': 69,\n",
       " 'can': 70,\n",
       " 'via': 71,\n",
       " 'guessing': 72,\n",
       " 'taken': 73,\n",
       " 'your': 74,\n",
       " 'attached': 75,\n",
       " 'level': 76,\n",
       " 'toronto': 77,\n",
       " '^your': 78,\n",
       " 'across': 79,\n",
       " 'realistically': 80,\n",
       " 'course': 81,\n",
       " 'impact': 82,\n",
       " '+': 83,\n",
       " '\\\\n\\\\nsorry': 84,\n",
       " 'pieces': 85,\n",
       " 'species': 86,\n",
       " 'frowned': 87,\n",
       " '/': 88,\n",
       " 'piss': 89,\n",
       " 'mentality': 90,\n",
       " 'money': 91,\n",
       " '80s': 92,\n",
       " 'geforce': 93,\n",
       " 'published': 94,\n",
       " 'introduced': 95,\n",
       " '\\\\n\\\\ni': 96,\n",
       " 'vet': 97,\n",
       " 'onto': 98,\n",
       " 'foundation': 99,\n",
       " 'norm': 100,\n",
       " 'following': 101,\n",
       " '\\\\n\\\\npeople': 102,\n",
       " 'fashioned': 103,\n",
       " 'training': 104,\n",
       " \"'you\\\\\": 105,\n",
       " 'times': 106,\n",
       " 'incident': 107,\n",
       " 'aim': 108,\n",
       " \"'so\": 109,\n",
       " '80+': 110,\n",
       " 'forces': 111,\n",
       " 'disappointed': 112,\n",
       " 'such': 113,\n",
       " 'ticket': 114,\n",
       " 'compliment': 115,\n",
       " 'heroes': 116,\n",
       " 'upset': 117,\n",
       " 'cities': 118,\n",
       " 'collecters': 119,\n",
       " '\\\\nlow|': 120,\n",
       " '^^^on': 121,\n",
       " 'specific': 122,\n",
       " 'guns': 123,\n",
       " 'reach': 124,\n",
       " 'difference': 125,\n",
       " 'benefit': 126,\n",
       " 'counseling': 127,\n",
       " 'nature': 128,\n",
       " 'vault': 129,\n",
       " 'would\\\\': 130,\n",
       " 'periods': 131,\n",
       " 'ya': 132,\n",
       " \"'if\": 133,\n",
       " 'men': 134,\n",
       " 'corsair': 135,\n",
       " 'decision': 136,\n",
       " 'fully-modular': 137,\n",
       " 'sports': 138,\n",
       " 'free\\\\n': 139,\n",
       " 'beer': 140,\n",
       " 'casting': 141,\n",
       " 'repost': 142,\n",
       " 'january': 143,\n",
       " 'expand': 144,\n",
       " 'banned': 145,\n",
       " '//www.reddit.com/r/askreddit/wiki/index': 146,\n",
       " 'sand': 147,\n",
       " 'rug': 148,\n",
       " 'capture': 149,\n",
       " 'chill': 150,\n",
       " 'growth': 151,\n",
       " 'explain': 152,\n",
       " 'whom': 153,\n",
       " 'doesnt': 154,\n",
       " 'when': 155,\n",
       " 'winter': 156,\n",
       " 'sucked': 157,\n",
       " 'know': 158,\n",
       " 'fit': 159,\n",
       " 'important': 160,\n",
       " 'coverage': 161,\n",
       " 'become': 162,\n",
       " 'soft': 163,\n",
       " 'dr': 164,\n",
       " 'tempted': 165,\n",
       " 'francisco': 166,\n",
       " 'naturally': 167,\n",
       " '1234-1234-1234': 168,\n",
       " 'mall': 169,\n",
       " 'salt': 170,\n",
       " '//www.modojo.com/features/nintendo_3ds_friend_codes_everything_you_need_to_know': 171,\n",
       " 'technology': 172,\n",
       " '\\\\n\\\\nit': 173,\n",
       " 'campus': 174,\n",
       " '27s': 175,\n",
       " 'certainly': 176,\n",
       " 'lots': 177,\n",
       " 'trigger': 178,\n",
       " 'beach': 179,\n",
       " \"'this\": 180,\n",
       " '//www.reddit.com/r/askscience/wiki/quickstart/askingquestions': 181,\n",
       " 'interacted': 182,\n",
       " 'usa': 183,\n",
       " '25': 184,\n",
       " 'somehow': 185,\n",
       " 'discover': 186,\n",
       " '\\\\n\\\\notherwise': 187,\n",
       " 'view': 188,\n",
       " 'crowd': 189,\n",
       " 'party': 190,\n",
       " 'npc': 191,\n",
       " 'choices': 192,\n",
       " '^^^rhavocbot': 193,\n",
       " 'fee': 194,\n",
       " 'visible': 195,\n",
       " 'equipment': 196,\n",
       " 'reminders': 197,\n",
       " 'command': 198,\n",
       " 'minor': 199,\n",
       " 'produce': 200,\n",
       " 'self': 201,\n",
       " 'usual': 202,\n",
       " 'satisfied': 203,\n",
       " 'argued': 204,\n",
       " 'million': 205,\n",
       " 'yadda': 206,\n",
       " 'moderators': 207,\n",
       " 'agency': 208,\n",
       " 'now': 209,\n",
       " 'copy': 210,\n",
       " 'bare': 211,\n",
       " 'fall': 212,\n",
       " 'repair': 213,\n",
       " 'u.c': 214,\n",
       " 'subject=delete': 215,\n",
       " 'but': 216,\n",
       " 'assets': 217,\n",
       " 'plans': 218,\n",
       " 'version': 219,\n",
       " 'roast': 220,\n",
       " 'grown': 221,\n",
       " 'faith': 222,\n",
       " 'fourth': 223,\n",
       " 'for': 224,\n",
       " 'incentive': 225,\n",
       " 'possible.': 226,\n",
       " 'faq': 227,\n",
       " 'cooldown': 228,\n",
       " 'ads': 229,\n",
       " 'mini': 230,\n",
       " '/r/videos.\\\\n\\\\n': 231,\n",
       " 'ready': 232,\n",
       " 'giveaway': 233,\n",
       " 'look': 234,\n",
       " '\\\\\\\\n': 235,\n",
       " 'him': 236,\n",
       " 'homeless': 237,\n",
       " 'choosing': 238,\n",
       " 'scratch': 239,\n",
       " 'one.\\\\n\\\\nrule': 240,\n",
       " 'could\\\\': 241,\n",
       " 'died': 242,\n",
       " 'well': 243,\n",
       " 'restraining': 244,\n",
       " 'dog': 245,\n",
       " 'engage': 246,\n",
       " 'appeal': 247,\n",
       " 'mode': 248,\n",
       " 'cash': 249,\n",
       " 'tactics': 250,\n",
       " 'ball': 251,\n",
       " 'anymore.': 252,\n",
       " 'choose': 253,\n",
       " 'stems': 254,\n",
       " '\\\\n\\\\n6': 255,\n",
       " 'p_35': 256,\n",
       " 'sexual': 257,\n",
       " 'concrete': 258,\n",
       " 'sort': 259,\n",
       " 'claiming': 260,\n",
       " '20review': 261,\n",
       " 'rise': 262,\n",
       " \"'how\": 263,\n",
       " 'purpose': 264,\n",
       " 'hills': 265,\n",
       " 'grid': 266,\n",
       " 'effective': 267,\n",
       " 'server': 268,\n",
       " 'imaginary': 269,\n",
       " '//www.direct2drive.com/': 270,\n",
       " 'turret': 271,\n",
       " 'these': 272,\n",
       " 'sides': 273,\n",
       " 'semi-modular': 274,\n",
       " 'education': 275,\n",
       " 'responses': 276,\n",
       " 'appealing': 277,\n",
       " 'council': 278,\n",
       " 'deeper': 279,\n",
       " 'to.\\\\n\\\\nif': 280,\n",
       " '^a': 281,\n",
       " 'plenty': 282,\n",
       " 'it.\\\\n\\\\n': 283,\n",
       " \"'here\": 284,\n",
       " 'playlist': 285,\n",
       " 'involving': 286,\n",
       " '^^^if': 287,\n",
       " 'budget': 288,\n",
       " 'musician': 289,\n",
       " 'korea': 290,\n",
       " 'laid': 291,\n",
       " 'interesting': 292,\n",
       " 'obsession': 293,\n",
       " 'adult': 294,\n",
       " 'pvp': 295,\n",
       " 'what': 296,\n",
       " 'cares': 297,\n",
       " 'admitted': 298,\n",
       " 'conservatives': 299,\n",
       " 'rarely': 300,\n",
       " '75': 301,\n",
       " 'strange': 302,\n",
       " ',': 303,\n",
       " 'hook': 304,\n",
       " 'appreciate': 305,\n",
       " 'blocksize': 306,\n",
       " 'blow': 307,\n",
       " 'quality': 308,\n",
       " 'brother': 309,\n",
       " 'liberals': 310,\n",
       " 'feedback\\\\n': 311,\n",
       " 'enough': 312,\n",
       " 'subject=help': 313,\n",
       " 'steps': 314,\n",
       " 'fields': 315,\n",
       " 'i7': 316,\n",
       " 'cope': 317,\n",
       " 'frequency': 318,\n",
       " 'whether': 319,\n",
       " 'sites': 320,\n",
       " '//www.reddit.com/r/roboragi/wiki/index': 321,\n",
       " 'using': 322,\n",
       " 'values': 323,\n",
       " 'sign': 324,\n",
       " 'foster': 325,\n",
       " '2fredd.it': 326,\n",
       " 'barely': 327,\n",
       " 'action': 328,\n",
       " 'restrict_sr=on': 329,\n",
       " 'calls': 330,\n",
       " 'up.': 331,\n",
       " 'assuming': 332,\n",
       " '/r/mylittlepony/comments/1lwzub/deviantart_imgur_mirror_bot_nightmirrormoon/': 333,\n",
       " 'haki': 334,\n",
       " 'cardname': 335,\n",
       " 'illness': 336,\n",
       " 'function': 337,\n",
       " 'wrath': 338,\n",
       " 'opens': 339,\n",
       " 'happened': 340,\n",
       " 'european': 341,\n",
       " 'since': 342,\n",
       " '/r/asksciencediscussion': 343,\n",
       " \"'social\": 344,\n",
       " 'degree': 345,\n",
       " 'timer': 346,\n",
       " 'universe': 347,\n",
       " 'permalink': 348,\n",
       " 'window': 349,\n",
       " 'say': 350,\n",
       " 'engine': 351,\n",
       " 'exercise': 352,\n",
       " 'vacation': 353,\n",
       " 'pulled': 354,\n",
       " 'results': 355,\n",
       " 'significant': 356,\n",
       " 'requirements': 357,\n",
       " 'count': 358,\n",
       " 'lacking': 359,\n",
       " 'comet': 360,\n",
       " 'appears': 361,\n",
       " 'influence': 362,\n",
       " 'basis': 363,\n",
       " '\\\\nyou': 364,\n",
       " 'sentence': 365,\n",
       " '6\\\\': 366,\n",
       " 'articles': 367,\n",
       " 'measure': 368,\n",
       " 'lo': 369,\n",
       " '//www.reddit.com/r/pricezombie/wiki/index': 370,\n",
       " 'global': 371,\n",
       " 'gym': 372,\n",
       " 'ratio': 373,\n",
       " 'everywhere': 374,\n",
       " \"'wow\": 375,\n",
       " \"'engineering\": 376,\n",
       " '20of': 377,\n",
       " 'creepy': 378,\n",
       " 'defend': 379,\n",
       " 'madison': 380,\n",
       " 'message=': 381,\n",
       " '^gosh': 382,\n",
       " 'claimed': 383,\n",
       " 'generated': 384,\n",
       " 'console': 385,\n",
       " 'congrats': 386,\n",
       " 'radio': 387,\n",
       " 'way.': 388,\n",
       " '!': 389,\n",
       " 'unique': 390,\n",
       " 'blast': 391,\n",
       " 'par': 392,\n",
       " '\\\\n\\\\npersonally': 393,\n",
       " 'miles': 394,\n",
       " 'silver': 395,\n",
       " 'girls': 396,\n",
       " 'q=': 397,\n",
       " 'vast': 398,\n",
       " 'palestinian': 399,\n",
       " 'everybody': 400,\n",
       " 'winning': 401,\n",
       " 'active': 402,\n",
       " 'r': 403,\n",
       " 'earthfans': 404,\n",
       " 'demand': 405,\n",
       " 'speakers': 406,\n",
       " '\\\\nbut': 407,\n",
       " 'shootings': 408,\n",
       " 'nyc': 409,\n",
       " 'depending': 410,\n",
       " 'special': 411,\n",
       " 'thing': 412,\n",
       " 'awoken': 413,\n",
       " '//github.com/buttscicles/tweetposter/issues': 414,\n",
       " 'process': 415,\n",
       " 'wage': 416,\n",
       " 'media': 417,\n",
       " 'voice': 418,\n",
       " 'oblivious': 419,\n",
       " 'pattern': 420,\n",
       " 'thought': 421,\n",
       " 'sense': 422,\n",
       " 'new': 423,\n",
       " 'jenner': 424,\n",
       " 'suppose': 425,\n",
       " 'existed': 426,\n",
       " 'split': 427,\n",
       " 'lt': 428,\n",
       " 'night': 429,\n",
       " 'down': 430,\n",
       " 'gambling': 431,\n",
       " 'buy': 432,\n",
       " 'without': 433,\n",
       " 'performs': 434,\n",
       " 'subject=post+review+request': 435,\n",
       " 'go': 436,\n",
       " 'added': 437,\n",
       " 'international': 438,\n",
       " '-\\\\n\\\\nthis': 439,\n",
       " 'anymore': 440,\n",
       " 'stats': 441,\n",
       " 'beneficial': 442,\n",
       " 'transport': 443,\n",
       " 'primary': 444,\n",
       " 'democratic': 445,\n",
       " 'gather': 446,\n",
       " 'fuck': 447,\n",
       " 'straight': 448,\n",
       " 'ban.\\\\n\\\\n': 449,\n",
       " 'hall': 450,\n",
       " 'dakka': 451,\n",
       " 'caught': 452,\n",
       " 'flat': 453,\n",
       " 'armor': 454,\n",
       " 'trump': 455,\n",
       " '\\\\n|-|-|-|-|-|': 456,\n",
       " 'giant': 457,\n",
       " 'y': 458,\n",
       " '\\\\n\\\\nfirst': 459,\n",
       " 'carry': 460,\n",
       " 'mana': 461,\n",
       " 'visual': 462,\n",
       " 'fill': 463,\n",
       " 'tends': 464,\n",
       " 'serve': 465,\n",
       " '^^^call': 466,\n",
       " 'ear': 467,\n",
       " 'support': 468,\n",
       " 'gave': 469,\n",
       " 'writer': 470,\n",
       " 'submit': 471,\n",
       " 'everyday': 472,\n",
       " 'official': 473,\n",
       " 'watching': 474,\n",
       " 'so': 475,\n",
       " 'efficient': 476,\n",
       " 'breaks': 477,\n",
       " 'pve': 478,\n",
       " '^bot': 479,\n",
       " 'requirement': 480,\n",
       " 'summon': 481,\n",
       " 'thinking': 482,\n",
       " 'angle': 483,\n",
       " 'dnc': 484,\n",
       " 'hey': 485,\n",
       " 'worries': 486,\n",
       " '15': 487,\n",
       " 'weaknesses': 488,\n",
       " 'crate': 489,\n",
       " 'easier': 490,\n",
       " 'digress': 491,\n",
       " 'small': 492,\n",
       " 'equipped': 493,\n",
       " 'spent': 494,\n",
       " 'cared': 495,\n",
       " 'aye': 496,\n",
       " 'half': 497,\n",
       " 'nightmare': 498,\n",
       " 'money.': 499,\n",
       " 'harmful': 500,\n",
       " 'torrie': 501,\n",
       " 'stock': 502,\n",
       " 'replied': 503,\n",
       " '\\\\n\\\\nwhen': 504,\n",
       " 'aoe': 505,\n",
       " 'songs': 506,\n",
       " 'achieve': 507,\n",
       " 'admittedly': 508,\n",
       " 'smoking': 509,\n",
       " 'highway': 510,\n",
       " 'figuring': 511,\n",
       " 'adding': 512,\n",
       " 'suddenly': 513,\n",
       " 'yet': 514,\n",
       " 'collective': 515,\n",
       " 'default': 516,\n",
       " 'there': 517,\n",
       " 'tries': 518,\n",
       " '7th': 519,\n",
       " 'looks': 520,\n",
       " 'aggressive': 521,\n",
       " 'manga': 522,\n",
       " 'signs': 523,\n",
       " 'launch': 524,\n",
       " 'pitch': 525,\n",
       " 'declare': 526,\n",
       " 'bay': 527,\n",
       " 'brief': 528,\n",
       " 'uh': 529,\n",
       " 'bass': 530,\n",
       " 'ultimately': 531,\n",
       " 'hard\\\\n\\\\n\\\\\\\\': 532,\n",
       " 'dvd': 533,\n",
       " 'shooter': 534,\n",
       " 'pull': 535,\n",
       " 'dodgers': 536,\n",
       " 'home': 537,\n",
       " 'z': 538,\n",
       " 'earn': 539,\n",
       " 'opponents': 540,\n",
       " 'goes': 541,\n",
       " \"'don\\\\'t\": 542,\n",
       " 'personality': 543,\n",
       " '360': 544,\n",
       " 'removing': 545,\n",
       " 'popular': 546,\n",
       " 'capitalism': 547,\n",
       " 'stack': 548,\n",
       " 'reminded': 549,\n",
       " \"'yep\": 550,\n",
       " 'sword': 551,\n",
       " '//www.tineye.com/search': 552,\n",
       " 'alcohol': 553,\n",
       " 'claims': 554,\n",
       " '//offerando.info/football': 555,\n",
       " 'urge': 556,\n",
       " 'service': 557,\n",
       " 'expense': 558,\n",
       " 'hour': 559,\n",
       " 'oil': 560,\n",
       " 'witcher': 561,\n",
       " 'top': 562,\n",
       " 'modern': 563,\n",
       " 'dollars': 564,\n",
       " 'recently': 565,\n",
       " 'subscribe': 566,\n",
       " 'had': 567,\n",
       " 'tiny': 568,\n",
       " 'imagine': 569,\n",
       " 'removed': 570,\n",
       " 'effects': 571,\n",
       " 'child': 572,\n",
       " 'tbh': 573,\n",
       " 'swear': 574,\n",
       " 'steam\\\\n': 575,\n",
       " 'allow': 576,\n",
       " 'cultural': 577,\n",
       " 'hilarious': 578,\n",
       " 'contacting': 579,\n",
       " 'rough': 580,\n",
       " 'yesterday': 581,\n",
       " 'writing': 582,\n",
       " 'respectful': 583,\n",
       " 'conservative': 584,\n",
       " 'main': 585,\n",
       " 'bought': 586,\n",
       " 'rules.\\\\n\\\\n': 587,\n",
       " 'addressing': 588,\n",
       " 'videos': 589,\n",
       " 'awesome': 590,\n",
       " 'climate': 591,\n",
       " 'spectrum': 592,\n",
       " 'have': 593,\n",
       " 'noting': 594,\n",
       " 'used': 595,\n",
       " 'villain': 596,\n",
       " 'equate': 597,\n",
       " 'paying': 598,\n",
       " 'accused': 599,\n",
       " 'camera': 600,\n",
       " 'connection': 601,\n",
       " 'serious': 602,\n",
       " 'fiance': 603,\n",
       " 'feed': 604,\n",
       " 'out': 605,\n",
       " '^^|': 606,\n",
       " 'heard': 607,\n",
       " 'came': 608,\n",
       " 'situation': 609,\n",
       " 'difficult': 610,\n",
       " 'reaction': 611,\n",
       " 'comment': 612,\n",
       " \"'it\": 613,\n",
       " 'dislike': 614,\n",
       " 'including': 615,\n",
       " 'fights': 616,\n",
       " 'justification': 617,\n",
       " 'feminist': 618,\n",
       " 'hardest': 619,\n",
       " '10': 620,\n",
       " 'to=/r/randomactsofblowjob': 621,\n",
       " 'causing': 622,\n",
       " 'harder': 623,\n",
       " 'prepare': 624,\n",
       " 'mmr': 625,\n",
       " 'npcs': 626,\n",
       " '0aplease': 627,\n",
       " 'severe': 628,\n",
       " 'sound': 629,\n",
       " 'hung': 630,\n",
       " 'friendly': 631,\n",
       " 'potter': 632,\n",
       " 'promise': 633,\n",
       " 'dragon': 634,\n",
       " 'taking': 635,\n",
       " \"'people\": 636,\n",
       " 'still': 637,\n",
       " 'trolls': 638,\n",
       " 'else\\\\': 639,\n",
       " 'mistaken': 640,\n",
       " 'wealthy': 641,\n",
       " 'wet': 642,\n",
       " 'political': 643,\n",
       " 'friday': 644,\n",
       " 'loop': 645,\n",
       " 'specs': 646,\n",
       " 'troops': 647,\n",
       " 'late': 648,\n",
       " 'brothers': 649,\n",
       " 'designed': 650,\n",
       " 'strengths': 651,\n",
       " 'genius': 652,\n",
       " '/r/advice': 653,\n",
       " 'og': 654,\n",
       " 'conditions': 655,\n",
       " 'began': 656,\n",
       " \"'physics'\\\\n\\\\n\": 657,\n",
       " 'forum': 658,\n",
       " 'conclude': 659,\n",
       " 'illegal': 660,\n",
       " 'lf3m': 661,\n",
       " 'loompa': 662,\n",
       " 'suited': 663,\n",
       " \"'computing\": 664,\n",
       " '^/': 665,\n",
       " 'who': 666,\n",
       " 'listentothis': 667,\n",
       " 'literal': 668,\n",
       " '14': 669,\n",
       " 'actions': 670,\n",
       " 'closed': 671,\n",
       " 'developers': 672,\n",
       " 'middle': 673,\n",
       " 'refused': 674,\n",
       " 'danger': 675,\n",
       " 'office': 676,\n",
       " 'placed': 677,\n",
       " 'butt': 678,\n",
       " 'salary': 679,\n",
       " 'mountain': 680,\n",
       " 'experiences': 681,\n",
       " 'mexican': 682,\n",
       " 'dinner': 683,\n",
       " 'machines': 684,\n",
       " 'culture': 685,\n",
       " 'ok.': 686,\n",
       " '6': 687,\n",
       " '18th': 688,\n",
       " 'barracuda': 689,\n",
       " 'talking': 690,\n",
       " 'defeats': 691,\n",
       " 'verified': 692,\n",
       " 'shooting': 693,\n",
       " 'promote': 694,\n",
       " 'easily': 695,\n",
       " 'abstract': 696,\n",
       " 'lean': 697,\n",
       " 'seeks': 698,\n",
       " 'management': 699,\n",
       " 'orange': 700,\n",
       " 'wiki': 701,\n",
       " 'human': 702,\n",
       " 'load': 703,\n",
       " '1st': 704,\n",
       " 'prevent': 705,\n",
       " 'dick': 706,\n",
       " '2012': 707,\n",
       " 'chief': 708,\n",
       " 'cousin': 709,\n",
       " 'images': 710,\n",
       " 'v': 711,\n",
       " 'speak': 712,\n",
       " 'somewhere': 713,\n",
       " 'tomorrow': 714,\n",
       " 'picks': 715,\n",
       " 'guild': 716,\n",
       " 'arguments': 717,\n",
       " 'freaking': 718,\n",
       " 'spicy': 719,\n",
       " 'companies': 720,\n",
       " '//reddit.com/r/randomactsofblowjob/search': 721,\n",
       " 'volume': 722,\n",
       " 'equal': 723,\n",
       " '20my': 724,\n",
       " 'evo-series': 725,\n",
       " 'common': 726,\n",
       " 'sit': 727,\n",
       " 'same': 728,\n",
       " 'meet': 729,\n",
       " 'mint': 730,\n",
       " 'wave': 731,\n",
       " 'logical': 732,\n",
       " 'pay': 733,\n",
       " 'bored': 734,\n",
       " '//www.reddit.com/r/globaloffensivetrade/wiki/rules': 735,\n",
       " 'fixed': 736,\n",
       " 'republican': 737,\n",
       " 'programs': 738,\n",
       " 'incredibly': 739,\n",
       " 'compete': 740,\n",
       " 'hurts': 741,\n",
       " 'cooking': 742,\n",
       " 'pages': 743,\n",
       " 'consider': 744,\n",
       " 'towards': 745,\n",
       " 'dead': 746,\n",
       " 'additional': 747,\n",
       " 'beliefs': 748,\n",
       " 'chat': 749,\n",
       " 'two': 750,\n",
       " 'playoffs': 751,\n",
       " 'exclusive': 752,\n",
       " 'pipe': 753,\n",
       " 'skill': 754,\n",
       " 'contact': 755,\n",
       " 'run': 756,\n",
       " 'supported': 757,\n",
       " 'm': 758,\n",
       " 'non': 759,\n",
       " '80': 760,\n",
       " 'bug': 761,\n",
       " '//www.reddit.com/r/remindmebot/comments/24duzp/remindmebot_info/': 762,\n",
       " \"'to\": 763,\n",
       " 'to=/r/globaloffensive': 764,\n",
       " '20thread': 765,\n",
       " 'combination': 766,\n",
       " 'nsfwfilter=off': 767,\n",
       " 'odd': 768,\n",
       " 'voted': 769,\n",
       " 'forms': 770,\n",
       " 'french': 771,\n",
       " 'trusted': 772,\n",
       " 'comments': 773,\n",
       " '.5bserious.5d_tags': 774,\n",
       " '0d': 775,\n",
       " 'casual': 776,\n",
       " 'clan': 777,\n",
       " 'shave': 778,\n",
       " 'lined': 779,\n",
       " 'mecha': 780,\n",
       " 'jeg': 781,\n",
       " 'year': 782,\n",
       " 'strike': 783,\n",
       " 'consult': 784,\n",
       " 'raises': 785,\n",
       " 'quote': 786,\n",
       " 'super': 787,\n",
       " 'just': 788,\n",
       " 'weather': 789,\n",
       " 'dropped': 790,\n",
       " 'remember': 791,\n",
       " 'prove': 792,\n",
       " 'business': 793,\n",
       " 'calories': 794,\n",
       " 'resolution': 795,\n",
       " 'everytime': 796,\n",
       " 'bright': 797,\n",
       " 'paris': 798,\n",
       " '\\\\': 799,\n",
       " 'busy': 800,\n",
       " 'pt': 801,\n",
       " 'engaged': 802,\n",
       " 'reasonable': 803,\n",
       " 'computer': 804,\n",
       " '20post': 805,\n",
       " 'currently': 806,\n",
       " 'native': 807,\n",
       " 'roastee': 808,\n",
       " 'recognize': 809,\n",
       " 'cancel': 810,\n",
       " 'minds': 811,\n",
       " 'located': 812,\n",
       " 'jet': 813,\n",
       " 'ownership': 814,\n",
       " 'better.': 815,\n",
       " 'housing': 816,\n",
       " 'soon': 817,\n",
       " 'rb': 818,\n",
       " 'dating': 819,\n",
       " 'change': 820,\n",
       " 'definition': 821,\n",
       " 'relation': 822,\n",
       " 'baby': 823,\n",
       " '\\\\ngoogle|': 824,\n",
       " 'factory': 825,\n",
       " 'drinks': 826,\n",
       " 'caring': 827,\n",
       " 'free': 828,\n",
       " 'to': 829,\n",
       " 'stand': 830,\n",
       " 'garden': 831,\n",
       " 'beating': 832,\n",
       " 'solely': 833,\n",
       " 'contain': 834,\n",
       " 'reminder': 835,\n",
       " 'called': 836,\n",
       " 'fair': 837,\n",
       " '//m.youtube.com/watch': 838,\n",
       " 'complain': 839,\n",
       " 'essentially': 840,\n",
       " 'en': 841,\n",
       " 'root': 842,\n",
       " '^': 843,\n",
       " 'scared': 844,\n",
       " \"'according\": 845,\n",
       " 'initiation': 846,\n",
       " 'imdb': 847,\n",
       " \"'ok\": 848,\n",
       " 'post.\\\\n\\\\n': 849,\n",
       " 'attack': 850,\n",
       " 'heads': 851,\n",
       " 'twice': 852,\n",
       " 'absolutely': 853,\n",
       " 'counting': 854,\n",
       " 'adds': 855,\n",
       " 'lol': 856,\n",
       " 'percent': 857,\n",
       " 'op': 858,\n",
       " 'was': 859,\n",
       " 'period': 860,\n",
       " 'zedd-': 861,\n",
       " 'causes': 862,\n",
       " 'pavement': 863,\n",
       " 'spread': 864,\n",
       " '.\\\\n\\\\n\\\\n': 865,\n",
       " 'samsung': 866,\n",
       " 'women': 867,\n",
       " 'loan': 868,\n",
       " 'virginia': 869,\n",
       " 'communicate': 870,\n",
       " 'doc': 871,\n",
       " 'ladder': 872,\n",
       " 'ad': 873,\n",
       " 'god': 874,\n",
       " 'impossible': 875,\n",
       " 'christ': 876,\n",
       " 'facto': 877,\n",
       " 'que': 878,\n",
       " 'inch': 879,\n",
       " 'tension': 880,\n",
       " \"'of\": 881,\n",
       " 'background': 882,\n",
       " 'last': 883,\n",
       " 'imo': 884,\n",
       " 'thats': 885,\n",
       " 'mental': 886,\n",
       " 'young': 887,\n",
       " 'tool': 888,\n",
       " 'votes': 889,\n",
       " '5-6': 890,\n",
       " 'tv': 891,\n",
       " 'gigabyte': 892,\n",
       " 'california': 893,\n",
       " 'ranked': 894,\n",
       " 'eye': 895,\n",
       " 'to=/r/videos': 896,\n",
       " 'laws': 897,\n",
       " 'accept': 898,\n",
       " 'bone': 899,\n",
       " 'battle': 900,\n",
       " 'wish': 901,\n",
       " 'into': 902,\n",
       " 'concern': 903,\n",
       " 'happy': 904,\n",
       " 'way': 905,\n",
       " 'you.\\\\n\\\\n': 906,\n",
       " 'linking': 907,\n",
       " \"'lol\": 908,\n",
       " 'forget': 909,\n",
       " 'stars': 910,\n",
       " 'should': 911,\n",
       " 'concerns': 912,\n",
       " '16': 913,\n",
       " 'nuts': 914,\n",
       " '//www.reddit.com/r/askscience/search': 915,\n",
       " 'ie': 916,\n",
       " 'repeat': 917,\n",
       " 'wiki_vague_titles': 918,\n",
       " 'looked': 919,\n",
       " 'coma': 920,\n",
       " 'move': 921,\n",
       " 'deserves': 922,\n",
       " 'screwed': 923,\n",
       " 'writers': 924,\n",
       " 'lot': 925,\n",
       " 'defensive': 926,\n",
       " 'textbox': 927,\n",
       " 'himself': 928,\n",
       " 'valuable': 929,\n",
       " 'poster': 930,\n",
       " 'insecure': 931,\n",
       " 'willing': 932,\n",
       " \"'for\": 933,\n",
       " '//www.reddit.com/r/havoc_bot': 934,\n",
       " '^^^debug|': 935,\n",
       " 'development': 936,\n",
       " 'creating': 937,\n",
       " 'excited': 938,\n",
       " 'olds': 939,\n",
       " '^message': 940,\n",
       " 'ui': 941,\n",
       " 'data': 942,\n",
       " 'imply': 943,\n",
       " 'tight': 944,\n",
       " 'learning': 945,\n",
       " 'obvious': 946,\n",
       " 'participate': 947,\n",
       " \"'while\": 948,\n",
       " 'ugly': 949,\n",
       " 'surrounded': 950,\n",
       " '2014': 951,\n",
       " 'calm': 952,\n",
       " 'mine': 953,\n",
       " 'south': 954,\n",
       " 'jump': 955,\n",
       " 'pink': 956,\n",
       " 'asap': 957,\n",
       " 'bringing': 958,\n",
       " 'spending': 959,\n",
       " 'decrease': 960,\n",
       " 'flooded': 961,\n",
       " 'mostly': 962,\n",
       " 'envy': 963,\n",
       " 'afford': 964,\n",
       " 'everyone\\\\': 965,\n",
       " 'manner': 966,\n",
       " 'premium': 967,\n",
       " 'substantial': 968,\n",
       " 'episodes': 969,\n",
       " 'deal': 970,\n",
       " 'shirt': 971,\n",
       " 'stressed': 972,\n",
       " 'uncomfortable': 973,\n",
       " 'application': 974,\n",
       " '^info': 975,\n",
       " '^\\\\\\\\': 976,\n",
       " 'establishment': 977,\n",
       " 'catching': 978,\n",
       " 'screws': 979,\n",
       " 'prices': 980,\n",
       " 'matter': 981,\n",
       " 'to=millennialdan': 982,\n",
       " \"'thanks\": 983,\n",
       " 'distance': 984,\n",
       " 'played': 985,\n",
       " 'idea': 986,\n",
       " 'based': 987,\n",
       " 'abandoned': 988,\n",
       " 'trash': 989,\n",
       " 'treat': 990,\n",
       " 'continue': 991,\n",
       " \"'what\": 992,\n",
       " 'pace': 993,\n",
       " 'foreign': 994,\n",
       " 'seem': 995,\n",
       " '^^^feedback': 996,\n",
       " 'rehost': 997,\n",
       " 'truth': 998,\n",
       " 'family': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {s:i for i,s in enumerate(word_set)} # string to index\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "stoi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "528d2903-24ae-4b90-80b1-b5ceb9042fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'UNKNOWN_TOKEN' does not exist in the list tokenized_sentences.\n"
     ]
    }
   ],
   "source": [
    "found = False\n",
    "for sentence in tokenized_sentences:\n",
    "    if 'UNKNOWN_TOKEN' in sentence:\n",
    "        found = True\n",
    "        break\n",
    "\n",
    "if found:\n",
    "    print(\"'UNKNOWN_TOKEN' exists in the list tokenized_sentences.\")\n",
    "else:\n",
    "    print(\"'UNKNOWN_TOKEN' does not exist in the list tokenized_sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9284acdd-07cf-4df7-aac3-5cf267cf00de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START', \"'body\", \"'\", 'SENTENCE_END']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08589bc6-f148-4b23-bd5a-bf61bbdbf7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = [word for sentence in tokenized_sentences for word in sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa94e928-eaee-4ef3-a46b-c5d3ff47fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = [word for sentence in tokenized_sentences for word in sentence if word in word_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "082e812d-0072-4e01-860f-e20992026e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = [stoi[word] for word in tokenized_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38aeaa35-dbb9-44fc-aba4-6c6d2bd66a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500138\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e0cd637-a15f-414f-8967-79041bb0f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(tokenized_sentences)\n",
    "n1 = int(0.8 * len(tokenized_sentences))\n",
    "n2 = int(0.9 * len(tokenized_sentences))\n",
    "\n",
    "Xtr = tokenized_words[:n1]\n",
    "Ytr = tokenized_words[1:n1+1]\n",
    "Xdev = tokenized_words[n1:n2]\n",
    "Ydev = tokenized_words[n1+1:n2+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8db1e525-5277-4e72-9257-71e863f25e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54917\n",
      "[4749, 1835, 2953, 2234, 3621, 1589, 4802, 423, 2495, 2290]\n"
     ]
    }
   ],
   "source": [
    "print(len(Xtr))\n",
    "print(Ytr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69566eab-f3ca-4e4f-98f1-232750530dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5138\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    # Create four dictionaries\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    # Copy the last hidden state into hprev\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    ### Forward pass (each time step)\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        # At this time step, input an encoded char, as a (27, 5) one hot vector\n",
    "        xs[t][inputs[t]] = 1\n",
    "        # At this time step, follow the formulas to get the hidden state at this time step\n",
    "        # (100,27) @ (27, 1) = (100, 1), (100, 100) @ (100, 1) = (100, 1) + (100, 1) -> (100, 1) (column vector of 100 outputs\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "\n",
    "        # At this time step, compute the output state\n",
    "        # (27, 100) @ (100, 1) = (27, 1) + (27, 1) = (1, 27)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "\n",
    "        # Normalize our probabiltiies using softmax\n",
    "        # ps = (1, 27)\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "\n",
    "        # Take the loss of the correct output probabilities\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    ### Backward pass: compute gradients going backwards\n",
    "    # dWxh (100, 27), dWhh (100, 100), dWhy = (27, 100)\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    # dbh (100, 1),  dby (27, 1)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    # (100, 1)\n",
    "    # Start off with zero\n",
    "    dhnext = np.zeros_like(hs[0]) \n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backpropogate through the softmax to the logits\n",
    "        # (27, 5)\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\n",
    "        # (27, 5) @ (5, 100) = (27, 100)\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        # (27, 5) -> This secrety should sum across the columns dim = 1 to produce (27, 1)\n",
    "        dby += dy\n",
    "\n",
    "        # dh(t), dh(t)raw\n",
    "        # Backpropogate into the previous layer (100, 27) @ (27, 5) = (100, 5)\n",
    "        dh = np.dot(Why.T, dy) + dhnext\n",
    "        # Backprop through tanh nonlinearity (element-wise forward -> element-wise backward)\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh \n",
    "\n",
    "        # dWxh, dWhh, dbh\n",
    "        # dWxh = (100, 5) @ (5, 27) = (100, 27)\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        # dWhh = (100, 5) (5, 100) = (100, 100)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        # Calculate dbh and dWxh\n",
    "        dbh += dhraw\n",
    "\n",
    "        # Derive part of the next time step's gradient\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    # Clipping gradients\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "477f3cfd-f145-439e-b3f4-860a9b63a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n): # Passed in the previous inputs, the first letter of input, and 200 samples\n",
    "    \"\"\" \n",
    "    Sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    # Create a one hot vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[stoi['SENTENCE_START']] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96906098-7dc4-4582-88f7-627f0695a808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " choices ban.\\n\\n selling special 9734 nowhere submitter feminist humor prior luckily college attack estate \\n\\nnow dislike death knowledge freedom eventually opinion /r/totesmessenger anymore. owned social screws 50 //offerando.info/football admittedly subscribe permalink leading hook pace getting 10,000 incorrect nfl counter application duration dumbass inner agency solution classic to= therapist business contents mistaken still scam location struggle everyone fulfill caught relating graphics sat le store piece kidding \\nin plants usa drink showcase porn \\n\\ntype|item|price\\n relative expectations crown wrote 'human magic they 'omg lazy decks indicate stopped extreme among turkey improve suggesting blade file total lights society reached example thoughts hook defensive \\\\/r/randomactsofblowjob there brick 20report 'exactly brief chains big roads e around locations wow weapon comet advertising really pool lay take flee album not six re-approve meantime animation 20that men rehabilitation //www.p0ody-files.com/ff_to_ebook/mobile/makeepub.php ^info nbsp use story / flag troll scene equation que planning lasanta corsair eu relating currency guessing street students version chapter goal \\n\\n\\ni butt germans safari when imo messing & inside pump rewards weather vendor till one.\\n\\nrule \\r\\n\\r\\n______\\r\\n\\r\\n^^^i affects he\\ break chicken guide gt marry ; dig making safe occasion built forth find politics category random losing notorious locations visible names exist allow ^them extremely 'haha raider taxes tumblr| enforcement \n",
      "----\n",
      "iter 0, loss: 213.610474\n",
      "----\n",
      " song certain with me the , song song song song song song song song song to song think song being SENTENCE_START in song that song is this song song song a by being song to more , song they that song song it\\ song song song it song only song song i song song i song who it\\ being song song that into it : song , song other song being SENTENCE_START himself , song song a song not to . also song it\\ song song but song song song is being , being song song song song song song song song it\\ being it it\\ song song song song song , , song being song it song an song song song song song song song it song song , song song me song song in to song song song song song song song an song to it\\ song song 's song song 's that song not song all in , song song me song song song song song himself song it\\ that song song song on it\\ song song song it\\ it song it\\ song out song song , is which song lose song song song song song being \n",
      "----\n",
      "iter 100, loss: 215.888845\n",
      "----\n",
      " on you and and SENTENCE_END the isn\\'t your `` and there but SENTENCE_START `` SENTENCE_END as did SENTENCE_START i it SENTENCE_START `` `` did on the n't for for but as , better it but mom other n't the gon are `` requirements in bigger SENTENCE_END n't you this types them been SENTENCE_START the most having , baby other '' . SENTENCE_END SENTENCE_START the SENTENCE_END . SENTENCE_END SENTENCE_START do SENTENCE_END for the 's SENTENCE_END on SENTENCE_END to for . them it days are of work a on SENTENCE_END * original on the SENTENCE_END `` really make except any . much SENTENCE_START bb for SENTENCE_END the you for as that , it , a it just it a the and the in `` it it , more me been the * the it on well my the ticket . , SENTENCE_END requirements as 's SENTENCE_END boat him parents to me SENTENCE_END as . SENTENCE_END such SENTENCE_END the `` difference with SENTENCE_END probably think and for your SENTENCE_END as a refuse gt , is there end a my the 'm SENTENCE_START . is SENTENCE_END threw `` SENTENCE_END 's mean to 's having efficiently you im n't to day SENTENCE_START so as and \n",
      "----\n",
      "iter 200, loss: 212.882806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (txt, ))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Forward seq_length characters through the net and fetch gradient\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m loss, dWxh, dWhh, dWhy, dbh, dby, hprev \u001b[38;5;241m=\u001b[39m \u001b[43mlossFun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhprev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m smooth_loss \u001b[38;5;241m=\u001b[39m smooth_loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.999\u001b[39m \u001b[38;5;241m+\u001b[39m loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n",
      "Cell \u001b[0;32mIn[20], line 62\u001b[0m, in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     58\u001b[0m dhraw \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m hs[t] \u001b[38;5;241m*\u001b[39m hs[t]) \u001b[38;5;241m*\u001b[39m dh \n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# dWxh, dWhh, dbh\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# dWxh = (100, 5) @ (5, 27) = (100, 27)\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m dWxh \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdhraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# dWhh = (100, 5) (5, 100) = (100, 100)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m dWhh \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dhraw, hs[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 # input to hidden - (100, 27)\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 # hidden to hidden (100, 100)\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 # hidden to output (27, 100)\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias (100, 1)\n",
    "by = np.zeros((vocab_size, 1)) # output bias (27, 1)\n",
    "\n",
    "n, p = 0, 0\n",
    "\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size) * seq_length # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "    # Prepare inputs (Sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(tokenized_sentences) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # Go from start of data\n",
    "    inputs = Xtr[p:p+seq_length]\n",
    "    targets = Ytr[p:p+seq_length]\n",
    "\n",
    "    \n",
    "    # Sample from the model now and then, and print the result\n",
    "    if n % 100 == 0:\n",
    "        # Pass in the previous inputs, the first letter of input, and 200 samples\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ' '.join(itos[ix] for ix in sample_ix)\n",
    "        print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    \n",
    "    # Forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "\n",
    "\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: \n",
    "        print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update   \n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "71ea5368-5949-4d77-8168-c3ac1289665d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi['SENTENCE_END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fa2b2524-3491-4e0c-b976-4f99aa04d1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi['SENTENCE_START']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "640d7d03-0ac2-4fb9-8d7b-a2e70565743f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hprev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_ix \u001b[38;5;241m=\u001b[39m sample(\u001b[43mhprev\u001b[49m, \u001b[38;5;241m773\u001b[39m, \u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m      2\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(itos[ix] \u001b[38;5;28;01mfor\u001b[39;00m ix \u001b[38;5;129;01min\u001b[39;00m sample_ix)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (txt, ))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hprev' is not defined"
     ]
    }
   ],
   "source": [
    "sample_ix = sample(hprev, 773, 200)\n",
    "txt = ' '.join(itos[ix] for ix in sample_ix)\n",
    "print ('----\\n %s \\n----' % (txt, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
