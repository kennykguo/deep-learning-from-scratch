{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff91fc80-0bc4-471f-91e9-6c51ac49657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d27627-2fa1-4c6e-bfad-dd87852ae629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "Character to index mapping: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "Vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "print(words[:10])\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(f'Character to index mapping: {itos}')\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Shuffle the words\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac0e6ea-cbf7-48fa-98d6-9dcf2fe77f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(words):\n",
    "    encoded = []\n",
    "    for w in words:\n",
    "        encoded.extend([stoi[ch] for ch in '.' + w ])\n",
    "    return encoded\n",
    "\n",
    "encoded = encode_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f0bfad2-2aff-4766-82f4-4a8d4b9bc7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754cc12d-c2ee-4650-8287-0deadf2ac4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(encoded)\n",
    "n1 = int(0.8 * n)\n",
    "block_size = 8\n",
    "batch_size = 32\n",
    "\n",
    "train_seq = encoded[:n1]\n",
    "dev_seq = encoded[n1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1e98e8b-8081-4f09-811c-4c39040b7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(seq, block_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(0, len(seq) - block_size, block_size):\n",
    "        X.append(seq[i:i+block_size])\n",
    "        Y.append(seq[i+1:i+block_size+1])\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32)\n",
    "    return X, Y\n",
    "\n",
    "Xtr, Ytr = create_pairs(train_seq, block_size)\n",
    "Xdev, Ydev = create_pairs(dev_seq, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1caa97e5-c13d-417f-8c04-620c3685f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  9., 15., 14.,  4., 18.,  5.,  0.])\n",
      "tensor([ 9., 15., 14.,  4., 18.,  5.,  0., 24.])\n",
      "tensor([24.,  1., 22.,  9.,  5., 14.,  0., 10.])\n",
      "tensor([ 1., 22.,  9.,  5., 14.,  0., 10., 15.])\n",
      "tensor([15., 18.,  9.,  0., 10., 21.,  1., 14.])\n",
      "tensor([18.,  9.,  0., 10., 21.,  1., 14., 12.])\n",
      "Training data shapes - X: torch.Size([22814, 8]), Y: torch.Size([22814, 8])\n",
      "Development data shapes - X: torch.Size([5703, 8]), Y: torch.Size([5703, 8])\n"
     ]
    }
   ],
   "source": [
    "print(Xtr[1])\n",
    "print(Ytr[1])\n",
    "print(Xtr[2])\n",
    "print(Ytr[2])\n",
    "print(Xtr[3])\n",
    "print(Ytr[3])\n",
    "print(f'Training data shapes - X: {Xtr.shape}, Y: {Ytr.shape}')\n",
    "print(f'Development data shapes - X: {Xdev.shape}, Y: {Ydev.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7fddc288-34bc-4c65-bb6e-fa74c5799979",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "hidden_size = 50\n",
    "time_steps = 8\n",
    "input_size = 27\n",
    "\n",
    "# Parameters\n",
    "Fvh = torch.randn(vocab_size, hidden_size)\n",
    "i1vh = torch.randn(vocab_size, hidden_size)\n",
    "i2vh = torch.randn(vocab_size, hidden_size)\n",
    "Ovh = torch.randn(vocab_size, hidden_size)\n",
    "\n",
    "Fhh = torch.randn(hidden_size, hidden_size)\n",
    "i1hh = torch.randn(hidden_size, hidden_size)\n",
    "i2hh = torch.randn(hidden_size, hidden_size)\n",
    "Ohh = torch.randn(hidden_size, hidden_size)\n",
    "\n",
    "bias1 = torch.zeros(hidden_size)\n",
    "bias2 = torch.zeros(hidden_size)\n",
    "bias3 = torch.zeros(hidden_size)\n",
    "bias4 = torch.zeros(hidden_size)\n",
    "\n",
    "output_matrix = torch.randn(hidden_size, vocab_size)\n",
    "\n",
    "# Storage\n",
    "preact1 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "preact2 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "preact3 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "preact4 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "\n",
    "act1 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "act2 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "act3 = torch.zeros(time_steps, batch_size, hidden_size)\n",
    "act4 = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "\n",
    "Cin = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "Cout = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "Ctout = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "\n",
    "Hin = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "Hout = torch.zeros((time_steps, batch_size, hidden_size))\n",
    "\n",
    "logits = torch.zeros((time_steps, batch_size, vocab_size))\n",
    "\n",
    "c0 = torch.zeros(batch_size, hidden_size)\n",
    "h0 = torch.zeros((batch_size, hidden_size))\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "# To update\n",
    "dFvh = torch.zeros(vocab_size, hidden_size) / 0.0\n",
    "di1vh = torch.zeros(vocab_size, hidden_size) / 0.0\n",
    "di2vh = torch.zeros(vocab_size, hidden_size) / 0.0\n",
    "dOvh = torch.zeros(vocab_size, hidden_size) / 0.0\n",
    "\n",
    "dFhh = torch.zeros(hidden_size, hidden_size)/ 0.0\n",
    "di1hh = torch.zeros(hidden_size, hidden_size)/ 0.0\n",
    "di2hh = torch.zeros(hidden_size, hidden_size)/ 0.0\n",
    "dOhh = torch.zeros(hidden_size, hidden_size)/ 0.0\n",
    "\n",
    "dbias1 = torch.zeros(hidden_size) # (30)\n",
    "dbias2 = torch.zeros(hidden_size) # (30)\n",
    "dbias3 = torch.zeros(hidden_size) # (30)\n",
    "dbias4 = torch.zeros(hidden_size) # (30)\n",
    "\n",
    "doutput_matrix = torch.randn(hidden_size, vocab_size) # (30, 27)\n",
    "\n",
    "\n",
    "# Placeholders (indexed with t)\n",
    "dlogits = torch.zeros((time_steps, batch_size, vocab_size)) # (30, 27)\n",
    "dhidden1 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dhidden2 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dtotal = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "\n",
    "dpreact1 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30) \n",
    "dpreact2 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dpreact3 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dpreact4 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "\n",
    "dact1 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30) \n",
    "dact2 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dact3 = torch.zeros(time_steps, batch_size, hidden_size) # (32, 30)\n",
    "dact4 = torch.zeros((time_steps, batch_size, hidden_size)) # (32, 30)\n",
    "\n",
    "dC = torch.zeros((time_steps, batch_size, hidden_size)) # (32, 30)\n",
    "dCt = torch.zeros((time_steps, batch_size, hidden_size)) # (32, 30)\n",
    "dHin = torch.zeros((time_steps, batch_size, hidden_size)) # (32, 30)\n",
    "dHout = torch.zeros((time_steps, batch_size, hidden_size)) # (32, 30)\n",
    "dlogits = torch.zeros((time_steps, batch_size, vocab_size)) # (32, 27)\n",
    "\n",
    "dc0 = torch.zeros(batch_size, hidden_size)\n",
    "dh0 = torch.zeros((batch_size, hidden_size))\n",
    "dcn = torch.zeros(batch_size, hidden_size)\n",
    "dhn = torch.zeros((batch_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8874dd90-65a7-4687-952d-60a7ec05836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.3\n",
    "max_iterations = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a1b15766-4d25-43b2-ae0a-dfd12d510093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3348)\n",
      "tensor(0.6200)\n",
      "tensor(0.2890)\n",
      "tensor(0.5910)\n",
      "tensor(0.5431)\n",
      "tensor(1.0991)\n",
      "tensor(0.7895)\n",
      "tensor(1.1335)\n",
      "tensor(1.0317)\n",
      "tensor(1.0579)\n",
      "tensor(0.6918)\n",
      "tensor(1.5787)\n",
      "tensor(1.2611)\n",
      "tensor(1.1265)\n",
      "tensor(0.9631)\n",
      "tensor(0.0421)\n",
      "tensor(0.6413)\n",
      "tensor(1.6291)\n",
      "tensor(0.7625)\n",
      "tensor(0.4544)\n",
      "tensor(1.0313)\n",
      "tensor(0.6991)\n",
      "tensor(0.3032)\n",
      "tensor(0.7002)\n",
      "tensor(2.4601)\n",
      "tensor(1.7386)\n",
      "tensor(0.8131)\n",
      "tensor(0.8166)\n",
      "tensor(0.7481)\n",
      "tensor(1.2493)\n",
      "tensor(1.0261)\n",
      "tensor(1.4124)\n",
      "tensor(1.1449)\n",
      "tensor(1.3375)\n",
      "tensor(0.0400)\n",
      "tensor(1.0540)\n",
      "tensor(2.2097)\n",
      "tensor(1.0375)\n",
      "tensor(0.0628)\n",
      "tensor(0.7894)\n",
      "tensor(1.3876)\n",
      "tensor(0.1040)\n",
      "tensor(0.0034)\n",
      "tensor(1.1724)\n",
      "tensor(1.6316)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m dact3[t] \u001b[38;5;241m=\u001b[39m dHout[t] \u001b[38;5;241m*\u001b[39m Ctout[t] \u001b[38;5;66;03m# (32, 27) * (32, 27) = (32, 27)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Backpropogate dC (current cell state)\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m dC[t] \u001b[38;5;241m=\u001b[39m dHout[t] \u001b[38;5;241m*\u001b[39m act4[t] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCout\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (32, 27) * (32, 27) * (32, 27) = (32, 27)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Backpropogate act1 and previous cell state\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Forget gate activations\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Last cell activations\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while max_iterations < 100000:\n",
    "    # Start with no cell activations, and no hidden activations\n",
    "    h0 = torch.zeros((batch_size, hidden_size)) # (1, 30)\n",
    "    c0 = torch.zeros((batch_size, hidden_size)) # (1, 30)\n",
    "    \n",
    "    # Loop over all batches (6504 total)\n",
    "    for batch_num in range(Xtr.shape[0]):\n",
    "        # Get a batch of random numbers into correct shape\n",
    "        Xb = Xtr[batch_num, :] # ( 8, 1)\n",
    "        Xb = Xb.to(torch.long) # ( 8, 1)\n",
    "        Xb = F.one_hot(Xb, 27) # (8, 27)\n",
    "        Xb = Xb / 1.0 # (8, 27)\n",
    "        Yb = Ytr[batch_num] # (8, 1)\n",
    "        Yb = Yb.to(torch.long) #(8, 1)\n",
    "    \n",
    "        # Forward propagation\n",
    "        for t in range(time_steps): # 8\n",
    "            if t == 0:\n",
    "                Hin[t] = h0 # (1, 30)\n",
    "                Cin[t] = c0 # (1, 30)\n",
    "            else:\n",
    "                Hin[t] = Hout[t-1] # (1, 30)\n",
    "                Cin[t] = Cout[t-1] # (1, 30)\n",
    "            loss = 0\n",
    "\n",
    "            # (1, 30)\n",
    "            preact1[t] = Xb[t] @ Fvh + Hin[t] @ Fhh + bias1 # (1, 27) @ (27, 30) + (1, 30) @ (30, 30) + (30)\n",
    "            preact2[t] = Xb[t] @ i1vh + Hin[t] @ i1hh * bias2 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "            preact3[t] = Xb[t] @ i2vh + Hin[t] @ i2hh + bias3 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "            preact4[t] = Xb[t] @ Ovh + Hin[t] @ Ohh + bias4 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "\n",
    "            \n",
    "            act1[t] = torch.sigmoid(preact1[t]) # (1, 30)\n",
    "            act2[t] = torch.sigmoid(preact2[t]) # (1, 30)\n",
    "            act3[t] = torch.tanh(preact3[t]) # (1, 30)\n",
    "            act4[t] = torch.sigmoid(preact4[t]) # (1, 30)\n",
    "            \n",
    "            Cout[t] = act1[t] * Cin[t] + act2[t] * act3[t] # (1, 30)\n",
    "            if t < time_steps -1: Cin[t+1] = Cout[t]\n",
    "            Ctout[t] = torch.tanh(Cout[t]) # (1, 30)\n",
    "            Hout[t] = Ctout[t] * act4[t] # (1, 30)\n",
    "            if t < time_steps -1: Hin[t+1] = Hout[t] \n",
    "            \n",
    "            \n",
    "            logits[t] = Hout[t] @ output_matrix # (1, 27)\n",
    "            counts = logits[t].exp() # (1, 27)\n",
    "            counts_sum = counts.sum(1, keepdims=True) # (1)\n",
    "            counts_sum_inv = counts_sum**-1 # (1)\n",
    "            probs = counts * counts_sum_inv # (1, 27)\n",
    "            logprobs = probs.log() #(1, 27)\n",
    "            loss += -logprobs[0, Yb[t]]\n",
    "    \n",
    "        if (max_iterations % 1000 == 0):\n",
    "                print (loss / time_steps)\n",
    "        \n",
    "        h0 = Hout[t-1]\n",
    "        c0 = Cout[t-1]\n",
    "        max_iterations += 1    \n",
    "        \n",
    "    ### ------------------------------------------------------------------------------------------------------------------\n",
    "        # Backward pass\n",
    "        for t in reversed(range(time_steps)):\n",
    "        \n",
    "            # Backpropogate cross entropy\n",
    "            dlogits[t] = F.softmax(logits[t], 1)\n",
    "            dlogits[t][torch.arange(batch_size), Yb] -= 1\n",
    "            dlogits /= n\n",
    "            \n",
    "            # Backpropogate dHout\n",
    "            # t = 7, 6, 5, 4, 3, 2, 1, 0\n",
    "            if (t < time_steps-1):\n",
    "                # dHout of a previous time step, must add dHin of the next time step\n",
    "                # dHout of one time step, becomes the input for the next time step\n",
    "                # Hout[t] = Hin[t+1]\n",
    "                dHout[t] = dHin[t+1]\n",
    "                dHout[t] =  dlogits[t] @ output_matrix.T + dHin[t+1] # (32, 27) @ (27, 30) = (32, 30) dHt on paper derivation\n",
    "            else: # t = 7\n",
    "                dHout[t] =  dlogits[t] @ output_matrix.T + dhn\n",
    "        \n",
    "            # Backpropogate output matrix\n",
    "            doutput_matrix = dlogits[t].T @ dHout[t] # (32, 27)\n",
    "            \n",
    "            # Backpropogate dact3 (output gate activations)\n",
    "            dact3[t] = dHout[t] * Ctout[t] # (32, 27) * (32, 27) = (32, 27)\n",
    "            \n",
    "            # Backpropogate dC (current cell state)\n",
    "            dC[t] = dHout[t] * act4[t] * (1 - torch.tanh(Cout[t])**2) # (32, 27) * (32, 27) * (32, 27) = (32, 27)\n",
    "            \n",
    "            # Backpropogate act1 and previous cell state\n",
    "            if t > 0:\n",
    "                # Forget gate activations\n",
    "                # Last cell activations\n",
    "                dact1[t] = dC[t] * Cout[t-1] # (32, 27) * (32, 27) = (32, 27)\n",
    "                dC[t-1] = dC[t] * act1[t]\n",
    "            else:\n",
    "                dact1[t] = dC[t] * c0\n",
    "                dc0 = dC[t] * act1[t]\n",
    "            \n",
    "            # Backpropogate i1 activations\n",
    "            dact2[t] = dC[t] * act3[t]\n",
    "            \n",
    "            # Backpropogate i2 activations\n",
    "            dact3[t] = dC[t] * act2[t]\n",
    "            \n",
    "            # Backpropogate all preactivations\n",
    "            dpreact1[t] = dact1[t] * act1[t] * ( 1- act1[t])\n",
    "            dpreact2[t] = dact2[t] * act2[t] * ( 1- act2[t])\n",
    "            dpreact3[t] = dact3[t] * (1 - torch.tanh(dpreact3[t])**2)\n",
    "            dpreact4[t] = dact4[t] * act4[t] * ( 1- act4[t])\n",
    "            \n",
    "            # Backpropogate gates\n",
    "            dFvh = Xb[t].reshape(vocab_size, 1) @ dpreact1[t] # (27, 32) (32, 30) = (27, 30)\n",
    "            dFhh = Hin[t].reshape(hidden_size, 1) @ dpreact1[t] \n",
    "            di1vh = Xb[t].reshape(vocab_size, 1) @ dpreact2[t]\n",
    "            di1hh = Hin[t].reshape(hidden_size, 1) @ dpreact2[t]\n",
    "            di2vh = Xb[t].reshape(vocab_size, 1) @ dpreact3[t]\n",
    "            di2hh = Hin[t].reshape(hidden_size, 1) @ dpreact3[t]\n",
    "            dOvh = Xb[t].reshape(vocab_size, 1) @ dpreact4[t]\n",
    "            dOhh = Hin[t].reshape(hidden_size, 1) @ dpreact4[t]\n",
    "            \n",
    "            # Backpropogate prevh\n",
    "            dHin[t] = dpreact1[t] @ Fhh.T + dpreact2[t] @ i1hh.T + dpreact3[t] @ i2hh.T + dpreact4[t] @ Ohh.T\n",
    "    \n",
    "    ### ------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "        # Update parameters using gradients\n",
    "        Fvh -= lr * dFvh\n",
    "        i1vh -= lr * di1vh\n",
    "        i2vh -= lr * di2vh\n",
    "        Ovh -= lr * dOvh\n",
    "        \n",
    "        Fhh -= lr * dFhh\n",
    "        i1hh -= lr * di1hh\n",
    "        i2hh -= lr * di2hh\n",
    "        Ohh -= lr * dOhh\n",
    "        \n",
    "        bias1 -= lr * dbias1\n",
    "        bias2 -= lr * dbias2\n",
    "        bias3 -= lr * dbias3\n",
    "        bias4 -= lr * dbias4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4c78234a-e277-4118-bb33-bea7f86b2fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
    "    return e_x / torch.sum(e_x, dim=1, keepdim=True)\n",
    "\n",
    "def sample_model(start_vector, Fvh, Fhh, i1vh, i1hh, i2vh, i2hh, Ovh, Ohh, bias1, bias2, bias3, bias4, output_matrix):\n",
    "    \"\"\"\n",
    "    Generate a sequence of samples from the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_vector: Initial one-hot encoded vector to start the generation (torch tensor of shape (1, input_size))\n",
    "    - Other parameters: Model weights and biases\n",
    "    \n",
    "    Returns:\n",
    "    - Generated sequence (list of integers)\n",
    "    \"\"\"\n",
    "    input_size = start_vector.shape[1]\n",
    "    hidden_size = Fvh.shape[1]\n",
    "    \n",
    "    # Initialize hidden and cell states\n",
    "    h = torch.zeros((1, hidden_size))\n",
    "    c = torch.zeros((1, hidden_size))\n",
    "    \n",
    "    # Initialize the generated sequence with the index of the start vector\n",
    "    generated_sequence = [torch.argmax(start_vector).item()]\n",
    "    \n",
    "    # Loop until '.' is generated or the maximum sequence length is reached\n",
    "    while generated_sequence[-1] != 0 and len(generated_sequence) <= max_length:\n",
    "        x = start_vector  # Use the provided start vector as input\n",
    "        \n",
    "        preact1 = x @ Fvh + h @ Fhh + bias1\n",
    "        preact2 = x @ i1vh + h @ i1hh + bias2\n",
    "        preact3 = x @ i2vh + h @ i2hh + bias3\n",
    "        preact4 = x @ Ovh + h @ Ohh + bias4\n",
    "        \n",
    "        act1 = torch.sigmoid(preact1)\n",
    "        act2 = torch.sigmoid(preact2)\n",
    "        act3 = torch.tanh(preact3)\n",
    "        act4 = torch.sigmoid(preact4)\n",
    "        \n",
    "        c = act1 * c + act2 * act3\n",
    "        h = torch.tanh(c) * act4\n",
    "        \n",
    "        logits = h @ output_matrix\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        # Sample from the probability distribution to get the next input\n",
    "        next_index = torch.multinomial(probs.squeeze(), 1).item()\n",
    "        \n",
    "        generated_sequence.append(next_index)\n",
    "        \n",
    "        # Update the start vector with the one-hot encoded representation of the next index\n",
    "        start_vector = torch.zeros((1, input_size))\n",
    "        start_vector[0, next_index] = 1\n",
    "        \n",
    "    return [itos[ch] for ch in generated_sequence]\n",
    "\n",
    "# Example usage\n",
    "input_size = 27  # Size of the one-hot encoded vector\n",
    "start_index = 1  # Index representing the starting point (e.g., 'a')\n",
    "start_vector = torch.zeros((1, input_size))\n",
    "start_vector[0, start_index] = 3  # One-hot encode the starting point\n",
    "\n",
    "# Assume Fvh, Fhh, i1vh, i1hh, i2vh, i2hh, Ovh, Ohh, bias1, bias2, bias3, bias4, and output_matrix are already defined\n",
    "\n",
    "max_length = 15  # Maximum length of the generated sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7f6d2bc2-0b8b-4014-a382-629827d57d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aqpolaarzggmmggg\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = sample_model(start_vector, Fvh, Fhh, i1vh, i1hh, i2vh, i2hh, Ovh, Ohh, bias1, bias2, bias3, bias4, output_matrix)\n",
    "print(''.join(generated_sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
