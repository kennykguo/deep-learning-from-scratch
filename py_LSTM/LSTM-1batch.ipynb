{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff91fc80-0bc4-471f-91e9-6c51ac49657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5d27627-2fa1-4c6e-bfad-dd87852ae629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "Character to index mapping: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "Vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "# Read in all the words\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "print(words[:10])\n",
    "\n",
    "# Build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(f'Character to index mapping: {itos}')\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Shuffle the words\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aac0e6ea-cbf7-48fa-98d6-9dcf2fe77f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 25, 21, 8, 5, 14, 7, 0, 4, 9, 15, 14, 4, 18, 5, 0, 24, 1, 22, 9]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the words with '.' so the model can learn when a name should end\n",
    "def encode_words(words):\n",
    "    encoded = []\n",
    "    for w in words:\n",
    "        encoded.extend([stoi[ch] for ch in '.' + w ])\n",
    "    return encoded\n",
    "\n",
    "encoded = encode_words(words)\n",
    "encoded[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "754cc12d-c2ee-4650-8287-0deadf2ac4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for creating training and test sets\n",
    "n = len(encoded)\n",
    "n1 = int(0.8 * n)\n",
    "block_size = 8\n",
    "batch_size = 1\n",
    "\n",
    "train_seq = encoded[:n1]\n",
    "dev_seq = encoded[n1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e98e8b-8081-4f09-811c-4c39040b7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(seq, block_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(0, len(seq) - block_size, block_size):\n",
    "        X.append(seq[i:i+block_size])\n",
    "        Y.append(seq[i+1:i+block_size+1])\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32)\n",
    "    return X, Y\n",
    "\n",
    "Xtr, Ytr = create_pairs(train_seq, block_size)\n",
    "Xdev, Ydev = create_pairs(dev_seq, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1caa97e5-c13d-417f-8c04-620c3685f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  9., 15., 14.,  4., 18.,  5.,  0.])\n",
      "tensor([ 9., 15., 14.,  4., 18.,  5.,  0., 24.])\n",
      "tensor([24.,  1., 22.,  9.,  5., 14.,  0., 10.])\n",
      "tensor([ 1., 22.,  9.,  5., 14.,  0., 10., 15.])\n",
      "tensor([15., 18.,  9.,  0., 10., 21.,  1., 14.])\n",
      "tensor([18.,  9.,  0., 10., 21.,  1., 14., 12.])\n",
      "Training data shapes - X: torch.Size([22814, 8]), Y: torch.Size([22814, 8])\n",
      "Development data shapes - X: torch.Size([5703, 8]), Y: torch.Size([5703, 8])\n"
     ]
    }
   ],
   "source": [
    "# We are sure that we can pass in previous example's hidden layer\n",
    "print(Xtr[1])\n",
    "print(Ytr[1])\n",
    "print(Xtr[2])\n",
    "print(Ytr[2])\n",
    "print(Xtr[3])\n",
    "print(Ytr[3])\n",
    "print(f'Training data shapes - X: {Xtr.shape}, Y: {Ytr.shape}')\n",
    "print(f'Development data shapes - X: {Xdev.shape}, Y: {Ydev.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cae0b00-fd29-48c1-b142-60f568b5558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "batch_size = 1\n",
    "hidden_size = 30\n",
    "time_steps = 8\n",
    "input_size = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa2067b3-2040-49a4-a839-c1c10a07bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
    "    return e_x / torch.sum(e_x, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7fddc288-34bc-4c65-bb6e-fa74c5799979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize parameters\n",
    "Fvh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "i1vh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "i2vh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "Ovh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "\n",
    "Fhh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "i1hh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "i2hh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "Ohh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "\n",
    "bias1 = torch.zeros(1, hidden_size)\n",
    "bias2 = torch.zeros(1, hidden_size)\n",
    "bias3 = torch.zeros(1, hidden_size)\n",
    "bias4 = torch.zeros(1, hidden_size)\n",
    "\n",
    "output_matrix = torch.randn(hidden_size, vocab_size) * 0.01\n",
    "output_bias = torch.zeros(1, vocab_size)\n",
    "\n",
    "X = {}\n",
    "H = {}\n",
    "C = {}\n",
    "Ct = {}\n",
    "preact1 = {}\n",
    "preact2 = {}\n",
    "preact3 = {}\n",
    "preact4 = {}\n",
    "act1 = {}\n",
    "act2 = {}\n",
    "act3 = {}\n",
    "act4 = {}\n",
    "logits = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b15766-4d25-43b2-ae0a-dfd12d510093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(26.3670)\n",
      "iteration 0\n",
      "----\n",
      " tbmsqqcxgdlfsbp \n",
      "----\n",
      "loss tensor(26.1093)\n",
      "iteration 1000\n",
      "----\n",
      " pkgkfqcwimnudcd \n",
      "----\n",
      "loss tensor(25.8782)\n",
      "iteration 2000\n",
      "----\n",
      " thkcuiotbwbelqq \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 0\n",
    "\n",
    "while max_iterations < 100000:\n",
    "    \n",
    "    # Looping over batches (each batch is just a single row)\n",
    "    for batch_num in range(Xtr.shape[0]):\n",
    "\n",
    "        # Initial cell state and hidden state and loss\n",
    "        H[-1] = torch.zeros(1, hidden_size)\n",
    "        C[-1] = torch.zeros(1, hidden_size)\n",
    "        loss = 0   \n",
    "        \n",
    "        # Get a batch of random numbers into correct shape\n",
    "        Xb = Xtr[batch_num, :].to(torch.long) # (1, 8)\n",
    "        Xb = F.one_hot(Xb, 27).float() # (8, 27)\n",
    "        Yb = Ytr[batch_num].to(torch.long) # (8, 1)\n",
    "        \n",
    "### --------------------------------------------------------------------------------------------------------------        \n",
    "        # Forward pass over all time steps\n",
    "        for t in range(time_steps):\n",
    "            preact1[t] = Xb[t] @ Fvh + H[t-1] @ Fhh + bias1 # (1, 27) @ (27, 30) + (1, 30) @ (30, 30) + (30) = (1, 30)\n",
    "            preact2[t] = Xb[t]  @ i1vh + H[t-1]  @ i1hh * bias2 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "            preact3[t] = Xb[t]  @ i2vh + H[t-1]  @ i2hh + bias3 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "            preact4[t] = Xb[t]  @ Ovh + H[t-1]  @ Ohh + bias4 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "            \n",
    "            act1[t] = torch.sigmoid(preact1[t]) # (1, 30)\n",
    "            act2[t] = torch.sigmoid(preact2[t]) # (1, 30)\n",
    "            act3[t] = torch.tanh(preact3[t]) # (1, 30)\n",
    "            act4[t] = torch.sigmoid(preact4[t]) # (1, 30)\n",
    "\n",
    "            # Forget gate * previous cell state + i1 gate * i2 gate\n",
    "            C[t] = act1[t] * C[t-1]  + act2[t]  * act3[t]  # (1, 30)\n",
    "            Ct[t] = torch.tanh(C[t]) # (1, 30)\n",
    "            H[t] = Ct[t] * act4[t]  # (1, 30)          \n",
    "\n",
    "            # Logits\n",
    "            logits[t] = H[t]  @ output_matrix + output_bias # (1, 30) @ (30, 27) + (1, 27) = (1, 27)\n",
    "\n",
    "            # Cross entropy\n",
    "            counts = logits[t].exp()\n",
    "            counts_sum = counts.sum(1, keepdims=True)\n",
    "            counts_sum_inv = counts_sum**-1\n",
    "            probs = counts * counts_sum_inv\n",
    "            logprobs = probs.log()\n",
    "            loss += -logprobs[0, Yb[t]]\n",
    "\n",
    "        # Continue the sequence for time step -1 of the next input sequence\n",
    "        H[-1] = H[time_steps - 1]\n",
    "        C[-1] = C[time_steps - 1]\n",
    "\n",
    "        # Print out the loss occasionally\n",
    "        if max_iterations % 1000 == 0:\n",
    "            print(\"loss\", loss)\n",
    "            print(\"iteration\", max_iterations)\n",
    "            sample_ix = sample(H[-1], C[-1], 2, 15)\n",
    "            txt = ''.join(itos[ix] for ix in sample_ix)\n",
    "            print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "        # Increment max iterations\n",
    "        max_iterations += 1 \n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------------\n",
    "        # Backward pass\n",
    "        # Set to zeros initially\n",
    "        dHnext = torch.zeros_like(H[0])\n",
    "        dCnext = torch.zeros_like(C[0])\n",
    "        \n",
    "        dFvh = torch.zeros(vocab_size, hidden_size)\n",
    "        di1vh = torch.zeros(vocab_size, hidden_size)\n",
    "        di2vh = torch.zeros(vocab_size, hidden_size)\n",
    "        dOvh = torch.zeros(vocab_size, hidden_size)\n",
    "        \n",
    "        dFhh = torch.zeros(hidden_size, hidden_size)\n",
    "        di1hh = torch.zeros(hidden_size, hidden_size)\n",
    "        di2hh = torch.zeros(hidden_size, hidden_size)\n",
    "        dOhh = torch.zeros(hidden_size, hidden_size)\n",
    "        \n",
    "        dbias1 = torch.zeros(1, hidden_size)\n",
    "        dbias2 = torch.zeros(1, hidden_size)\n",
    "        dbias3 = torch.zeros(1, hidden_size)\n",
    "        dbias4 = torch.zeros(1, hidden_size)\n",
    "        \n",
    "        doutput_matrix = torch.zeros(hidden_size, vocab_size)\n",
    "        doutput_bias = torch.zeros(1, vocab_size)\n",
    "\n",
    "        for t in reversed(range(time_steps)):\n",
    "            # Backpropogate cross entropy\n",
    "            dlogits = F.softmax(logits[t], 1)\n",
    "            dlogits [0, Yb[t]] -= 1\n",
    "            \n",
    "            # Backpropogate output matrix and its bias\n",
    "            doutput_matrix +=  H[t].T @ dlogits # (30, 1) @ (1, 27) = (30, 27)\n",
    "            doutput_bias += dlogits # (1, 27)\n",
    "            \n",
    "            # Backpropogate into H\n",
    "            dH = dlogits  @ output_matrix.T + dHnext # (1, 27) @ (27, 30) + (1, 30) = (1, 30)\n",
    "            \n",
    "            # Backpropogate dact4 (output gate activations)\n",
    "            dact4  = dH  * Ct[t]  # (1, 30) * (1, 30) = (1, 30)\n",
    "            \n",
    "            # Backpropogate dC (current cell state)\n",
    "            dC = dH * act4[t] * (1 - torch.tanh(C[t]) ** 2) + dCnext # (1, 30) * (1, 30) * (1, 30) = (1, 30)\n",
    "            \n",
    "            ### Backpropogate act1 and previous cell state\n",
    "            # Forget gate activations\n",
    "            # dCnext activations\n",
    "            dact1 = dC * C[t-1] # (1, 30) * (1, 30) = (1, 30)\n",
    "            dCnext = dC * act1[t]\n",
    "            \n",
    "            # Backpropogate i1 activations\n",
    "            dact2  = dC  * act3[t] # (1, 27)\n",
    "            \n",
    "            # Backpropogate i2 activations\n",
    "            dact3  = dC  * act2[t]  # (1, 27)\n",
    "            \n",
    "            # Backpropogate all preactivations\n",
    "            dpreact1 = dact1 * act1[t] * (1 - act1[t])\n",
    "            dpreact2 = dact2 * act2[t] * (1 - act2[t])\n",
    "            dpreact3 = dact3 * (1 - torch.tanh(preact3[t]) ** 2)\n",
    "            dpreact4 = dact4 * act4[t] * (1 - act4[t])\n",
    "            \n",
    "            # Backpropogate gates\n",
    "            dFvh += Xb[t].view(vocab_size, 1) @ dpreact1  # (27, 1) (1, 30) = (27, 30)\n",
    "            dFhh += H[t-1].view(hidden_size, 1) @ dpreact1  # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias1 += dpreact1\n",
    "            \n",
    "            di1vh += Xb[t].view(vocab_size, 1) @ dpreact2 # (27, 1) (1, 30) = (27, 30)\n",
    "            di1hh += H[t-1].view(hidden_size, 1) @ dpreact2 # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias2 += dpreact2\n",
    "            \n",
    "            di2vh += Xb[t].view(vocab_size, 1) @ dpreact3 # (27, 1) (1, 30) = (27, 30)\n",
    "            di2hh += H[t-1].view(hidden_size, 1) @ dpreact3 # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias3 += dpreact3\n",
    "            \n",
    "            dOvh += Xb[t].view(vocab_size, 1) @ dpreact4 # (27, 1) (1, 30) = (27, 30)\n",
    "            dOhh += H[t-1].view(hidden_size, 1) @ dpreact4 # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias4 += dpreact4\n",
    "            \n",
    "            # Backpropogate dHnext\n",
    "            # (1, 30) @ (30, 30) = (1, 30)\n",
    "            dHnext  = dpreact1  @ Fhh.T + dpreact2  @ i1hh.T + dpreact3  @ i2hh.T + dpreact4  @ Ohh.T\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_([dFvh, di1vh, di2vh, dOvh, dFhh, di1hh, di2hh, dOhh, dbias1, dbias2, dbias3, dbias4, doutput_matrix, doutput_bias], max_norm=1.0)\n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "        # Update parameters using gradients\n",
    "        Fvh -= lr * dFvh\n",
    "        i1vh -= lr * di1vh\n",
    "        i2vh -= lr * di2vh\n",
    "        Ovh -= lr * dOvh\n",
    "\n",
    "        Fhh -= lr * dFhh\n",
    "        i1hh -= lr * di1hh\n",
    "        i2hh -= lr * di2hh\n",
    "        Ohh -= lr * dOhh\n",
    "        \n",
    "        bias1 -= lr * dbias1\n",
    "        bias2 -= lr * dbias2\n",
    "        bias3 -= lr * dbias3\n",
    "        bias4 -= lr * dbias4\n",
    "\n",
    "        output_matrix -= lr * doutput_matrix\n",
    "        output_bias -= lr * doutput_bias\n",
    "    \n",
    "    if (max_iterations == 10000):\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4c78234a-e277-4118-bb33-bea7f86b2fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(hprev, cprev, seed_ix, n):\n",
    "    # Create one hot vector based on input seed_ix\n",
    "    x = torch.zeros(1, vocab_size)\n",
    "    x[0, seed_ix] = 1\n",
    "    ixes = []\n",
    "\n",
    "    for t in range(n):\n",
    "        preact1 = x @ Fvh + hprev @ Fhh + bias1 # (1, 27) @ (27, 30) + (1, 30) @ (30, 30) + (30) = (1, 30)\n",
    "        preact2 = x  @ i1vh + hprev  @ i1hh * bias2 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "        preact3 = x  @ i2vh + hprev  @ i2hh + bias3 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "        preact4 = x  @ Ovh + hprev  @ Ohh + bias4 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "        \n",
    "        act1 = torch.sigmoid(preact1) # (1, 30)\n",
    "        act2 = torch.sigmoid(preact2) # (1, 30)\n",
    "        act3 = torch.tanh(preact3) # (1, 30)\n",
    "        act4 = torch.sigmoid(preact4) # (1, 30)\n",
    "\n",
    "        # Forget gate * previous cell state + i1 gate * i2 gate\n",
    "        C = act1 * cprev  + act2  * act3  # (1, 30)\n",
    "        Ct = torch.tanh(C) # (1, 30)\n",
    "        H = Ct * act4  # (1, 30)          \n",
    "\n",
    "        # Logits\n",
    "        logits = H  @ output_matrix + output_bias # (1, 30) @ (30, 27) + (1, 27) = (1, 27)\n",
    "\n",
    "        # Probability\n",
    "        counts = logits.exp()\n",
    "        counts_sum = counts.sum(1, keepdims=True)\n",
    "        counts_sum_inv = counts_sum**-1\n",
    "        probs = counts * counts_sum_inv\n",
    "\n",
    "        # Sample from distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        if (ix == 0):\n",
    "            break\n",
    "        ixes.append(ix)\n",
    "\n",
    "        # Update our parameters\n",
    "        x = torch.zeros(1, vocab_size)\n",
    "        x[0, ix] = 1\n",
    "        hprev = H\n",
    "        cprev = C\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7f6d2bc2-0b8b-4014-a382-629827d57d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " hlazsmbsaan \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "hprev = torch.zeros(1, hidden_size)\n",
    "cprev = torch.zeros(1, hidden_size)\n",
    "\n",
    "sample_ix = sample(hprev, cprev, 2, 15)\n",
    "txt = ''.join(itos[ix] for ix in sample_ix)\n",
    "print('----\\n %s \\n----' % (txt,))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
