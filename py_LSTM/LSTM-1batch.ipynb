{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff91fc80-0bc4-471f-91e9-6c51ac49657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d27627-2fa1-4c6e-bfad-dd87852ae629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "Character to index mapping: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "Vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "# Read in all the words\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "print(words[:10])\n",
    "\n",
    "# Build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(f'Character to index mapping: {itos}')\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Shuffle the words\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac0e6ea-cbf7-48fa-98d6-9dcf2fe77f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the words with '.' so the model can learn when a name should end\n",
    "def encode_words(words):\n",
    "    encoded = []\n",
    "    for w in words:\n",
    "        encoded.extend([stoi[ch] for ch in '.' + w ])\n",
    "    return encoded\n",
    "\n",
    "encoded = encode_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0bfad2-2aff-4766-82f4-4a8d4b9bc7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 25, 21, 8, 5, 14, 7, 0, 4, 9, 15, 14, 4, 18, 5, 0, 24, 1, 22, 9]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "754cc12d-c2ee-4650-8287-0deadf2ac4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for creating training and test sets\n",
    "n = len(encoded)\n",
    "n1 = int(0.8 * n)\n",
    "block_size = 8\n",
    "batch_size = 1\n",
    "\n",
    "train_seq = encoded[:n1]\n",
    "dev_seq = encoded[n1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e98e8b-8081-4f09-811c-4c39040b7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(seq, block_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(0, len(seq) - block_size, block_size):\n",
    "        X.append(seq[i:i+block_size])\n",
    "        Y.append(seq[i+1:i+block_size+1])\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32)\n",
    "    return X, Y\n",
    "\n",
    "Xtr, Ytr = create_pairs(train_seq, block_size)\n",
    "Xdev, Ydev = create_pairs(dev_seq, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1caa97e5-c13d-417f-8c04-620c3685f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  9., 15., 14.,  4., 18.,  5.,  0.])\n",
      "tensor([ 9., 15., 14.,  4., 18.,  5.,  0., 24.])\n",
      "tensor([24.,  1., 22.,  9.,  5., 14.,  0., 10.])\n",
      "tensor([ 1., 22.,  9.,  5., 14.,  0., 10., 15.])\n",
      "tensor([15., 18.,  9.,  0., 10., 21.,  1., 14.])\n",
      "tensor([18.,  9.,  0., 10., 21.,  1., 14., 12.])\n",
      "Training data shapes - X: torch.Size([22814, 8]), Y: torch.Size([22814, 8])\n",
      "Development data shapes - X: torch.Size([5703, 8]), Y: torch.Size([5703, 8])\n"
     ]
    }
   ],
   "source": [
    "# We are sure that we can pass in previous example's hidden layer\n",
    "print(Xtr[1])\n",
    "print(Ytr[1])\n",
    "print(Xtr[2])\n",
    "print(Ytr[2])\n",
    "print(Xtr[3])\n",
    "print(Ytr[3])\n",
    "print(f'Training data shapes - X: {Xtr.shape}, Y: {Ytr.shape}')\n",
    "print(f'Development data shapes - X: {Xdev.shape}, Y: {Ydev.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cae0b00-fd29-48c1-b142-60f568b5558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "batch_size = 1\n",
    "hidden_size = 30\n",
    "time_steps = 8\n",
    "input_size = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa2067b3-2040-49a4-a839-c1c10a07bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
    "    return e_x / torch.sum(e_x, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fddc288-34bc-4c65-bb6e-fa74c5799979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize parameters\n",
    "Fvh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "i1vh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "i2vh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "Ovh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "\n",
    "Fhh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "i1hh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "i2hh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "Ohh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "\n",
    "bias1 = torch.zeros(1, hidden_size)\n",
    "bias2 = torch.zeros(1, hidden_size)\n",
    "bias3 = torch.zeros(1, hidden_size)\n",
    "bias4 = torch.zeros(1, hidden_size)\n",
    "\n",
    "output_matrix = torch.randn(hidden_size, vocab_size) * 0.01\n",
    "output_bias = torch.zeros(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1b15766-4d25-43b2-ae0a-dfd12d510093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum tensor([[27.0007]])\n",
      "counts_sum tensor([[27.0029]])\n",
      "counts_sum tensor([[27.0163]])\n",
      "counts_sum tensor([[27.0615]])\n",
      "counts_sum tensor([[27.2335]])\n",
      "counts_sum tensor([[27.8465]])\n",
      "counts_sum tensor([[30.7204]])\n",
      "counts_sum tensor([[64.6073]])\n",
      "counts_sum tensor([[180105.9375]])\n",
      "counts_sum tensor([[4.3857e+09]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Backpropogate all preactivations\u001b[39;00m\n\u001b[1;32m    134\u001b[0m dpreact1  \u001b[38;5;241m=\u001b[39m dact1  \u001b[38;5;241m*\u001b[39m act1[t] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m act1[t])\n\u001b[0;32m--> 135\u001b[0m dpreact2  \u001b[38;5;241m=\u001b[39m dact2  \u001b[38;5;241m*\u001b[39m act2[t]  \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mact2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    136\u001b[0m dpreact3  \u001b[38;5;241m=\u001b[39m dact3  \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(preact3[t])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    137\u001b[0m dpreact4  \u001b[38;5;241m=\u001b[39m dact4  \u001b[38;5;241m*\u001b[39m act4[t]  \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m act4[t])\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/_tensor.py:966\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iterations = 0\n",
    "\n",
    "while max_iterations < 100000:\n",
    "    \n",
    "    # (1, 30)\n",
    "    X = {}\n",
    "    H = {}\n",
    "    C = {}\n",
    "    Ct = {}\n",
    "    preact1 = {}\n",
    "    preact2 = {}\n",
    "    preact3 = {}\n",
    "    preact4 = {}\n",
    "    act1 = {}\n",
    "    act2 = {}\n",
    "    act3 = {}\n",
    "    act4 = {}\n",
    "    logits = {}\n",
    "    \n",
    "    # Loop over all batches\n",
    "    for batch_num in range(Xtr.shape[0]):\n",
    "        \n",
    "### --------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Forward propagation \n",
    "        \n",
    "### --------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # Get a batch of random numbers into correct shape\n",
    "        Xb = Xtr[batch_num, :] # (1, 8)\n",
    "        Xb = Xb.to(torch.long) # (1, 8)\n",
    "        Xb = F.one_hot(Xb, 27) # (1, 8, 27)\n",
    "        Xb = Xb / 1.0 # (8, 27)\n",
    "        Yb = Ytr[batch_num] # (8, 1)\n",
    "        Yb = Yb.to(torch.long) # (8, 1)\n",
    "        \n",
    "### --------------------------------------------------------------------------------------------------------------        \n",
    "\n",
    "        loss = 0\n",
    "        \n",
    "        if (max_iterations == 0):\n",
    "            H[-1] = torch.zeros(1, hidden_size)\n",
    "            C[-1] = torch.zeros(1, hidden_size)\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            preact1[t] = Xb[t] @ Fvh + H[t-1] @ Fhh + bias1 # (1, 27) @ (27, 30) + (1, 30) @ (30, 30) + (30)\n",
    "            preact2[t] = Xb[t]  @ i1vh + H[t-1]  @ i1hh * bias2 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "            preact3[t] = Xb[t]  @ i2vh + H[t-1]  @ i2hh + bias3 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "            preact4[t] = Xb[t]  @ Ovh + H[t-1]  @ Ohh + bias4 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "            \n",
    "            act1[t] = torch.sigmoid(preact1[t]) # (1, 30)\n",
    "            act2[t] = torch.sigmoid(preact2[t]) # (1, 30)\n",
    "            act3[t] = torch.tanh(preact3[t]) # (1, 30)\n",
    "            act4[t] = torch.sigmoid(preact4[t]) # (1, 30)\n",
    "            \n",
    "            C[t] = act1[t] * C[t-1]  + act2[t]  * act3[t]  # (1, 30)\n",
    "            Ct[t] = torch.tanh(C[t]) # (1, 30)\n",
    "            H[t] = Ct[t] * act4[t]  # (1, 30)          \n",
    "\n",
    "            logits[t] = H[t]  @ output_matrix + output_bias # (1, 30) @ (30, 27) + (1, 27) = (1, 27)\n",
    "\n",
    "            # Cross entropy\n",
    "            counts = logits[t].exp() # (1, 27)\n",
    "            counts_sum = counts.sum(1, keepdims=True) # (1)\n",
    "            counts_sum_inv = counts_sum**-1 # (1)\n",
    "            probs = counts * counts_sum_inv # (1, 27)\n",
    "            logprobs = probs.log() #(1, 27)\n",
    "            loss += -logprobs[0, Yb[t]]\n",
    "            \n",
    "        if (max_iterations % 1000 == 0):\n",
    "            print(\"counts_sum\", counts_sum)\n",
    "\n",
    "        # Continue the sequence for time step -1 of the next input sequence\n",
    "        H[-1] = H[time_steps - 1]\n",
    "        C[-1] = C[time_steps - 1]\n",
    "\n",
    "        max_iterations += 1    \n",
    "        \n",
    "### ---------------------------------------------------------------------------------------------------------------\n",
    "        # Backward pass\n",
    "        # Set to zeros initially\n",
    "        dHnext = torch.zeros_like(H[0]) # (1, 30)\n",
    "        dCnext = torch.zeros_like(H[0]) # (1, 27)\n",
    "        \n",
    "        dFvh = torch.zeros(vocab_size, hidden_size)\n",
    "        di1vh = torch.zeros(vocab_size, hidden_size)\n",
    "        di2vh = torch.zeros(vocab_size, hidden_size)\n",
    "        dOvh = torch.zeros(vocab_size, hidden_size)\n",
    "        \n",
    "        dFhh = torch.zeros(hidden_size, hidden_size)\n",
    "        di1hh = torch.zeros(hidden_size, hidden_size)\n",
    "        di2hh = torch.zeros(hidden_size, hidden_size)\n",
    "        dOhh = torch.zeros(hidden_size, hidden_size)\n",
    "        \n",
    "        dbias1 = torch.zeros(1, hidden_size) # (30)\n",
    "        dbias2 = torch.zeros(1, hidden_size) # (30)\n",
    "        dbias3 = torch.zeros(1, hidden_size) # (30)\n",
    "        dbias4 = torch.zeros(1, hidden_size) # (30)\n",
    "        \n",
    "        doutput_matrix = torch.zeros(hidden_size, vocab_size)  * 0.01 # (30, 27)\n",
    "        doutput_bias = torch.zeros(1, vocab_size) # (30)\n",
    "\n",
    "        for t in reversed(range(time_steps)):\n",
    "            # Backpropogate cross entropy\n",
    "            dlogits = F.softmax(logits[t], 1)\n",
    "            dlogits [0, Yb] -= 1\n",
    "            \n",
    "            # Backpropogate output matrix and its bias\n",
    "            doutput_matrix +=  (H[t].T) @ dlogits # (30, 1) @ (1, 27) = (30, 27)\n",
    "            doutput_bias += dlogits # (1, 27)\n",
    "            \n",
    "            # Backpropogate into H\n",
    "            dH = dlogits  @ output_matrix.T + dHnext # (1, 27) @ (27, 30) + (1, 30) = (1, 30)\n",
    "            \n",
    "            # Backpropogate dact4 (output gate activations)\n",
    "            dact4  = dH  * Ct[t]  # (1, 30) * (1, 30) = (1, 30)\n",
    "            \n",
    "            # Backpropogate dC (current cell state)\n",
    "            dC  = dH  * act4[t]  * (1 - torch.tanh(C[t]) **2) + dCnext # (1, 30) * (1, 30) * (1, 30) = (1, 30)\n",
    "            \n",
    "            ### Backpropogate act1 and previous cell state\n",
    "            # Forget gate activations\n",
    "            # dCnext activations\n",
    "            dact1  = dC  * C[t-1] # (1, 30) * (1, 30) = (1, 30)\n",
    "            dCnext = dC  * act1[t]\n",
    "            \n",
    "            # Backpropogate i1 activations\n",
    "            dact2  = dC  * act3[t] # (1, 27)\n",
    "            \n",
    "            # Backpropogate i2 activations\n",
    "            dact3  = dC  * act2[t]  # (1, 27)\n",
    "            \n",
    "            # Backpropogate all preactivations\n",
    "            dpreact1  = dact1  * act1[t] * (1- act1[t])\n",
    "            dpreact2  = dact2  * act2[t]  * (1- act2[t])\n",
    "            dpreact3  = dact3  * (1 - torch.tanh(preact3[t])**2)\n",
    "            dpreact4  = dact4  * act4[t]  * (1- act4[t])\n",
    "            \n",
    "            # Backpropogate gates\n",
    "            dFvh += Xb[t].reshape(vocab_size, 1) @ dpreact1  # (27, 1) (1, 30) = (27, 30)\n",
    "            dFhh += H[t-1].reshape(hidden_size, 1) @ dpreact1  # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias1 += dpreact1\n",
    "            \n",
    "            di1vh += Xb[t].reshape(vocab_size, 1) @ dpreact2 # (27, 1) (1, 30) = (27, 30)\n",
    "            di1hh += H[t-1].reshape(hidden_size, 1) @ dpreact2 # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias2 += dpreact2\n",
    "            \n",
    "            di2vh += Xb[t].reshape(vocab_size, 1) @ dpreact3 # (27, 1) (1, 30) = (27, 30)\n",
    "            di2hh += H[t-1].reshape(hidden_size, 1) @ dpreact3 # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias3 += dpreact3\n",
    "            \n",
    "            dOvh += Xb[t].reshape(vocab_size, 1) @ dpreact4 # (27, 1) (1, 30) = (27, 30)\n",
    "            dOhh += H[t-1].reshape(hidden_size, 1) @ dpreact4 # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias4 += dpreact4\n",
    "            \n",
    "            # Backpropogate dHnext\n",
    "            dHnext  = dpreact1  @ Fhh.T + dpreact2  @ i1hh.T + dpreact3  @ i2hh.T + dpreact4  @ Ohh.T\n",
    "    \n",
    "    ### ------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "        # Update parameters using gradients\n",
    "        Fvh -= lr * dFvh\n",
    "        i1vh -= lr * di1vh\n",
    "        i2vh -= lr * di2vh\n",
    "        Ovh -= lr * dOvh\n",
    "\n",
    "        Fhh -= lr * dFhh\n",
    "        i1hh -= lr * di1hh\n",
    "        i2hh -= lr * di2hh\n",
    "        Ohh -= lr * dOhh\n",
    "        \n",
    "        bias1 -= lr * dbias1\n",
    "        bias2 -= lr * dbias2\n",
    "        bias3 -= lr * dbias3\n",
    "        bias4 -= lr * dbias4\n",
    "\n",
    "        output_matrix -= lr * doutput_matrix\n",
    "        doutput_bias -= lr * doutput_bias\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15019249-3413-4e8c-8c4d-eb7d6483babc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-324.0932, -324.1512,  -60.6865,  -82.8280, -124.9950, -324.0212,\n",
       "          -20.4404,  -47.0357, -171.5341, -324.1687,  -68.0903, -117.7951,\n",
       "         -283.1645, -148.7504, -323.6219, -180.4457,  -23.7667,   -7.0451,\n",
       "         -276.2080, -179.7781, -121.6247,  -74.8806,  -60.3296,  -24.7178,\n",
       "          -15.4333, -222.3771,  -57.2788],\n",
       "        [ 324.7320,  324.6779,   60.7525,   82.9095,  125.1568,  324.5805,\n",
       "           20.4604,   47.1652,  171.7828,  324.6581,   68.1179,  117.9171,\n",
       "          283.5695,  149.0448,  324.0399,  180.7481,   23.7718,    7.0400,\n",
       "          276.6554,  179.9780,  121.8557,   74.9900,   60.3949,   24.7211,\n",
       "           15.4331,  222.5418,   57.2915],\n",
       "        [-324.8392, -324.7778,  -60.7667,  -82.9027, -125.1790, -324.6648,\n",
       "          -20.4745,  -47.1755, -171.8122, -324.7286,  -68.1422, -117.9584,\n",
       "         -283.6296, -149.0522, -324.1298, -180.7840,  -23.7686,   -7.0099,\n",
       "         -276.7013, -180.0337, -121.8341,  -74.9700,  -60.3927,  -24.7194,\n",
       "          -15.4308, -222.5859,  -57.3049],\n",
       "        [ 324.5634,  324.5617,   60.7398,   82.8696,  125.1175,  324.4285,\n",
       "           20.4809,   47.1394,  171.7147,  324.5412,   68.0878,  117.8921,\n",
       "          283.4802,  148.9921,  323.9666,  180.6599,   23.7708,    7.0300,\n",
       "          276.5675,  179.9377,  121.7788,   74.9571,   60.3489,   24.7157,\n",
       "           15.4278,  222.5386,   57.2750],\n",
       "        [-324.6073, -324.5818,  -60.7583,  -82.8739, -125.1628, -324.4705,\n",
       "          -20.4765,  -47.1484, -171.7387, -324.5852,  -68.1141, -117.8949,\n",
       "         -283.4892, -148.9779, -323.9884, -180.6847,  -23.7713,   -7.0252,\n",
       "         -276.5925, -179.9742, -121.8216,  -74.9477,  -60.3539,  -24.7068,\n",
       "          -15.4311, -222.5144,  -57.2988],\n",
       "        [ 324.4292,  324.4280,   60.7312,   82.8900,  125.0869,  324.2925,\n",
       "           20.4719,   47.1084,  171.6771,  324.4385,   68.0975,  117.8767,\n",
       "          283.3861,  148.9341,  323.8492,  180.5927,   23.7662,    7.0559,\n",
       "          276.4680,  179.8901,  121.7413,   74.9361,   60.3415,   24.7308,\n",
       "           15.4223,  222.4568,   57.2807],\n",
       "        [-324.7070, -324.6765,  -60.7754,  -82.9059, -125.1459, -324.5539,\n",
       "          -20.4737,  -47.1637, -171.7662, -324.6546,  -68.1197, -117.9240,\n",
       "         -283.5471, -149.0243, -324.0423, -180.7324,  -23.7594,   -7.0276,\n",
       "         -276.6434, -179.9784, -121.8156,  -74.9849,  -60.3751,  -24.7153,\n",
       "          -15.4457, -222.5303,  -57.2999],\n",
       "        [ 324.4858,  324.4649,   60.7378,   82.8601,  125.1158,  324.3414,\n",
       "           20.4658,   47.1013,  171.7047,  324.4763,   68.1206,  117.8935,\n",
       "          283.4380,  148.9833,  323.9026,  180.6257,   23.7704,    7.0399,\n",
       "          276.5084,  179.9075,  121.7699,   74.9515,   60.3568,   24.7162,\n",
       "           15.4422,  222.4750,   57.2869],\n",
       "        [-324.3762, -324.4094,  -60.7144,  -82.8495, -125.0858, -324.2628,\n",
       "          -20.4755,  -47.1005, -171.6769, -324.4130,  -68.0922, -117.8521,\n",
       "         -283.3764, -148.9056, -323.8192, -180.5830,  -23.7630,   -7.0299,\n",
       "         -276.4413, -179.8815, -121.7167,  -74.9293,  -60.3604,  -24.7230,\n",
       "          -15.4481, -222.4443,  -57.2972],\n",
       "        [ 324.7623,  324.7045,   60.7794,   82.8956,  125.1833,  324.5726,\n",
       "           20.4745,   47.1746,  171.7788,  324.6626,   68.1123,  117.9346,\n",
       "          283.5565,  149.0322,  324.0583,  180.7353,   23.7680,    7.0280,\n",
       "          276.6585,  180.0285,  121.8464,   74.9857,   60.3787,   24.7240,\n",
       "           15.4357,  222.5484,   57.2745],\n",
       "        [-324.4273, -324.4542,  -60.7345,  -82.8568, -125.0896, -324.3130,\n",
       "          -20.4817,  -47.1105, -171.6871, -324.4518,  -68.0776, -117.8596,\n",
       "         -283.3911, -148.9374, -323.8647, -180.6017,  -23.7698,   -7.0222,\n",
       "         -276.4780, -179.8868, -121.7405,  -74.9365,  -60.3623,  -24.7194,\n",
       "          -15.4384, -222.4690,  -57.2856],\n",
       "        [ 324.3755,  324.4116,   60.7324,   82.8714,  125.0720,  324.2487,\n",
       "           20.4805,   47.1075,  171.6520,  324.4038,   68.1140,  117.8480,\n",
       "          283.3802,  148.9244,  323.8350,  180.5751,   23.7757,    7.0411,\n",
       "          276.4467,  179.8707,  121.7281,   74.9257,   60.3482,   24.7182,\n",
       "           15.4353,  222.4594,   57.2761],\n",
       "        [ 324.5641,  324.5420,   60.7486,   82.8724,  125.1207,  324.4311,\n",
       "           20.4725,   47.1539,  171.7117,  324.5372,   68.1112,  117.8869,\n",
       "          283.4605,  148.9641,  323.9343,  180.6592,   23.7696,    7.0484,\n",
       "          276.5414,  179.9351,  121.7893,   74.9649,   60.3738,   24.7282,\n",
       "           15.4398,  222.5021,   57.2943],\n",
       "        [ 324.7625,  324.6922,   60.7730,   82.9199,  125.1590,  324.5789,\n",
       "           20.4814,   47.1607,  171.7923,  324.6813,   68.1210,  117.9427,\n",
       "          283.5762,  149.0318,  324.0764,  180.7572,   23.7641,    7.0273,\n",
       "          276.6731,  180.0206,  121.8448,   74.9784,   60.3778,   24.7123,\n",
       "           15.4433,  222.5560,   57.3005],\n",
       "        [ 324.5278,  324.5404,   60.7252,   82.8826,  125.1035,  324.3982,\n",
       "           20.4845,   47.1426,  171.7170,  324.5245,   68.1164,  117.8654,\n",
       "          283.4524,  148.9702,  323.9282,  180.6677,   23.7609,    7.0360,\n",
       "          276.5456,  179.9406,  121.7731,   74.9507,   60.3637,   24.6889,\n",
       "           15.4377,  222.4845,   57.2996],\n",
       "        [-324.4514, -324.4615,  -60.7398,  -82.8864, -125.0916, -324.3414,\n",
       "          -20.4666,  -47.0977, -171.6916, -324.4904,  -68.0803, -117.8695,\n",
       "         -283.4222, -148.9465, -323.8787, -180.6328,  -23.7530,   -7.0190,\n",
       "         -276.5069, -179.8928, -121.7726,  -74.9539,  -60.3517,  -24.7053,\n",
       "          -15.4091, -222.4725,  -57.2927],\n",
       "        [-324.2551, -324.3151,  -60.7098,  -82.8445, -125.0531, -324.1420,\n",
       "          -20.4655,  -47.0763, -171.6176, -324.3038,  -68.0910, -117.8145,\n",
       "         -283.2873, -148.8548, -323.7248, -180.5239,  -23.7450,   -7.0421,\n",
       "         -276.3718, -179.8183, -121.6702,  -74.9085,  -60.3455,  -24.7208,\n",
       "          -15.4461, -222.4226,  -57.2837],\n",
       "        [ 324.3727,  324.4096,   60.7303,   82.8684,  125.0761,  324.2566,\n",
       "           20.4598,   47.1190,  171.6503,  324.4181,   68.0777,  117.8554,\n",
       "          283.3481,  148.9068,  323.8145,  180.5876,   23.7549,    7.0271,\n",
       "          276.4339,  179.8711,  121.7294,   74.9307,   60.3499,   24.7220,\n",
       "           15.4503,  222.4523,   57.2827],\n",
       "        [ 324.8794,  324.7968,   60.8009,   82.9181,  125.1897,  324.6669,\n",
       "           20.4589,   47.1854,  171.8101,  324.7564,   68.1178,  117.9631,\n",
       "          283.6541,  149.0626,  324.1374,  180.7983,   23.7728,    7.0279,\n",
       "          276.7353,  180.0532,  121.8778,   74.9978,   60.3896,   24.7070,\n",
       "           15.4341,  222.5883,   57.3025],\n",
       "        [ 324.0487,  324.0831,   60.6297,   82.8024,  124.9760,  323.9647,\n",
       "           20.4464,   47.0296,  171.5248,  324.1213,   68.0923,  117.7348,\n",
       "          283.1169,  148.7265,  323.5701,  180.4438,   23.7489,    7.0511,\n",
       "          276.1754,  179.7738,  121.5881,   74.8465,   60.3247,   24.7000,\n",
       "           15.4408,  222.3693,   57.2916],\n",
       "        [ 323.9074,  323.9467,   60.5857,   82.7711,  124.9035,  323.8485,\n",
       "           20.4210,   46.9731,  171.4888,  323.9891,   68.0738,  117.6809,\n",
       "          283.0155,  148.6208,  323.4273,  180.3851,   23.7476,    7.0361,\n",
       "          276.0141,  179.7173,  121.5313,   74.7918,   60.2781,   24.7118,\n",
       "           15.4503,  222.2899,   57.2702],\n",
       "        [-324.6735, -324.6429,  -60.7650,  -82.8807, -125.1565, -324.5257,\n",
       "          -20.4890,  -47.1607, -171.7907, -324.6098,  -68.1098, -117.9036,\n",
       "         -283.5223, -149.0173, -324.0149, -180.7253,  -23.7706,   -7.0260,\n",
       "         -276.6182, -179.9800, -121.8253,  -74.9553,  -60.3598,  -24.7148,\n",
       "          -15.4269, -222.5349,  -57.2899],\n",
       "        [ 325.1161,  324.9790,   60.8254,   82.9391,  125.2579,  324.8522,\n",
       "           20.4734,   47.2013,  171.8967,  324.8884,   68.1481,  117.9999,\n",
       "          283.7438,  149.1203,  324.2738,  180.8709,   23.7649,    7.0035,\n",
       "          276.8395,  180.1306,  121.9151,   75.0037,   60.3987,   24.7200,\n",
       "           15.4205,  222.6534,   57.3050],\n",
       "        [-324.7675, -324.7302,  -60.7722,  -82.9124, -125.1581, -324.5973,\n",
       "          -20.4823,  -47.1798, -171.7815, -324.6738,  -68.1140, -117.9595,\n",
       "         -283.5875, -149.0539, -324.0793, -180.7441,  -23.7709,   -7.0416,\n",
       "         -276.6850, -180.0067, -121.8385,  -74.9868,  -60.3819,  -24.7149,\n",
       "          -15.4427, -222.5510,  -57.2813],\n",
       "        [-324.6324, -324.6129,  -60.7511,  -82.9147, -125.1463, -324.4908,\n",
       "          -20.4878,  -47.1530, -171.7554, -324.6028,  -68.1127, -117.9061,\n",
       "         -283.5207, -148.9849, -323.9914, -180.7000,  -23.7699,   -7.0266,\n",
       "         -276.6073, -179.9626, -121.8302,  -74.9547,  -60.3612,  -24.7102,\n",
       "          -15.4452, -222.5170,  -57.2997],\n",
       "        [-324.7179, -324.6815,  -60.7797,  -82.8954, -125.1475, -324.5607,\n",
       "          -20.4788,  -47.1733, -171.7761, -324.6338,  -68.1236, -117.9393,\n",
       "         -283.5577, -149.0385, -324.0352, -180.7136,  -23.7807,   -7.0273,\n",
       "         -276.6672, -179.9880, -121.8364,  -74.9695,  -60.3756,  -24.7150,\n",
       "          -15.4535, -222.5267,  -57.3078],\n",
       "        [-324.7234, -324.6674,  -60.7594,  -82.8926, -125.1585, -324.5669,\n",
       "          -20.4838,  -47.1823, -171.7834, -324.6504,  -68.1154, -117.9229,\n",
       "         -283.5577, -149.0434, -324.0501, -180.7395,  -23.7567,   -7.0359,\n",
       "         -276.6660, -180.0105, -121.8433,  -74.9904,  -60.3773,  -24.7205,\n",
       "          -15.4475, -222.5427,  -57.2968],\n",
       "        [ 324.6848,  324.6483,   60.7685,   82.8850,  125.1677,  324.5236,\n",
       "           20.4837,   47.1555,  171.7792,  324.6244,   68.1067,  117.9308,\n",
       "          283.5295,  149.0352,  324.0120,  180.7073,   23.7764,    7.0251,\n",
       "          276.6251,  179.9906,  121.8065,   74.9753,   60.3757,   24.7159,\n",
       "           15.4333,  222.5389,   57.2661],\n",
       "        [ 324.5460,  324.5386,   60.7407,   82.8868,  125.1180,  324.4105,\n",
       "           20.4757,   47.1097,  171.7284,  324.5285,   68.0887,  117.8843,\n",
       "          283.4724,  148.9753,  323.9460,  180.6462,   23.7702,    7.0404,\n",
       "          276.5518,  179.9357,  121.7928,   74.9625,   60.3691,   24.7347,\n",
       "           15.4239,  222.4947,   57.2783],\n",
       "        [-324.9182, -324.8341,  -60.7907,  -82.9352, -125.2022, -324.6950,\n",
       "          -20.4736,  -47.1889, -171.8351, -324.7775,  -68.1291, -117.9692,\n",
       "         -283.6450, -149.0844, -324.1371, -180.8080,  -23.7734,   -7.0136,\n",
       "         -276.7504, -180.0578, -121.8745,  -74.9986,  -60.3890,  -24.7017,\n",
       "          -15.4180, -222.5991,  -57.2991]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4c78234a-e277-4118-bb33-bea7f86b2fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def sample_model(start_vector, Fvh, Fhh, i1vh, i1hh, i2vh, i2hh, Ovh, Ohh, bias1, bias2, bias3, bias4, output_matrix):\n",
    "    \"\"\"\n",
    "    Generate a sequence of samples from the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_vector: Initial one-hot encoded vector to start the generation (torch tensor of shape (1, input_size))\n",
    "    - Other parameters: Model weights and biases\n",
    "    \n",
    "    Returns:\n",
    "    - Generated sequence (list of integers)\n",
    "    \"\"\"\n",
    "    input_size = start_vector.shape[1]\n",
    "    hidden_size = Fvh.shape[1]\n",
    "    \n",
    "    # Initialize hidden and cell states\n",
    "    h = torch.zeros((1, hidden_size))\n",
    "    c = torch.zeros((1, hidden_size))\n",
    "    \n",
    "    # Initialize the generated sequence with the index of the start vector\n",
    "    generated_sequence = [torch.argmax(start_vector).item()]\n",
    "    \n",
    "    # Loop until '.' is generated or the maximum sequence length is reached\n",
    "    while generated_sequence[-1] != 0 and len(generated_sequence) <= max_length:\n",
    "        x = start_vector  # Use the provided start vector as input\n",
    "        \n",
    "        preact1 = x @ Fvh + h @ Fhh + bias1\n",
    "        preact2 = x @ i1vh + h @ i1hh + bias2\n",
    "        preact3 = x @ i2vh + h @ i2hh + bias3\n",
    "        preact4 = x @ Ovh + h @ Ohh + bias4\n",
    "        \n",
    "        act1 = torch.sigmoid(preact1)\n",
    "        act2 = torch.sigmoid(preact2)\n",
    "        act3 = torch.tanh(preact3)\n",
    "        act4 = torch.sigmoid(preact4)\n",
    "        \n",
    "        c = act1 * c + act2 * act3\n",
    "        h = torch.tanh(c) * act4\n",
    "        \n",
    "        logits = h @ output_matrix\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        # Sample from the probability distribution to get the next input\n",
    "        next_index = torch.multinomial(probs.squeeze(), 1).item()\n",
    "        \n",
    "        generated_sequence.append(next_index)\n",
    "        \n",
    "        # Update the start vector with the one-hot encoded representation of the next index\n",
    "        start_vector = torch.zeros((1, input_size))\n",
    "        start_vector[0, next_index] = 1\n",
    "        \n",
    "    return [itos[ch] for ch in generated_sequence]\n",
    "\n",
    "# Example usage\n",
    "input_size = 27  # Size of the one-hot encoded vector\n",
    "start_index = 1  # Index representing the starting point (e.g., 'a')\n",
    "start_vector = torch.zeros((1, input_size))\n",
    "start_vector[0, start_index] = 3  # One-hot encode the starting point\n",
    "\n",
    "# Assume Fvh, Fhh, i1vh, i1hh, i2vh, i2hh, Ovh, Ohh, bias1, bias2, bias3, bias4, and output_matrix are already defined\n",
    "\n",
    "max_length = 15  # Maximum length of the generated sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7f6d2bc2-0b8b-4014-a382-629827d57d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aquriizpiz.\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = sample_model(start_vector, Fvh, Fhh, i1vh, i1hh, i2vh, i2hh, Ovh, Ohh, bias1, bias2, bias3, bias4, output_matrix)\n",
    "print(''.join(generated_sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
