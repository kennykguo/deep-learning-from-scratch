{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ff91fc80-0bc4-471f-91e9-6c51ac49657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b5d27627-2fa1-4c6e-bfad-dd87852ae629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "Character to index mapping: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "Vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "# Read in all the words\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "print(words[:10])\n",
    "\n",
    "# Build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(f'Character to index mapping: {itos}')\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Shuffle the words\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "aac0e6ea-cbf7-48fa-98d6-9dcf2fe77f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 25, 21, 8, 5, 14, 7, 0, 4, 9, 15, 14, 4, 18, 5, 0, 24, 1, 22, 9]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the words with '.' so the model can learn when a name should end\n",
    "def encode_words(words):\n",
    "    encoded = []\n",
    "    for w in words:\n",
    "        encoded.extend([stoi[ch] for ch in '.' + w ])\n",
    "    return encoded\n",
    "\n",
    "encoded = encode_words(words)\n",
    "encoded[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "754cc12d-c2ee-4650-8287-0deadf2ac4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for creating training and test sets\n",
    "n = len(encoded)\n",
    "n1 = int(0.8 * n)\n",
    "block_size = 8\n",
    "batch_size = 1\n",
    "\n",
    "train_seq = encoded[:n1]\n",
    "dev_seq = encoded[n1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b1e98e8b-8081-4f09-811c-4c39040b7c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(seq, block_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(0, len(seq) - block_size, block_size):\n",
    "        X.append(seq[i:i+block_size])\n",
    "        Y.append(seq[i+1:i+block_size+1])\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32)\n",
    "    return X, Y\n",
    "\n",
    "Xtr, Ytr = create_pairs(train_seq, block_size)\n",
    "Xdev, Ydev = create_pairs(dev_seq, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1caa97e5-c13d-417f-8c04-620c3685f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  9., 15., 14.,  4., 18.,  5.,  0.])\n",
      "tensor([ 9., 15., 14.,  4., 18.,  5.,  0., 24.])\n",
      "tensor([24.,  1., 22.,  9.,  5., 14.,  0., 10.])\n",
      "tensor([ 1., 22.,  9.,  5., 14.,  0., 10., 15.])\n",
      "tensor([15., 18.,  9.,  0., 10., 21.,  1., 14.])\n",
      "tensor([18.,  9.,  0., 10., 21.,  1., 14., 12.])\n",
      "Training data shapes - X: torch.Size([22814, 8]), Y: torch.Size([22814, 8])\n",
      "Development data shapes - X: torch.Size([5703, 8]), Y: torch.Size([5703, 8])\n"
     ]
    }
   ],
   "source": [
    "# We are sure that we can pass in previous example's hidden layer\n",
    "print(Xtr[1])\n",
    "print(Ytr[1])\n",
    "print(Xtr[2])\n",
    "print(Ytr[2])\n",
    "print(Xtr[3])\n",
    "print(Ytr[3])\n",
    "print(f'Training data shapes - X: {Xtr.shape}, Y: {Ytr.shape}')\n",
    "print(f'Development data shapes - X: {Xdev.shape}, Y: {Ydev.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4cae0b00-fd29-48c1-b142-60f568b5558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "batch_size = 1\n",
    "hidden_size = 30\n",
    "time_steps = 8\n",
    "input_size = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fa2067b3-2040-49a4-a839-c1c10a07bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax(x):\n",
    "#     e_x = torch.exp(x - torch.max(x, dim=1, keepdim=True)[0])\n",
    "#     return e_x / torch.sum(e_x, dim=1, keepdim=True)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = torch.exp(x)\n",
    "    return e_x / torch.sum(e_x, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7fddc288-34bc-4c65-bb6e-fa74c5799979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize parameters\n",
    "Fvh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "i1vh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "i2vh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "Ovh = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "\n",
    "Fhh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "i1hh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "i2hh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "Ohh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "\n",
    "bias1 = torch.zeros(1, hidden_size)\n",
    "bias2 = torch.zeros(1, hidden_size)\n",
    "bias3 = torch.zeros(1, hidden_size)\n",
    "bias4 = torch.zeros(1, hidden_size)\n",
    "\n",
    "output_matrix = torch.randn(hidden_size, vocab_size) * 0.01\n",
    "output_bias = torch.zeros(1, vocab_size)\n",
    "\n",
    "X = {}\n",
    "H = {}\n",
    "C = {}\n",
    "Ct = {}\n",
    "preact1 = {}\n",
    "preact2 = {}\n",
    "preact3 = {}\n",
    "preact4 = {}\n",
    "act1 = {}\n",
    "act2 = {}\n",
    "act3 = {}\n",
    "act4 = {}\n",
    "logits = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4c78234a-e277-4118-bb33-bea7f86b2fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(hprev, cprev, seed_ix, n):\n",
    "    # Create one hot vector based on input seed_ix\n",
    "    x = torch.zeros(1, vocab_size)\n",
    "    x[0, seed_ix] = 1\n",
    "    ixes = []\n",
    "\n",
    "    for t in range(n):\n",
    "        preact1 = x @ Fvh + hprev @ Fhh + bias1 # (1, 27) @ (27, 30) + (1, 30) @ (30, 30) + (30) = (1, 30)\n",
    "        preact2 = x  @ i1vh + hprev  @ i1hh * bias2 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "        preact3 = x  @ i2vh + hprev  @ i2hh + bias3 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "        preact4 = x  @ Ovh + hprev  @ Ohh + bias4 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "        \n",
    "        act1 = torch.sigmoid(preact1) # (1, 30)\n",
    "        act2 = torch.sigmoid(preact2) # (1, 30)\n",
    "        act3 = torch.tanh(preact3) # (1, 30)\n",
    "        act4 = torch.sigmoid(preact4) # (1, 30)\n",
    "\n",
    "        # Forget gate * previous cell state + i1 gate * i2 gate\n",
    "        C = act1 * cprev  + act2  * act3  # (1, 30)\n",
    "        Ct = torch.tanh(C) # (1, 30)\n",
    "        H = Ct * act4  # (1, 30)          \n",
    "\n",
    "        # Logits\n",
    "        logits = H  @ output_matrix + output_bias # (1, 30) @ (30, 27) + (1, 27) = (1, 27)\n",
    "\n",
    "        # Probability\n",
    "        counts = logits.exp()\n",
    "        counts_sum = counts.sum(1, keepdims=True)\n",
    "        counts_sum_inv = counts_sum**-1\n",
    "        probs = counts * counts_sum_inv\n",
    "\n",
    "        # Sample from distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        if (ix == 0):\n",
    "            break\n",
    "        ixes.append(ix)\n",
    "\n",
    "        # Update our parameters\n",
    "        x = torch.zeros(1, vocab_size)\n",
    "        x[0, ix] = 1\n",
    "        hprev = H\n",
    "        cprev = C\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a1b15766-4d25-43b2-ae0a-dfd12d510093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(25.4559)\n",
      "iteration:  0\n",
      "----\n",
      " dee \n",
      "----\n",
      "loss:  tensor(21.4874)\n",
      "iteration:  1000\n",
      "----\n",
      " dlsc \n",
      "----\n",
      "loss:  tensor(23.5546)\n",
      "iteration:  2000\n",
      "----\n",
      " jrerwni \n",
      "----\n",
      "loss:  tensor(20.9428)\n",
      "iteration:  3000\n",
      "----\n",
      " palmy \n",
      "----\n",
      "loss:  tensor(19.0532)\n",
      "iteration:  4000\n",
      "----\n",
      " jqanen \n",
      "----\n",
      "loss:  tensor(26.2684)\n",
      "iteration:  5000\n",
      "----\n",
      " npihany \n",
      "----\n",
      "loss:  tensor(22.2071)\n",
      "iteration:  6000\n",
      "----\n",
      " grytenn \n",
      "----\n",
      "loss:  tensor(19.8890)\n",
      "iteration:  7000\n",
      "----\n",
      " rasie \n",
      "----\n",
      "loss:  tensor(15.2436)\n",
      "iteration:  8000\n",
      "----\n",
      " avabcl \n",
      "----\n",
      "loss:  tensor(14.7985)\n",
      "iteration:  9000\n",
      "----\n",
      " jlzen \n",
      "----\n",
      "loss:  tensor(19.4003)\n",
      "iteration:  10000\n",
      "----\n",
      " heza \n",
      "----\n",
      "loss:  tensor(16.3885)\n",
      "iteration:  11000\n",
      "----\n",
      " slhyanni \n",
      "----\n",
      "loss:  tensor(16.7889)\n",
      "iteration:  12000\n",
      "----\n",
      " cetmay \n",
      "----\n",
      "loss:  tensor(23.2901)\n",
      "iteration:  13000\n",
      "----\n",
      " zalria \n",
      "----\n",
      "loss:  tensor(20.9950)\n",
      "iteration:  14000\n",
      "----\n",
      " nenla \n",
      "----\n",
      "loss:  tensor(21.3691)\n",
      "iteration:  15000\n",
      "----\n",
      " tittha \n",
      "----\n",
      "loss:  tensor(21.7759)\n",
      "iteration:  16000\n",
      "----\n",
      " corakdo \n",
      "----\n",
      "loss:  tensor(22.9309)\n",
      "iteration:  17000\n",
      "----\n",
      " peza \n",
      "----\n",
      "loss:  tensor(15.9543)\n",
      "iteration:  18000\n",
      "----\n",
      " saianse \n",
      "----\n",
      "loss:  tensor(20.5803)\n",
      "iteration:  19000\n",
      "----\n",
      " ley \n",
      "----\n",
      "loss:  tensor(21.0014)\n",
      "iteration:  20000\n",
      "----\n",
      " monset \n",
      "----\n",
      "loss:  tensor(19.6828)\n",
      "iteration:  21000\n",
      "----\n",
      " edra \n",
      "----\n",
      "loss:  tensor(17.4550)\n",
      "iteration:  22000\n",
      "----\n",
      " bea \n",
      "----\n",
      "loss:  tensor(17.6764)\n",
      "iteration:  23000\n",
      "----\n",
      " taeleose \n",
      "----\n",
      "loss:  tensor(17.1579)\n",
      "iteration:  24000\n",
      "----\n",
      " laontm \n",
      "----\n",
      "loss:  tensor(15.4545)\n",
      "iteration:  25000\n",
      "----\n",
      " hatofa \n",
      "----\n",
      "loss:  tensor(17.0832)\n",
      "iteration:  26000\n",
      "----\n",
      " rynan \n",
      "----\n",
      "loss:  tensor(15.1573)\n",
      "iteration:  27000\n",
      "----\n",
      " cana \n",
      "----\n",
      "loss:  tensor(11.7579)\n",
      "iteration:  28000\n",
      "----\n",
      " muton \n",
      "----\n",
      "loss:  tensor(20.3217)\n",
      "iteration:  29000\n",
      "----\n",
      " yel \n",
      "----\n",
      "loss:  tensor(16.8169)\n",
      "iteration:  30000\n",
      "----\n",
      " jenxan \n",
      "----\n",
      "loss:  tensor(19.8526)\n",
      "iteration:  31000\n",
      "----\n",
      " dri \n",
      "----\n",
      "loss:  tensor(13.8541)\n",
      "iteration:  32000\n",
      "----\n",
      " lesan \n",
      "----\n",
      "loss:  tensor(21.8181)\n",
      "iteration:  33000\n",
      "----\n",
      " kah \n",
      "----\n",
      "loss:  tensor(15.5664)\n",
      "iteration:  34000\n",
      "----\n",
      " tharil \n",
      "----\n",
      "loss:  tensor(19.6613)\n",
      "iteration:  35000\n",
      "----\n",
      " zaniam \n",
      "----\n",
      "loss:  tensor(16.2495)\n",
      "iteration:  36000\n",
      "----\n",
      " meniri \n",
      "----\n",
      "loss:  tensor(18.9609)\n",
      "iteration:  37000\n",
      "----\n",
      " ri \n",
      "----\n",
      "loss:  tensor(14.6977)\n",
      "iteration:  38000\n",
      "----\n",
      " vah \n",
      "----\n",
      "loss:  tensor(21.1093)\n",
      "iteration:  39000\n",
      "----\n",
      " laitten \n",
      "----\n",
      "loss:  tensor(15.4780)\n",
      "iteration:  40000\n",
      "----\n",
      " edellli \n",
      "----\n",
      "loss:  tensor(25.6298)\n",
      "iteration:  41000\n",
      "----\n",
      " covayre \n",
      "----\n",
      "loss:  tensor(16.7649)\n",
      "iteration:  42000\n",
      "----\n",
      " nakara \n",
      "----\n",
      "loss:  tensor(18.4521)\n",
      "iteration:  43000\n",
      "----\n",
      " roituo \n",
      "----\n",
      "loss:  tensor(20.6295)\n",
      "iteration:  44000\n",
      "----\n",
      " ker \n",
      "----\n",
      "loss:  tensor(15.5297)\n",
      "iteration:  45000\n",
      "----\n",
      " admione \n",
      "----\n",
      "loss:  tensor(22.0601)\n",
      "iteration:  46000\n",
      "----\n",
      " y \n",
      "----\n",
      "loss:  tensor(20.9313)\n",
      "iteration:  47000\n",
      "----\n",
      " jiolue \n",
      "----\n",
      "loss:  tensor(16.5785)\n",
      "iteration:  48000\n",
      "----\n",
      " rentin \n",
      "----\n",
      "loss:  tensor(16.1046)\n",
      "iteration:  49000\n",
      "----\n",
      " erie \n",
      "----\n",
      "loss:  tensor(20.6440)\n",
      "iteration:  50000\n",
      "----\n",
      " skagh \n",
      "----\n",
      "loss:  tensor(13.3233)\n",
      "iteration:  51000\n",
      "----\n",
      " ayduco \n",
      "----\n",
      "loss:  tensor(19.5683)\n",
      "iteration:  52000\n",
      "----\n",
      " litaa \n",
      "----\n",
      "loss:  tensor(15.7481)\n",
      "iteration:  53000\n",
      "----\n",
      " jikan \n",
      "----\n",
      "loss:  tensor(20.8072)\n",
      "iteration:  54000\n",
      "----\n",
      " anisniye \n",
      "----\n",
      "loss:  tensor(18.0565)\n",
      "iteration:  55000\n",
      "----\n",
      " kashjel \n",
      "----\n",
      "loss:  tensor(17.1611)\n",
      "iteration:  56000\n",
      "----\n",
      " chenin \n",
      "----\n",
      "loss:  tensor(16.6548)\n",
      "iteration:  57000\n",
      "----\n",
      " leiqua \n",
      "----\n",
      "loss:  tensor(19.2076)\n",
      "iteration:  58000\n",
      "----\n",
      " rocin \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[202], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m     counts_sum_inv \u001b[38;5;241m=\u001b[39m counts_sum\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m     probs \u001b[38;5;241m=\u001b[39m counts \u001b[38;5;241m*\u001b[39m counts_sum_inv\n\u001b[0;32m---> 53\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m \u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlogprobs[\u001b[38;5;241m0\u001b[39m, Yb[t]]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Continue the sequence for time step -1 of the next input sequence by passing it on\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iterations = 0\n",
    "\n",
    "while max_iterations < 100000:\n",
    "    \n",
    "    # Looping over batches (each batch is just a single row)\n",
    "    for batch_num in range(Xtr.shape[0]):\n",
    "\n",
    "        # Initial cell state and hidden state and loss\n",
    "        if (max_iterations == 0):\n",
    "            H[-1] = torch.zeros(1, hidden_size)\n",
    "            C[-1] = torch.zeros(1, hidden_size)\n",
    "        loss = 0   \n",
    "        \n",
    "        # Get a batch of random numbers into correct shape\n",
    "        # Get corresponding row from training set\n",
    "        Xb = Xtr[batch_num, :].to(torch.long) # (1, 8)\n",
    "        # Conver to one hot vectors\n",
    "        Xb = F.one_hot(Xb, 27).float() # (8, 27)\n",
    "        # Get corresponding row from training set\n",
    "        Yb = Ytr[batch_num].to(torch.long) # (8, 1)\n",
    "        \n",
    "### --------------------------------------------------------------------------------------------------------------        \n",
    "        # Forward pass over all time steps\n",
    "        for t in range(time_steps):\n",
    "            # Pass through weight matrices (last hidden activations and current time step's input)\n",
    "            # This means that there is one less hidden layer than the number of activations inputted\n",
    "            # Xb[t] gets a single row from the training set\n",
    "            preact1[t] = Xb[t] @ Fvh + H[t-1] @ Fhh + bias1 # (1, 27) @ (27, 30) + (1, 30) @ (30, 30) + (30) = (1, 30)\n",
    "            preact2[t] = Xb[t]  @ i1vh + H[t-1]  @ i1hh * bias2 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "            preact3[t] = Xb[t]  @ i2vh + H[t-1]  @ i2hh + bias3 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "            preact4[t] = Xb[t]  @ Ovh + H[t-1]  @ Ohh + bias4 # (32, 27) @ (27, 30) + (32, 30) @ (30, 30) + (30)\n",
    "\n",
    "            # Compute activations\n",
    "            act1[t] = torch.sigmoid(preact1[t]) # (1, 30)\n",
    "            act2[t] = torch.sigmoid(preact2[t]) # (1, 30)\n",
    "            act3[t] = torch.tanh(preact3[t]) # (1, 30)\n",
    "            act4[t] = torch.sigmoid(preact4[t]) # (1, 30)\n",
    "            \n",
    "            # Forget gate * previous cell state + i1 gate * i2 gate\n",
    "            C[t] = act1[t] * C[t-1]  + act2[t]  * act3[t]  # (1, 30)\n",
    "            Ct[t] = torch.tanh(C[t]) # (1, 30)\n",
    "            # Compute current time step's hidden output to be passed onto the next time step\n",
    "            H[t] = Ct[t] * act4[t]  # (1, 30)          \n",
    "\n",
    "            # Logits\n",
    "            logits[t] = H[t]  @ output_matrix + output_bias # (1, 30) @ (30, 27) + (1, 27) = (1, 27)\n",
    "\n",
    "            # Cross entropy\n",
    "            counts = logits[t].exp()\n",
    "            counts_sum = counts.sum(1, keepdims=True)\n",
    "            counts_sum_inv = counts_sum**-1\n",
    "            probs = counts * counts_sum_inv\n",
    "            logprobs = probs.log()\n",
    "            loss += -logprobs[0, Yb[t]]\n",
    "\n",
    "        # Continue the sequence for time step -1 of the next input sequence by passing it on\n",
    "        H[-1] = H[time_steps - 1]\n",
    "        C[-1] = C[time_steps - 1]\n",
    "\n",
    "        # Print out the loss occasionally, and sample from the model occassionally\n",
    "        if max_iterations % 1000 == 0:\n",
    "            print(\"loss: \", loss)\n",
    "            print(\"iteration: \", max_iterations)\n",
    "            # Pass in the last hidden layer, and last cell state\n",
    "            # Pass in the index which can be referenced through itos\n",
    "            sample_ix = sample(H[-1], C[-1], 0, 15)\n",
    "            txt = ''.join(itos[ix] for ix in sample_ix)\n",
    "            print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "        # Increment max iterations (each training example means one iteration)\n",
    "        max_iterations += 1 \n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------------\n",
    "        # Backward pass\n",
    "        # Set to zeros initially\n",
    "        dHnext = torch.zeros_like(H[0])\n",
    "        dCnext = torch.zeros_like(C[0])\n",
    "        \n",
    "        dFvh = torch.zeros(vocab_size, hidden_size)\n",
    "        di1vh = torch.zeros(vocab_size, hidden_size)\n",
    "        di2vh = torch.zeros(vocab_size, hidden_size)\n",
    "        dOvh = torch.zeros(vocab_size, hidden_size)\n",
    "        \n",
    "        dFhh = torch.zeros(hidden_size, hidden_size)\n",
    "        di1hh = torch.zeros(hidden_size, hidden_size)\n",
    "        di2hh = torch.zeros(hidden_size, hidden_size)\n",
    "        dOhh = torch.zeros(hidden_size, hidden_size)\n",
    "        \n",
    "        dbias1 = torch.zeros(1, hidden_size)\n",
    "        dbias2 = torch.zeros(1, hidden_size)\n",
    "        dbias3 = torch.zeros(1, hidden_size)\n",
    "        dbias4 = torch.zeros(1, hidden_size)\n",
    "        \n",
    "        doutput_matrix = torch.zeros(hidden_size, vocab_size)\n",
    "        doutput_bias = torch.zeros(1, vocab_size)\n",
    "\n",
    "        for t in reversed(range(time_steps)):\n",
    "            # Backpropogate cross entropy\n",
    "            dlogits = F.softmax(logits[t], 1)\n",
    "            dlogits [0, Yb[t]] -= 1\n",
    "            \n",
    "            # Backpropogate output matrix and its bias\n",
    "            doutput_matrix +=  H[t].T @ dlogits # (30, 1) @ (1, 27) = (30, 27)\n",
    "            doutput_bias += dlogits # (1, 27)\n",
    "            \n",
    "            # Backpropogate into H (two derivatives due to 2 dependencies)\n",
    "            dH = dlogits  @ output_matrix.T + dHnext # (1, 27) @ (27, 30) + (1, 30) = (1, 30)\n",
    "            \n",
    "            # Backpropogate dact4 (output gate activations)\n",
    "            dact4  = dH  * Ct[t]  # (1, 30) * (1, 30) = (1, 30)\n",
    "            \n",
    "            # Backpropogate dC (current cell state and also two derivatives)\n",
    "            dC = dH * act4[t] * (1 - torch.tanh(C[t]) ** 2) + dCnext # (1, 30) * (1, 30) * (1, 30) = (1, 30)\n",
    "            \n",
    "            ### Backpropogate act1 and previous cell state\n",
    "            # Forget gate activations\n",
    "            # dCnext activations\n",
    "            dact1 = dC * C[t-1] # (1, 30) * (1, 30) = (1, 30)\n",
    "            dCnext = dC * act1[t]\n",
    "            \n",
    "            # Backpropogate i1 activations\n",
    "            dact2  = dC  * act3[t] # (1, 27)\n",
    "            \n",
    "            # Backpropogate i2 activations\n",
    "            dact3  = dC  * act2[t]  # (1, 27)\n",
    "            \n",
    "            # Backpropogate all preactivations\n",
    "            dpreact1 = dact1 * act1[t] * (1 - act1[t])\n",
    "            dpreact2 = dact2 * act2[t] * (1 - act2[t])\n",
    "            dpreact3 = dact3 * (1 - torch.tanh(preact3[t]) ** 2)\n",
    "            dpreact4 = dact4 * act4[t] * (1 - act4[t])\n",
    "            \n",
    "            # Backpropogate gates\n",
    "            dFvh += Xb[t].reshape(vocab_size, 1) @ dpreact1  # (27, 1) (1, 30) = (27, 30)\n",
    "            dFhh += H[t-1].reshape(hidden_size, 1) @ dpreact1  # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias1 += dpreact1\n",
    "            \n",
    "            di1vh += Xb[t].reshape(vocab_size, 1) @ dpreact2 # (27, 1) (1, 30) = (27, 30)\n",
    "            di1hh += H[t-1].reshape(hidden_size, 1) @ dpreact2 # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias2 += dpreact2\n",
    "            \n",
    "            di2vh += Xb[t].reshape(vocab_size, 1) @ dpreact3 # (27, 1) (1, 30) = (27, 30)\n",
    "            di2hh += H[t-1].reshape(hidden_size, 1) @ dpreact3 # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias3 += dpreact3\n",
    "            \n",
    "            dOvh += Xb[t].reshape(vocab_size, 1) @ dpreact4 # (27, 1) (1, 30) = (27, 30)\n",
    "            dOhh += H[t-1].reshape(hidden_size, 1) @ dpreact4 # (30, 1) (1, 30) = (30, 30)\n",
    "            dbias4 += dpreact4\n",
    "            \n",
    "            # Backpropogate dHnext\n",
    "            # (1, 30) @ (30, 30) ...  = (1, 30)\n",
    "            dHnext  = dpreact1  @ Fhh.T + dpreact2  @ i1hh.T + dpreact3  @ i2hh.T + dpreact4  @ Ohh.T\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_([dFvh, di1vh, di2vh, dOvh, dFhh, di1hh, di2hh, dOhh, dbias1, dbias2, dbias3, dbias4, doutput_matrix, doutput_bias], max_norm=1.0)\n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "        # Update parameters using gradients\n",
    "        Fvh -= lr * dFvh\n",
    "        i1vh -= lr * di1vh\n",
    "        i2vh -= lr * di2vh\n",
    "        Ovh -= lr * dOvh\n",
    "\n",
    "        Fhh -= lr * dFhh\n",
    "        i1hh -= lr * di1hh\n",
    "        i2hh -= lr * di2hh\n",
    "        Ohh -= lr * dOhh\n",
    "        \n",
    "        bias1 -= lr * dbias1\n",
    "        bias2 -= lr * dbias2\n",
    "        bias3 -= lr * dbias3\n",
    "        bias4 -= lr * dbias4\n",
    "\n",
    "        output_matrix -= lr * doutput_matrix\n",
    "        output_bias -= lr * doutput_bias\n",
    "    \n",
    "    if (max_iterations == 10000):\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1c891c73-92dc-46f4-ba00-29ed7e68943d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.9731e-01,  4.2908e-01,  7.9749e-02, -3.1476e-01, -1.8931e-01,\n",
       "          5.4785e-01, -2.9040e-02, -2.6152e-01, -1.5964e-01, -2.6184e-01,\n",
       "          1.2604e-01, -2.7313e-01, -5.3776e-02, -2.8828e-01,  3.6541e-01,\n",
       "          2.2846e-01, -8.3515e-02, -8.4945e-02,  6.6916e-02, -4.5706e-02,\n",
       "          1.5521e-02,  3.3268e-01, -2.6228e-01,  4.9539e-02, -3.2127e-01,\n",
       "         -2.4002e-01, -2.2564e-02],\n",
       "        [-1.4182e+00, -5.2848e-01,  2.1544e-01,  5.7761e-02, -2.8968e-01,\n",
       "         -4.4298e-01,  2.2780e-01,  1.0227e+00,  1.0928e-03, -8.8566e-01,\n",
       "          2.2792e-01,  3.1789e-01,  2.5596e-01,  5.6435e-01, -1.1300e+00,\n",
       "          1.4769e-01,  4.3240e-01,  9.0580e-02,  1.0102e-01,  9.2700e-01,\n",
       "          6.0826e-01, -7.2635e-01,  1.1025e-01,  2.6847e-01,  1.0599e-01,\n",
       "         -3.7137e-01,  1.8932e-01],\n",
       "        [ 5.7060e-01, -1.7362e+00, -2.6351e-01,  1.7233e-01,  5.5306e-01,\n",
       "         -3.1490e-01,  8.1408e-02,  7.1444e-01, -5.3695e-01, -4.7164e-01,\n",
       "         -1.9672e-01, -5.0022e-02,  7.4104e-01, -1.6689e-01,  2.5424e-01,\n",
       "         -5.6374e-01,  8.2319e-02,  8.8798e-02, -9.0891e-02,  3.4150e-01,\n",
       "          9.0042e-01, -6.9806e-01,  7.1754e-01, -1.3380e-01,  4.6052e-01,\n",
       "         -5.3097e-01,  6.4222e-02],\n",
       "        [-1.5738e+00, -9.9962e-01,  3.4806e-01, -2.9209e-03,  6.6666e-01,\n",
       "         -1.6808e+00,  2.9527e-01, -5.1862e-01, -1.0212e+00, -8.4322e-02,\n",
       "          2.3805e-01, -1.3664e-01,  1.5911e+00,  6.9500e-01, -6.5633e-02,\n",
       "         -8.4643e-01,  2.2088e-01,  6.5851e-02,  2.0811e-01,  3.7349e-01,\n",
       "          3.4916e-01, -1.9973e-01,  1.1860e+00, -1.8250e-01,  3.5585e-01,\n",
       "          6.4390e-03,  7.0905e-01],\n",
       "        [-4.1692e-01,  1.4388e+00, -3.3376e-01,  5.3360e-01,  3.2971e-01,\n",
       "          9.1392e-01,  9.2944e-02,  5.4589e-01, -2.4789e+00,  8.0615e-01,\n",
       "          2.3086e-01,  3.7096e-01,  2.1906e-01, -3.8672e-01, -1.5947e-01,\n",
       "          2.5050e-01, -3.0820e-01,  3.2308e-02, -2.7905e-02, -1.1547e-01,\n",
       "         -3.8980e-01, -1.1288e+00, -3.9000e-01, -2.8457e-01,  2.7306e-01,\n",
       "          5.6846e-01, -1.5409e-01],\n",
       "        [ 1.2683e-01,  8.2537e-01, -5.4117e-01,  4.2128e-01, -3.0385e-01,\n",
       "          3.3493e-01, -4.3867e-01,  5.7719e-01,  1.7516e+00,  7.9495e-01,\n",
       "         -3.3170e-01,  1.2047e-01, -1.2967e+00, -8.5866e-01, -5.0338e-01,\n",
       "          7.6503e-01, -2.4390e-01, -2.5275e-02, -2.0938e+00, -1.0712e-01,\n",
       "          5.3452e-01,  6.6211e-01, -3.1171e-01, -1.4130e-01, -3.0179e-01,\n",
       "          5.9677e-01,  1.8608e-02],\n",
       "        [-8.2665e-01, -1.0818e+00,  4.2256e-01,  4.2595e-01,  2.5728e-01,\n",
       "         -5.0049e-01, -4.1570e-02,  5.4741e-01,  1.7645e+00, -9.4692e-01,\n",
       "          5.6636e-01,  1.2797e+00,  4.8237e-02, -2.0876e-01, -1.5555e+00,\n",
       "         -1.3432e+00,  1.2932e-01,  2.5220e-01,  8.6058e-01,  5.3273e-01,\n",
       "          4.5912e-01, -1.3182e+00, -9.6212e-02,  5.2580e-01,  6.8053e-02,\n",
       "         -3.6335e-01,  1.3665e-01],\n",
       "        [-6.2797e-01, -4.8052e-01,  4.4962e-01,  7.8924e-01,  9.8635e-01,\n",
       "         -1.1361e+00,  3.6009e-01,  1.5582e+00, -2.0632e-01, -1.4023e+00,\n",
       "          9.6529e-01,  1.6026e-01, -1.3740e+00,  5.0718e-02,  2.1885e+00,\n",
       "         -1.0623e+00,  4.5185e-01,  1.6604e-01, -1.9001e-01,  3.6444e-01,\n",
       "         -5.3317e-01, -6.5193e-01,  7.5330e-02, -2.4324e-03,  1.8132e-01,\n",
       "         -1.4958e+00,  3.4201e-01],\n",
       "        [-8.4539e-01,  1.0467e+00, -2.1670e-01,  6.1583e-01, -6.5354e-01,\n",
       "          6.4002e-01,  4.0800e-03,  1.7912e-01,  3.8379e-01, -5.1193e-01,\n",
       "          1.3272e-01,  7.7043e-01, -1.1294e+00, -1.7509e-01, -1.2977e+00,\n",
       "          1.2490e+00,  6.2054e-02,  6.8839e-02, -6.7245e-01,  4.8584e-01,\n",
       "          2.5616e-01, -1.2511e-01, -5.7968e-01,  1.0401e-01, -1.1627e-01,\n",
       "          6.9283e-02,  2.4046e-01],\n",
       "        [ 1.2858e+00, -6.3629e-01,  5.1771e-02, -7.8076e-02,  5.7688e-01,\n",
       "         -7.0794e-01,  6.4065e-02,  2.6419e-01, -3.6080e-01, -9.4569e-02,\n",
       "         -7.1058e-02, -7.9417e-03,  1.2734e+00,  1.6393e-01, -6.4990e-02,\n",
       "         -1.0122e+00, -3.9434e-02, -7.3347e-02,  5.7301e-01,  4.3371e-02,\n",
       "          3.6157e-01, -7.5813e-01,  5.3017e-02, -3.9133e-01, -1.1304e-01,\n",
       "         -1.7322e-01, -1.3510e-01],\n",
       "        [-3.0205e-01,  2.3238e-01,  1.4347e-01,  1.4248e-01,  2.0585e-01,\n",
       "         -4.8317e-01,  6.2025e-02,  1.5009e-01,  8.2319e-01, -6.7169e-01,\n",
       "          1.8599e-01,  2.3029e-01, -2.2179e-01, -1.4508e-01,  1.1585e-01,\n",
       "          2.5694e-01,  1.2024e-01,  9.2006e-02, -1.6536e-01,  2.9013e-01,\n",
       "         -8.9362e-03, -1.6297e-01,  8.1683e-02, -1.1781e-01,  7.3726e-02,\n",
       "         -1.0311e+00,  1.8757e-01],\n",
       "        [-1.5919e-01, -2.1628e-01, -2.5310e-01,  1.3967e-01, -1.9747e-01,\n",
       "         -5.7889e-01,  2.0377e-01,  2.8164e-01,  6.6355e-01, -2.0777e+00,\n",
       "         -4.2745e-01,  1.1477e-01,  8.5750e-01, -2.5448e-01,  5.7208e-01,\n",
       "          1.1130e-01,  2.4945e-01,  4.2859e-02,  6.2617e-01,  3.1480e-01,\n",
       "          2.3993e-01,  3.5521e-01,  2.2688e-01, -2.6521e-01, -1.4697e-02,\n",
       "         -7.3412e-01,  1.9482e-01],\n",
       "        [-3.7543e-01, -1.2659e-01,  1.1850e-01, -6.0869e-02, -6.1587e-03,\n",
       "         -8.2255e-02, -5.5529e-02, -1.2564e-01,  3.2592e-01,  5.1515e-01,\n",
       "         -2.5382e-01,  1.2145e-01, -2.5787e-01,  2.5712e-01, -1.6269e-01,\n",
       "         -2.1008e-01,  2.0759e-02,  6.8523e-02,  6.2052e-01, -4.6697e-01,\n",
       "         -6.1517e-01,  5.6924e-01,  1.4022e-01,  1.8150e-01, -2.9486e-02,\n",
       "          8.3024e-02, -1.1654e-01],\n",
       "        [ 5.7518e-02, -8.9874e-01,  5.4581e-01,  1.1968e-01,  8.5976e-01,\n",
       "         -1.7942e+00,  4.3336e-01,  4.2498e-01, -8.5398e-01, -9.8643e-01,\n",
       "          5.4676e-01,  6.8328e-01,  4.5884e-01,  1.0842e+00,  2.4764e-01,\n",
       "         -1.0328e+00,  4.4029e-01,  1.2662e-01, -1.7222e+00,  1.1552e+00,\n",
       "          1.5105e+00, -8.8497e-01,  6.9506e-01, -5.5231e-01,  4.7974e-02,\n",
       "         -1.5855e+00,  9.1665e-01],\n",
       "        [ 4.3692e-01, -9.1339e-01,  2.3047e-01,  2.9948e-01,  8.3710e-01,\n",
       "         -8.3461e-01,  1.2144e-01,  8.5776e-01, -1.0110e+00, -4.7661e-01,\n",
       "          1.8294e-01, -1.3011e-01, -1.4474e-01,  2.9539e-01,  1.6328e+00,\n",
       "         -1.0317e+00, -3.6409e-01,  1.3977e-01, -6.0606e-01, -6.1914e-02,\n",
       "         -2.3154e-01, -8.8766e-01,  5.6664e-01, -3.5034e-01,  4.9339e-01,\n",
       "          6.5818e-01,  3.3487e-01],\n",
       "        [ 1.6992e-01, -1.2372e+00,  5.4442e-02,  5.1076e-01, -4.3051e-03,\n",
       "         -9.0791e-01,  1.1932e-01,  1.6927e-01, -9.5268e-01, -2.7284e-01,\n",
       "         -2.0496e-01,  3.3966e-01, -6.2736e-02,  3.7826e-01,  1.6572e+00,\n",
       "         -1.1048e+00,  2.1521e-01,  1.6071e-01,  4.0288e-01,  7.7564e-01,\n",
       "          5.0109e-02, -5.1825e-01,  2.8286e-01,  1.8529e-01,  6.6502e-01,\n",
       "         -9.7631e-01,  1.6055e-01],\n",
       "        [ 2.8304e-01,  3.5299e-01, -6.0526e-01,  1.1946e-01,  6.5650e-01,\n",
       "          4.6688e-01, -1.9454e-01,  4.7130e-01, -1.9237e+00, -2.8128e-01,\n",
       "         -5.7398e-01, -5.1852e-01,  3.8263e-01, -1.9941e-01,  1.8429e-01,\n",
       "         -6.1622e-03, -2.7876e-01, -8.9716e-02, -2.3847e-02, -8.1500e-02,\n",
       "         -6.5891e-01,  9.2992e-01,  4.0811e-01, -3.4893e-02,  2.3212e-01,\n",
       "          8.0390e-01,  1.1888e-01],\n",
       "        [ 6.8771e-01, -5.5401e-01,  9.1320e-02, -1.6479e-01,  2.2320e-01,\n",
       "         -5.7124e-01,  7.3884e-02,  8.3812e-02,  1.0139e-01, -4.5169e-01,\n",
       "          3.1204e-02, -1.6298e-01,  9.7731e-02, -1.7829e-02,  8.5858e-01,\n",
       "         -7.6114e-01, -5.4502e-02,  4.2769e-02,  1.8969e-01, -2.9786e-02,\n",
       "         -1.3784e-01, -2.8070e-01,  3.9914e-01, -1.9760e-01,  2.7717e-01,\n",
       "          2.7603e-01,  1.6329e-02],\n",
       "        [ 3.8446e-01,  2.0973e-01, -1.6523e-02, -1.1759e-01,  9.5278e-03,\n",
       "          9.1506e-02, -7.9265e-02, -6.8459e-03,  8.0374e-02, -1.7893e-01,\n",
       "         -2.6560e-01, -1.7778e-02, -8.7136e-02, -4.3557e-02,  9.2730e-02,\n",
       "          1.3011e-01, -1.0242e-01, -1.5032e-02,  5.0160e-01, -2.1973e-01,\n",
       "         -4.5275e-01,  1.3193e-01,  1.8811e-01, -1.7688e-01,  2.7211e-02,\n",
       "         -3.3233e-02,  2.3935e-02],\n",
       "        [-2.5301e-01, -6.9864e-01,  2.7715e-01,  5.2869e-01, -4.8497e-01,\n",
       "         -2.3877e-01,  3.1580e-01,  4.1459e-01,  5.9932e-01, -8.8304e-01,\n",
       "          5.4555e-01,  5.1012e-01, -1.7374e+00, -8.2315e-02,  1.0724e+00,\n",
       "         -1.4969e-01,  5.3867e-01,  1.6908e-01, -3.6604e-01,  1.1141e-01,\n",
       "          1.2755e-01, -4.1359e-01, -1.6687e-01,  4.3295e-01,  5.6635e-01,\n",
       "         -6.8587e-01, -4.1336e-02],\n",
       "        [ 5.8320e-01, -7.9940e-01,  1.7871e-01, -2.9081e-01,  3.7350e-02,\n",
       "         -9.4172e-01,  1.2271e-01, -1.6128e-01,  2.2246e-01, -5.4014e-01,\n",
       "         -9.1659e-02, -5.0379e-01, -1.1343e-02,  2.4416e-01,  7.6994e-01,\n",
       "         -1.0180e+00,  4.5354e-02,  2.4531e-02, -1.2577e-01,  3.7910e-01,\n",
       "         -1.7711e-01,  3.0996e-01,  5.7409e-01,  1.7372e-01,  3.1786e-01,\n",
       "          5.6528e-01,  9.3113e-02],\n",
       "        [-2.0563e+00,  3.9730e-01,  1.4406e-01,  4.8584e-02,  4.3586e-01,\n",
       "          1.0042e+00,  2.1978e-02, -6.9811e-01, -1.6695e-01,  9.9484e-01,\n",
       "          3.9854e-01,  2.3144e-02, -5.1915e-01, -2.0299e-01, -4.8042e-01,\n",
       "          7.9713e-01,  9.5350e-02,  4.6971e-02,  3.7110e-01, -1.0018e+00,\n",
       "         -3.1021e-01,  2.4607e-01, -6.3413e-02,  4.3284e-02, -3.3160e-02,\n",
       "          7.0124e-01, -2.2808e-01],\n",
       "        [ 9.6398e-01,  1.0583e+00,  2.1260e-02, -2.3209e-01,  1.7275e-01,\n",
       "         -3.7121e-01, -5.0916e-02,  5.4953e-02,  5.1545e-01, -1.9990e-01,\n",
       "         -2.4062e-01, -4.1673e-01, -2.7618e-01, -2.7385e-01,  5.2515e-01,\n",
       "          3.6401e-01, -2.1520e-02, -6.8208e-02, -3.4524e-01, -1.0094e-01,\n",
       "         -1.6265e-01,  4.0587e-01, -2.1527e-01, -4.5319e-03, -3.1022e-01,\n",
       "         -7.9670e-01,  5.0138e-02],\n",
       "        [ 2.3668e+00, -6.1874e-01, -3.8701e-01,  7.7306e-02, -8.5779e-02,\n",
       "          7.4124e-03, -2.6403e-01,  1.4181e-01,  5.9475e-02,  2.6670e-01,\n",
       "         -4.2133e-01,  1.0627e-01, -3.2624e-01, -5.5204e-01, -2.0946e-01,\n",
       "          3.2584e-01, -2.8418e-01, -1.7313e-01,  4.0607e-01,  2.3774e-01,\n",
       "          5.2074e-01,  2.2878e-01, -7.9000e-01,  1.9039e-02, -2.2185e-01,\n",
       "          1.2616e-01, -5.6339e-01],\n",
       "        [ 1.3042e+00,  8.0344e-01, -2.2146e-01,  1.5385e-02, -2.9429e-01,\n",
       "         -1.3496e-01, -8.2694e-02, -1.2233e-01, -4.6437e-01, -2.1206e-01,\n",
       "         -2.2105e-01,  4.9502e-03, -4.6269e-01, -2.7569e-01,  5.1908e-01,\n",
       "          3.5455e-01, -1.1740e-01, -6.0729e-02,  2.8675e-02,  7.5655e-02,\n",
       "         -8.3646e-02, -1.3217e-01,  5.2198e-02, -2.6853e-01,  2.0122e-01,\n",
       "         -2.7825e-01,  4.0292e-02],\n",
       "        [-9.1005e-01, -7.5095e-01,  1.8603e-01, -6.5467e-02, -7.9741e-01,\n",
       "          1.6240e-02,  2.9124e-01, -7.7080e-02,  1.1090e+00, -1.3731e+00,\n",
       "          4.1888e-01,  6.3995e-01,  1.0351e+00, -5.6158e-02, -1.9101e-01,\n",
       "         -3.2331e-01,  3.1102e-01,  1.0582e-01,  4.7303e-02,  3.3816e-01,\n",
       "         -1.2891e-01, -5.3003e-01,  1.2383e-02,  2.4648e-01,  2.2317e-01,\n",
       "          4.0425e-01, -1.5796e-01],\n",
       "        [ 1.9368e+00, -4.4903e-01, -1.9130e-01, -6.6292e-02, -1.5223e-01,\n",
       "          6.4275e-01, -1.7863e-01, -6.8052e-02, -4.6557e-01,  5.6915e-01,\n",
       "         -3.9914e-01, -2.4865e-01, -4.9743e-01, -6.0597e-01,  6.6478e-02,\n",
       "          6.5830e-01, -1.9100e-01, -9.5471e-02, -3.4940e-01,  5.2968e-02,\n",
       "          3.8273e-01,  1.7625e-02, -1.2048e-01, -3.2502e-01, -9.7602e-03,\n",
       "          2.4744e-01, -5.8225e-02],\n",
       "        [-1.1844e+00,  6.6408e-01, -1.2001e-01,  2.9440e-01, -1.7293e-01,\n",
       "          6.5323e-02,  2.6794e-02, -4.3466e-02, -1.5559e-01, -1.5382e-01,\n",
       "         -2.0068e-03,  3.9261e-01, -2.9260e-01,  4.1589e-01, -2.4723e-01,\n",
       "          1.5391e-01,  8.8376e-02,  3.9510e-02, -4.6142e-02,  1.2164e-01,\n",
       "         -2.8312e-01, -1.8156e-01,  1.6999e-01,  1.5688e-02,  3.9440e-01,\n",
       "          2.7823e-02, -2.8133e-02],\n",
       "        [-5.4499e-01, -1.1836e+00, -8.6214e-02, -4.1776e-02, -4.5930e-02,\n",
       "         -4.1729e-01,  9.5406e-03, -5.7067e-01,  1.0537e+00,  3.8330e-01,\n",
       "          7.0966e-02, -1.1389e-01, -1.3020e-01,  1.7812e-01,  9.3289e-01,\n",
       "         -6.8657e-01,  7.1770e-02,  8.9240e-02,  5.0511e-01, -4.2968e-01,\n",
       "         -1.2645e-01,  2.4234e-01,  1.7126e-02,  5.3682e-01,  1.3299e-01,\n",
       "          3.0062e-01, -1.2773e-01],\n",
       "        [-2.2526e-01,  4.8130e-01,  4.4901e-01, -7.3721e-01,  4.3793e-01,\n",
       "         -3.3351e-01,  1.0084e-02, -2.7418e-01, -2.3985e-01,  1.0318e-01,\n",
       "         -3.3221e-01, -5.8764e-01,  7.0693e-01,  6.4530e-01,  6.6752e-01,\n",
       "         -6.7422e-01,  7.6779e-02, -1.2694e-01,  1.0034e+00, -4.5322e-01,\n",
       "         -6.6972e-01,  8.0920e-01,  2.3222e-01,  1.9873e-01, -4.3916e-01,\n",
       "         -8.8693e-01,  1.7038e-01]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7f6d2bc2-0b8b-4014-a382-629827d57d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " xore \n",
      "----\n",
      "----\n",
      " barin \n",
      "----\n",
      "----\n",
      " pighaneyaci \n",
      "----\n",
      "----\n",
      " anvyn \n",
      "----\n",
      "----\n",
      " wahiah \n",
      "----\n",
      "----\n",
      " vira \n",
      "----\n",
      "----\n",
      " brynste \n",
      "----\n",
      "----\n",
      " kars \n",
      "----\n",
      "----\n",
      " shai \n",
      "----\n",
      "----\n",
      " keadylo \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "hprev = torch.zeros(1, hidden_size)\n",
    "cprev = torch.zeros(1, hidden_size)\n",
    "num_samples = 10\n",
    "for i in range(num_samples):\n",
    "    sample_ix = sample(hprev, cprev, 0, 15)\n",
    "    txt = ''.join(itos[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d9941-b476-4cd0-9cc2-8c6c89e270f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
