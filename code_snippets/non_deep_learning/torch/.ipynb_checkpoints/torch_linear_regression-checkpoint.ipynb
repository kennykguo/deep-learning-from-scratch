{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'toy_data_two_moon.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m npzfile \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoy_data_two_moon.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m x_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(npzfile[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marr_0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(npzfile[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marr_2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'toy_data_two_moon.npz'"
     ]
    }
   ],
   "source": [
    "npzfile = np.load(\"toy_data_two_moon.npz\")\n",
    "\n",
    "x_train = torch.FloatTensor(npzfile['arr_0'])\n",
    "y_train = torch.FloatTensor(npzfile['arr_2'])\n",
    "x_test = torch.FloatTensor(npzfile['arr_1'])\n",
    "y_test = torch.FloatTensor(npzfile['arr_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m121\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(x_train[:, \u001b[38;5;241m0\u001b[39m], x_train[:, \u001b[38;5;241m1\u001b[39m], marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, c\u001b[38;5;241m=\u001b[39my_train[:,\u001b[38;5;241m0\u001b[39m], s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m122\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(121)\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], marker='o', c=y_train[:,0], s=25, edgecolor='k')\n",
    "plt.title(\"Train data\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(x_test[:, 0], x_test[:, 1], marker='o', c=y_test[:,0], s=25, edgecolor='k')\n",
    "plt.title(\"Test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define our linear model - 2 inputs, 1 output (bias is included in linear layer)\n",
    "linear = nn.Linear(2, 1) \n",
    "# 2. Define our loss function - MSE\n",
    "loss_function = nn.MSELoss()\n",
    "# 3. Create our optimizer - lr = 0.1\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times we iterate over the dataset\n",
    "max_epoch = 100\n",
    "\n",
    "loss_log = [] # keep track of the loss values\n",
    "acc = [] # keep track of the accuracy \n",
    "for epoch in range(max_epoch):\n",
    "    with torch.no_grad():\n",
    "        y_test_hat = linear(x_test)\n",
    "        class_pred = (y_test_hat >= 0).float()\n",
    "        acc.append(float(sum(class_pred == y_test))/ float(y_test.shape[0]))\n",
    "\n",
    "    # Perform a training step\n",
    "    y_train_hat = linear(x_train)\n",
    "    loss = loss_function(y_train_hat, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_log.append(loss.item())\n",
    "    \n",
    "print(\"Accuracy of linear model(GD): %.2f%% \" %(acc[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc)\n",
    "plt.title(\"Model accuracy per iteration\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)\n",
    "plt.title(\"Model loss iteration\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression </h1>\n",
    "We can see that with binary classification our goal is possition the decision boundary such that it seperates the two classes. However by simply using linear regression with binary labels, we are forcing our model to try and position the decision boundary at a fixed distance from each data point, EVEN if it's on the \"correct\" side of the boundary. For example if a data point (label 1) was on the correct side of the boundary (positive side)\n",
    "BUT it was a large distance away from the boundary, the model would be penalised and the boundary would be moved. This is an obvious problem if there is a large spread of values for a given class and can lead to our model to never converge to an optimal solution. <br>\n",
    "\n",
    "<h3>Sigmoid</h3>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\" width=\"300\" align=\"center\">\n",
    "\n",
    "Instead we think about moving the descision boundary *as far as possible* from the positive points on one side and *as far as possible* from the negative points on the other side. To do this we can utilize the Sigmoid function, which maps all real numbers to the range 0 to 1: $$ R \\to [0, 1] $$ <br>\n",
    "By putting the Sigmoid function on the output of our model we can see that to reach the target value 1, the raw model ouput would have to be $+\\infty$ and 0 would be $-\\infty$. The optimal solution would therefore achieve our goal of maximising the distance between the data points and the descision boundary.<br>\n",
    "While this solves one problem, it introduces another, the sigmoid function's gradient (derivative) is very flat for values witha large magnitude. If we were to use the MSE loss to train a model with a Sigmoid on the output, training would not only be slow, but may not progress at all! To see why, consider the case where our model as made a very large incorrect prediction, the gradient in this case would be very very small, the update to our model would therefore also be very small. <br>\n",
    "To convince yourself of this try adding the sigmoid function (torch.sigmoid()) to the output of the above model and train!\n",
    "\n",
    "<h3>Cross-Entropy loss</h3>\n",
    "We therefore introduce a new loss function the <a href=\"https://en.wikipedia.org/wiki/Cross-entropy#Cross-entropy_loss_function_and_logistic_regression\">Cross-Entropy loss</a>:\n",
    "\n",
    "\\begin{align}\n",
    "L(\\hat y, y) &= -y\\ln (\\hat y) - (1 - y)\\ln (1 - \\hat y)\\\\\n",
    "\\\\\n",
    "Where&\\\\\n",
    "y &= Target \\: value\\\\\n",
    "\\hat y &= Predicted \\: value\\\\\n",
    "\\end{align}\n",
    "\n",
    "We obtain this loss from the maximum log-likelihood of a multinomial (or binomial in the binary case) distribution. That is, instead of simply regressing to a value, we our model is now produces a probability distribution for the likelihood that a given input belongs to a given class. For the single output binary logistic case we get a single probability that the input belongs the the possitive (1) class. <br>\n",
    "NOTE that using a Softmax output (rather than Sigmoid) with a Cross-Entropy loss is very common for multiclass classification but can also be used for binary classification (more on this later).\n",
    "\n",
    "<h3>It's still a linear model!</h3>\n",
    "It's important to realise that even though our model now has the non-linear Sigmoid function on it's output, this model is still only linear, an therefore can only produce a linear descision boundary! We've simply changed that way we think about our model and how we train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a Logistic Regression Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our linear model - 2 inputs, 1 output (bias is included in linear layer)\n",
    "logistic_linear = nn.Linear(2, 1) \n",
    "\n",
    "# Define our loss function - Binary Cross entropy With logits\n",
    "# By using the \"with logits\" version Pytorch will assume the outputs given are the RAW\n",
    "# model outputs without the Sigmoid (logits - log odd probabilities, it is the inverse of the sigmoid function!)\n",
    "# Pytorch incorporates the Sigmoid into the loss function for numerical stability!\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create our optimizer - lr = 0.1\n",
    "logistic_optimizer = torch.optim.SGD(logistic_linear.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times we iterate over the dataset\n",
    "max_epoch = 100\n",
    "\n",
    "logistic_loss_log = [] # keep track of the loss values\n",
    "logistic_acc = [] # keep track of the accuracy \n",
    "for epoch in range(max_epoch):\n",
    "    with torch.no_grad():\n",
    "        y_test_hat = logistic_linear(x_test)\n",
    "        \n",
    "        # The descision boundary is at 0.5 (between 0 and 1) AFTER the sigmoid\n",
    "        # The input to the Sigmoid function that gives 0.5 is 0!\n",
    "        # Therefore the descision boundary for the RAW output is at 0!!\n",
    "        class_pred = (y_test_hat > 0).float()\n",
    "        logistic_acc.append(float(sum(class_pred == y_test))/ float(y_test.shape[0]))\n",
    "        \n",
    "    # Perform a training step\n",
    "    y_train_hat = logistic_linear(x_train)\n",
    "    loss = loss_function(y_train_hat, y_train)\n",
    "    \n",
    "    logistic_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    logistic_optimizer.step()\n",
    "\n",
    "    logistic_loss_log.append(loss.item())\n",
    "    \n",
    "print(\"Accuracy of linear model(GD): %.2f%% \" %(logistic_acc[-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logistic_acc)\n",
    "plt.title(\"Model accuracy per iteration\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xlabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logistic_loss_log)\n",
    "plt.title(\"Model loss iteration\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xlabel(\"BCE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
