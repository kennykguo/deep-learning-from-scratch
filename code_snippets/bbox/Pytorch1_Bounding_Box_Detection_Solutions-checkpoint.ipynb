{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader as dataloader\n",
    "import torchvision.models as models\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import copy\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CUB200(Dataset):\n",
    "    def __init__(self, root, image_size, transform, test_train=1, return_masks=False):\n",
    "        \"\"\"\n",
    "        Initalize the dataframe\n",
    "         Gets the path and loads class labels  \n",
    "         Gets the file path and reads the file paths of the images\n",
    "         Gets the file path of the bounding boxes\n",
    "         Use custom test/train split 1/0\n",
    "         Merges on the \"index\" name\n",
    "        \"\"\"\n",
    "        class_list_path = os.path.join(root, \"image_class_labels.txt\")\n",
    "        self.data_df = pd.read_csv(class_list_path, sep=\" \", names=[\"index\", \"class\"])\n",
    "        data_list_path = os.path.join(root, \"images.txt\")\n",
    "        cub200_df = pd.read_csv(data_list_path, sep=\" \", names=[\"index\", \"file_path\"])\n",
    "        bbox_list_path = os.path.join(root, \"bounding_boxes.txt\")\n",
    "        bbox_df = pd.read_csv(bbox_list_path, sep=\" \", names=[\"index\", \"x\", \"y\", \"width\", \"height\"])\n",
    "        split_df = pd.read_csv(\"test_train_split.txt\", sep=\" \", names=[\"index\", \"split\"])\n",
    "\n",
    "        self.data_df = self.data_df.merge(cub200_df, left_on='index', right_on='index')\n",
    "        self.data_df = self.data_df.merge(bbox_df, left_on='index', right_on='index')\n",
    "        self.data_df = self.data_df.merge(split_df, left_on='index', right_on='index')\n",
    "\n",
    "        # Final dataframe\n",
    "        self.data_df = self.data_df[self.data_df.split != test_train]\n",
    "\n",
    "        # self.return_masks = return_masks\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "\n",
    "        self.root = root\n",
    "        self.image_root_dir = os.path.join(self.root, \"images\")\n",
    "        self.mask_root_dir = os.path.join(self.root, \"segmentations\")\n",
    "\n",
    "    def get_bbox_list(self, data, img_size):\n",
    "        bbox_array = [data[\"x\"],\n",
    "                      data[\"y\"],\n",
    "                      data[\"width\"],\n",
    "                      data[\"height\"]]\n",
    "        \n",
    "        # If the box is outside of the image in either direction x or y, adjust width to be the length up to the edge\n",
    "        if (bbox_array[0] + bbox_array[2]) > img_size[1]:\n",
    "            bbox_array[2] = img_size[1] - bbox_array[0]\n",
    "\n",
    "        if (bbox_array[1] + bbox_array[3]) > img_size[0]:\n",
    "            bbox_array[3] = img_size[0] - bbox_array[1]\n",
    "\n",
    "        return [bbox_array]\n",
    "\n",
    "    # Gets the number of output tensors and matches it to 4 coordinate outputs\n",
    "    def get_output_tensors(self, data_out):\n",
    "        if len(data_out[\"bboxes\"]) > 0:\n",
    "            bbox = torch.FloatTensor(data_out[\"bboxes\"][0]) / self.image_size\n",
    "            label = data_out[\"class_labels\"][0]\n",
    "        else:\n",
    "            bbox = torch.zeros(4)\n",
    "            label = -1\n",
    "\n",
    "        return bbox, [label]\n",
    "\n",
    "    # Gets an item from the dataloader\n",
    "    def __getitem__(self, index):\n",
    "        # Gets a single row. Allowing for indexing like a list\n",
    "        data_series = self.data_df.iloc(\"index\")[index]\n",
    "        file_path = data_series[\"file_path\"]\n",
    "        label = data_series[\"class\"]\n",
    "\n",
    "        # Build the file path\n",
    "        img_path = os.path.join(self.image_root_dir, file_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # Pass in the row to the dataframe, and the image's shape\n",
    "        # Gets the bbox dimensions as a list [x, y, width, height]\n",
    "        bbox_array = self.get_bbox_list(data_series, image.shape)\n",
    "\n",
    "        if self.return_masks:\n",
    "            mask_path = os.path.join(self.mask_root_dir, file_path).split(\".jpg\")[0] + \".png\"\n",
    "            mask = cv2.imread(mask_path)\n",
    "\n",
    "            data_out = self.transform(image=image, bboxes=bbox_array, mask=mask, class_labels=[label])\n",
    "            bbox, label = self.get_output_tensors(data_out)\n",
    "            mask = (data_out[\"mask\"][:, :, 0] > 100).long()\n",
    "\n",
    "            return data_out[\"image\"], mask, bbox, label\n",
    "        # We are not using a mask\n",
    "        # Transform the image using our data pipeline, and give its bbox, along with its class label\n",
    "        else:\n",
    "            data_out = self.transform(image=image, bboxes=bbox_array, class_labels=[label])\n",
    "            # Convert the bbox and label to an output tensor, which is returned as an item when dataloader is iterated upon\n",
    "            bbox, label = self.get_output_tensors(data_out)\n",
    "            return data_out[\"image\"], bbox, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "root = \"CUB_200_2011\"\n",
    "\n",
    "image_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_from_checkpoint = True\n",
    "\n",
    "save_dir = '.'\n",
    "\n",
    "model_name = 'ResNet34_CUB'\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Data augmentation pipeline\n",
    "train_transform = A.Compose([A.SmallestMaxSize(max_size=image_size),\n",
    "                             A.RandomCrop(height=image_size, width=image_size),\n",
    "                             A.HorizontalFlip(p=0.5),\n",
    "                             A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "                             A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "                             A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "                             A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225]),\n",
    "                            ToTensorV2()], \n",
    "                            # Specifies parameters for handling bounding boxes\n",
    "                            # COCO -> (x_min, y_min, width, height)\n",
    "                            # min_area to be considered = 0\n",
    "                            # visibility of box = 0\n",
    "                            # Specify corresponding labels\n",
    "                            bbox_params=A.BboxParams(format='coco',\n",
    "                                                     min_area=0, min_visibility=0.0, \n",
    "                                                     label_fields=['class_labels']))\n",
    "\n",
    "transform = A.Compose([A.SmallestMaxSize(max_size=image_size),\n",
    "                       A.RandomCrop(height=image_size, width=image_size),\n",
    "                       A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                   std=[0.229, 0.224, 0.225]),\n",
    "                       ToTensorV2()], \n",
    "                      bbox_params=A.BboxParams(format='coco',\n",
    "                                               min_area=0, min_visibility=0.0, \n",
    "                                               label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# nn.Module class that will return the IoU for a batch of outputs\n",
    "class BboxIOU(nn.Module):\n",
    "    \n",
    "    def xyhw_to_xyxy(self, bbox):\n",
    "        \"\"\"\n",
    "        Converts from (x_min, y_min, width, height) to (x_min, y_min, x_max, y_max) format\n",
    "        \"\"\"\n",
    "        # [(), (), (), ()]\n",
    "        new_bbox = torch.cat((bbox[:, 0:1], \n",
    "                              bbox[:, 1:2],\n",
    "                              bbox[:, 2:3] + bbox[:, 0:1], \n",
    "                              bbox[:, 3:4] + bbox[:, 1:2]), 1)\n",
    "        return new_bbox\n",
    "    \n",
    "    def bb_intersection_over_union(self, pred_xyhw, target_xyhw):\n",
    "        pred_xyxy = self.xyhw_to_xyxy(pred_xyhw) # [(), (), (), ()]\n",
    "        target_xyxy = self.xyhw_to_xyxy(target_xyhw) # [(), (), (), ()]\n",
    "        \n",
    "        # Determine the (x, y) - coordinates of the intersection rectangle\n",
    "        # (x_min, y_min, x_max, y_max)\n",
    "        xA = torch.cat((pred_xyxy[:, 0:1], target_xyxy[:, 0:1]), 1).max(dim=1)[0].unsqueeze(1) # Max of x_min\n",
    "        yA = torch.cat((pred_xyxy[:, 1:2], target_xyxy[:, 1:2]), 1).max(dim=1)[0].unsqueeze(1) # Max of y_min\n",
    "        xB = torch.cat((pred_xyxy[:, 2:3], target_xyxy[:, 2:3]), 1).min(dim=1)[0].unsqueeze(1) # Max of x_max\n",
    "        yB = torch.cat((pred_xyxy[:, 3:4], target_xyxy[:, 3:4]), 1).min(dim=1)[0].unsqueeze(1) # Max of y_max\n",
    "\n",
    "        # Compute the area of intersection rectangle\n",
    "        x_len = F.relu(xB - xA)\n",
    "        y_len = F.relu(yB - yA)\n",
    "        \n",
    "        # Negative area means no overlap\n",
    "        interArea = x_len * y_len\n",
    "\n",
    "#       If you don't have xyhw values, calculate areas like this\n",
    "#       w1 = (pred_xyxy[:, 0:1] - pred_xyxy[:, 2:3]).abs()\n",
    "#       h1 = (pred_xyxy[:, 1:2] - pred_xyxy[:, 3:4]).abs()\n",
    "\n",
    "#       w2 = (target_xyxy[:, 0:1] - target_xyxy[:, 2:3]).abs()\n",
    "#       h2 = (target_xyxy[:, 1:2] - target_xyxy[:, 3:4]).abs()\n",
    "\n",
    "#         area1 = w1 * h1\n",
    "#         area2 = w2 * h2\n",
    "\n",
    "        # Predicted and target box areas\n",
    "        area1 = pred_xyhw[:, 2:3] * pred_xyhw[:, 3:4]\n",
    "        area2 = target_xyhw[:, 2:3] * target_xyhw[:, 3:4]\n",
    "\n",
    "        # Compute the intersection over union by taking the intersection\n",
    "        # area and dividing it by the sum of prediction + ground-truth\n",
    "        iou = interArea / (area1 + area2 - interArea + 1e-5)\n",
    "\n",
    "        # Return the intersection over union value\n",
    "        return iou\n",
    "\n",
    "    def forward(self, predictions, data):\n",
    "        \"\"\"\n",
    "        data: list of data, index 0 is the input image index [0] is the target\n",
    "        predictions: raw output of the model, the first 4 outputs are assumed to be the bounding box values\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_bbox = torch.sigmoid(predictions[:, :4])\n",
    "        target_bbox = data[1].to(pred_bbox.device)\n",
    "        \n",
    "        return self.bb_intersection_over_union(pred_bbox, target_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initalize our datasets # (#, idx, class, file_path, x, y, width, height, split)\n",
    "train_data = CUB200(root, image_size=image_size, transform=train_transform, test_train = 0)\n",
    "test_data = CUB200(root, image_size=image_size, transform=transform, test_train = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_data.data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_data.data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data.dataloader as dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Split 90-10\n",
    "validation_split = 0.9\n",
    "\n",
    "# Total train examples\n",
    "n_train_examples = int(len(train_data) * validation_split)\n",
    "\n",
    "# Total validation examples\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "# Splits them based on values provided\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = dataloader.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = dataloader.DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = dataloader.DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "x, y,z = next(iter(train_loader))\n",
    "x.shape, y.shape, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "class NetworkConstructor(nn.Module):\n",
    "    # Make sure that the correct number of arguments are being passed through\n",
    "    def __init__(self, model, output_size, device, loss_function, batch_size, learning_rate, \n",
    "                 save_dir, eval_metric, train_loader, test_loader, valid_loader, start_from_checkpoint=True):\n",
    "        super(NetworkConstructor, self).__init__()\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.loss_fun = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_loss = []\n",
    "        self.val_acc = []\n",
    "        self.train_acc = []\n",
    "\n",
    "        # Dataloaders\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        # To update the pretrained model's output_size and update its last layer\n",
    "        self.output_size = output_size\n",
    "        self.model = self.change_output(model, output_size=output_size)\n",
    "\n",
    "        # Full filename\n",
    "        self.save_path = os.path.join(save_dir, model_name + \".pt\")\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.eval_metric = eval_metric\n",
    "\n",
    "        # Create save path from save_dir and model_name, we will save and load our checkpoint here\n",
    "        # Create the save directory if it does note exist\n",
    "        if not os.path.isdir(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "        # Loads checkpoint if starting from checkpoint is True\n",
    "        if start_from_checkpoint:\n",
    "            self.load_checkpoint()\n",
    "        else:\n",
    "            # If checkpoint does exist and start_from_checkpoint = False\n",
    "            # Raise an error to prevent accidental overwriting\n",
    "            if os.path.isfile(self.save_path):\n",
    "                raise ValueError(\"Warning Checkpoint exists\")\n",
    "            else:\n",
    "                print(\"Initalized model\")\n",
    "\n",
    "    # Helper function\n",
    "    def __get_layer__(self, num_ftrs, output_size):\n",
    "        layer = nn.Linear(num_ftrs, output_size).to(self.device)\n",
    "        return layer\n",
    "\n",
    "    # Update the output to output output_size classes\n",
    "    def change_output(self, model, output_size):\n",
    "        if output_size > 0:\n",
    "            if hasattr(model, \"fc\"):\n",
    "                num_ftrs = model.fc.in_features\n",
    "                model.fc = self.__get_layer__(num_ftrs, output_size)\n",
    "        return model\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        # Check if checkpoint exists\n",
    "        if os.path.isfile(self.save_path):\n",
    "            # Load Checkpoint\n",
    "            check_point = torch.load(self.save_path)\n",
    "\n",
    "            # Checkpoint is saved as a python dictionary\n",
    "            # Here we unpack the dictionary to get our previous training states\n",
    "            # self.model.load_state_dict(check_point['model_state_dict'])\n",
    "            # self.optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "\n",
    "            # self.start_epoch = check_point['epoch']\n",
    "            # self.best_valid_acc = check_point['best_valid_acc']\n",
    "\n",
    "            # self.train_loss_logger = check_point['train_loss_logger']\n",
    "            # self.train_acc_logger = check_point['train_acc_logger']\n",
    "            # self.val_acc_logger = check_point['val_acc_logger']\n",
    "\n",
    "            print(\"Checkpoint loaded, starting from epoch:\", self.start_epoch)\n",
    "        else:\n",
    "            # Raise Error if it does not exist\n",
    "            raise ValueError(\"Checkpoint Does not exist\")\n",
    "\n",
    "    def save_checkpoint(self, epoch, valid_acc):\n",
    "        self.best_valid_acc = valid_acc\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'best_valid_acc': valid_acc,\n",
    "            'train_loss_logger': self.train_loss_logger,\n",
    "            'train_acc_logger': self.train_acc_logger,\n",
    "            'val_acc_logger': self.val_acc_logger,\n",
    "        }, self.save_path)\n",
    "\n",
    "    # Training\n",
    "    def train_model(self, num_epochs):\n",
    "\n",
    "        # Loop over epoch\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            self.train()\n",
    "            for images, coords, t in enumerate(self.train_loader):\n",
    "                # Forward pass of image through network and get output\n",
    "                logits = self.forward(images.to(self.device))\n",
    "    \n",
    "                # Calculate loss using loss function\n",
    "                loss = self.loss_function(fx, data[1].to(self.device))\n",
    "    \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), 5)\n",
    "                self.optimizer.step()\n",
    "    \n",
    "                # Log the loss for plotting\n",
    "                self.train_loss.append(loss.item())\n",
    "\n",
    "            # Evaluation\n",
    "            train_counter = 0\n",
    "            valid_counter = 0\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(loader):\n",
    "                    # Forward pass of image through network\n",
    "                    fx = self.forward(data[0].to(self.device))\n",
    "                    # Log the cumulative sum of the acc\n",
    "                    train_counter += self.eval_metric(fx, data).sum().item()\n",
    "    \n",
    "            # Log the accuracy from the epoch\n",
    "            if train_test_val == \"train\":\n",
    "                self.train_acc_logger.append(epoch_acc / len(loader.dataset))\n",
    "            elif train_test_val == \"val\":\n",
    "                self.val_acc_logger.append(epoch_acc / len(loader.dataset))\n",
    "\n",
    "            \n",
    "            # Check if the current validation accuracy is greater than the previous best\n",
    "            # If so, then save the model\n",
    "            if valid_acc > self.best_valid_acc:\n",
    "                self.save_checkpoint(epoch, valid_acc)\n",
    "\n",
    "            if self.lr_schedule is not None:\n",
    "                self.lr_schedule.step()\n",
    "\n",
    "    # This function should perform a single evaluation epoch, it WILL NOT be used to train our model\n",
    "    def evaluate_model(self, train_test_val=\"test\"):\n",
    "        epoch_acc = 0\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(loader):\n",
    "                # Forward pass of image through network\n",
    "                fx = self.forward(data[0].to(self.device))\n",
    "\n",
    "                # Log the cumulative sum of the acc\n",
    "                epoch_acc += self.eval_metric(fx, data).sum().cpu().item()\n",
    "\n",
    "        # Log the accuracy from the epoch\n",
    "        if train_test_val == \"train\":\n",
    "            self.train_acc_logger.append(epoch_acc / len(loader.dataset))\n",
    "        elif train_test_val == \"val\":\n",
    "            self.val_acc_logger.append(epoch_acc / len(loader.dataset))\n",
    "\n",
    "        return epoch_acc / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "x = dataloader.DataLoader(train_data_split, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for label, images, coords, in enumerate(x):\n",
    "    print(images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of the ResNet34 Model\n",
    "# resnet = models.resnet34(pretrained=True)\n",
    "resnet = models.resnet34(weights=\"IMAGENET1K_V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loss_fun=nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_trainer = ModelTrainer(model=res_net.to(device), output_size=4, device=device, \n",
    "                             loss_fun=nn.BCEWithLogitsLoss(), batch_size=batch_size, \n",
    "                             learning_rate=learning_rate, save_dir=save_dir, model_name=model_name,\n",
    "                             eval_metric=BboxIOU(), start_from_checkpoint=start_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_trainer.set_data(train_set=train_data_split, test_set=test_data_split, val_set=valid_data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "images, bbox, labels = next(iter(model_trainer.test_loader))\n",
    "out = torchvision.utils.make_grid(images, normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "example_indx = 3\n",
    "ex_img = images[example_indx]\n",
    "\n",
    "# The bounding box is represented in the (x_min, y_min, width, height) format\n",
    "# aka the coordinate of the top left corner of the box and the box height and width\n",
    "\n",
    "# draw_bounding_boxes expects it in the (x_min, y_min, x_max, y_max) formatweights=ResNet18_Weights.IMAGENET1K_V1\n",
    "# aka the coordinates of the top left and bottom right corners of the box\n",
    "ex_label = bbox[example_indx].unsqueeze(0) * image_size\n",
    "ex_label[:, 2] += ex_label[:, 0]\n",
    "ex_label[:, 3] += ex_label[:, 1]\n",
    "\n",
    "img_out = (((ex_img - ex_img.min())/(ex_img.max() - ex_img.min())) * 255).to(torch.uint8)\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_out, ex_label, colors=(0, 255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "out = torchvision.utils.make_grid(img_box.unsqueeze(0).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_trainer.run_training(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The highest validation IoU was %.2f\" %(model_trainer.best_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_loss_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select an image to test\n",
    "example_indx = 4\n",
    "ex_img = images[example_indx]\n",
    "img_out = (((ex_img - ex_img.min())/(ex_img.max() - ex_img.min())) * 255).to(torch.uint8)\n",
    "\n",
    "real_label = bbox[example_indx].unsqueeze(0) * image_size\n",
    "real_label[:, 2] += real_label[:, 0]\n",
    "real_label[:, 3] += real_label[:, 1]\n",
    "\n",
    "# Get the model's prediction for the Bounding Box\n",
    "model_trainer.eval()\n",
    "with torch.no_grad():\n",
    "    pred_out = torch.sigmoid(model_trainer(ex_img.unsqueeze(0).to(device)))\n",
    "    pred_label = (pred_out * image_size).cpu()\n",
    "    pred_label[:, 2] += pred_label[:, 0]\n",
    "    pred_label[:, 3] += pred_label[:, 1]\n",
    "    \n",
    "# Draw the box on the image\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_out, real_label, colors=(0, 255, 0))\n",
    "img_box = torchvision.utils.draw_bounding_boxes(img_box, pred_label, colors=(255, 0, 0))\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "out = torchvision.utils.make_grid(img_box.unsqueeze(0).float(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize = (10,5))\n",
    "train_x = np.linspace(0, num_epochs, len(model_trainer.train_acc_logger))\n",
    "_ = plt.plot(train_x, model_trainer.train_acc_logger, c = \"y\")\n",
    "valid_x = np.linspace(0, num_epochs, len(model_trainer.val_acc_logger))\n",
    "_ = plt.plot(valid_x, model_trainer.val_acc_logger, c = \"k\")\n",
    "\n",
    "_ = plt.title(\"Average IoU\")\n",
    "_ = plt.legend([\"Training IoU\", \"Validation IoU\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_acc = model_trainer.evaluate_model(train_test_val=\"test\")\n",
    "print(\"The Test Average IoU is: %.2f\" %(test_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet18_STL10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (deep-learning)",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
