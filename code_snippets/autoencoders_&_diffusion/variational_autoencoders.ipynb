{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Variational Autoencoder </h1>\n",
    "Traditional Autoencoders form dense representations with not a lot of meaningful \"structure\". This is fine when all you want to do is compress an input and then reconstruct it, but what if you then want to generate new images by sampling the representation space (latent space)? Small movements is the latent space lead to large and discontinuous jumps in the reconstructed output. We need to apply an additional loss to force the created latent space to be smooth and nicely structured.\n",
    "<img src=\"https://miro.medium.com/max/1687/1*22cSCfmktNIwH5m__u2ffA.png\" width=\"1200\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhWb2qkq6Idq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vyfSkLIu6Id3"
   },
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "lr = 1e-4\n",
    "\n",
    "# Number of Training epochs\n",
    "nepoch = 10\n",
    "\n",
    "# The size of the Latent Vector\n",
    "latent_size = 128\n",
    "root = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6195,
     "status": "ok",
     "timestamp": 1570409783041,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "RJUrSrOl6Id-",
    "outputId": "a37cfcb0-da67-4107-fc85-893028c5d2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to .\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 9912422/9912422 [00:07<00:00, 1366809.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to .\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 431779.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:01<00:00, 958188.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 1765899.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define our transform\n",
    "# We'll upsample the images to 32x32 as it's easier to contruct our network\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_set = Datasets.MNIST(root=root, train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batchSize,shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = Datasets.MNIST(root=root, train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batchSize, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QSYKspHT6IeB"
   },
   "source": [
    "## KL Divergence penalty (loss)\n",
    "\n",
    "The KL divergance between two normal distributions where:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x) = N(\\mu_p,\\sigma_p)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "q(x) = N(\\mu_q,\\sigma_q)\n",
    "\\end{equation*}\n",
    "                                                          \n",
    "\\begin{equation*}\n",
    "KL(p,q) = \\ln(\\frac{\\sigma_q}{\\sigma_p}) + \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{2\\sigma_q^2} - \\frac{1}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "With a VAE using the Encoder we produce a $\\sigma$ and a $\\mu$ per dimension and sample from normal distribution (once per dimension) with the given $\\sigma$ and $\\mu$. We then pass this sampled vector to the Decoder which will try and reconstruct the original image.<br>\n",
    "The KL penalty (or loss) tries to force the distribution from the encoder to be that of a unit gaussian where $\\sigma=1$ and $\\mu =0$ (also known as a Standard Normal Distribution).<br>\n",
    "To do this we create a loss using the KL Divergence (a value that is always positive) between the distribution produced by the encoder and that of a unit gaussian.<br>\n",
    "\n",
    "\n",
    "So the above becomes:\n",
    "\\begin{equation*}\n",
    "p(x) = N(\\mu_p,\\sigma_p)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "q(x) = N(0,1)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "KL(p,q) = \\ln(\\frac{1}{\\sigma_p}) + \\frac{\\sigma_p^2 + (\\mu_p - 0)^2}{2*1^2} - \\frac{1}{2}\n",
    "\\end{equation*}\n",
    "\n",
    "Which we can simplify to:\n",
    "\\begin{equation*}\n",
    "KL(p,q) = -\\frac{1}{2}(2\\ln(\\sigma_p) - \\sigma_p^2 - \\mu_p^2 + 1)\n",
    "\\end{equation*}\n",
    "\n",
    "If we minimise this we bring our distribution closer to a unit Gaussian <br><br>\n",
    "Note: $\\sigma$ must always be $\\ge0$, instead of forcing this on our network we usually use $\\ln(\\sigma^2)$ or the \"log variance\" in its place (literally the log of the variance $\\sigma^2$ which simplifies to $2\\ln(\\sigma)$ which we can see in our equation above). This value is continuous in the range of $-\\infty$ to $\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n7ZMUFXx6IeC"
   },
   "source": [
    "## Why do we even bother??\n",
    "There is more then meets the eye with what is actually going in a VAE and what happens when we implement a KL penalty, for a good explanation we won't be able to beat check-out these great write-ups: <br>\n",
    "[Blog: Intuitively Understanding Variational Autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)<br>\n",
    "[Blog: Understanding Variational Autoencoders (VAEs)](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n",
    "\n",
    "What is the ELBO and how does it apply to a VAE? <br>\n",
    "[Youtube: Evidence Lower Bound (ELBO) - CLEARLY EXPLAINED!](https://www.youtube.com/watch?v=IXsA5Rpp25w&ab_channel=KapilSachdeva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxLSEi7e6IeD"
   },
   "outputs": [],
   "source": [
    "def vae_loss(recon, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon, x)\n",
    "    \n",
    "    # Here is our KL divergance loss implemented in code\n",
    "    # We will use the mean across the dimensions instead of the sum (which is common and would require different scaling)\n",
    "    kl_loss = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()\n",
    "    \n",
    "    # We'll tune the \"strength\" of KL divergance loss to get a good result \n",
    "    loss = recon_loss + 0.1 * kl_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgqQSvQg6IeG"
   },
   "source": [
    "## VAE Network\n",
    "The structure is very similar to a vanilla Auto Encoder with the addition of a $\\sigma$ output on the encoder.<br>\n",
    "It is functionally different as we sample from a standard normal distribution and scale it with $\\sigma$ and shift with $\\mu$.<br> This functionally the same as sampling from $N(\\mu,\\sigma)$<br>\n",
    "Note: we only do this during training, during test time we just use $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_out, 3, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels_out)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channels_out, channels_out, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels_out)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels_in, channels_out, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_skip = self.conv3(x)\n",
    "        \n",
    "        x = F.elu(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x) + x_skip\n",
    "        \n",
    "        return F.elu(self.bn2(x))\n",
    "    \n",
    "    \n",
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(channels_in)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_in, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels_in)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channels_in, channels_out, 3, 1, 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels_in, channels_out, 3, 1, 1)\n",
    "        self.up_nn = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = F.elu(self.bn2(x_in))\n",
    "        \n",
    "        x_skip = self.up_nn(self.conv3(x))\n",
    "        \n",
    "        x = self.up_nn(F.elu(self.bn2(self.conv1(x))))\n",
    "        return self.conv2(x) + x_skip\n",
    "\n",
    "    \n",
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, ch=32, z=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(channels, ch, 3, 1, 1)\n",
    "\n",
    "        self.conv_block1 = DownBlock(ch, ch)\n",
    "        self.conv_block2 = DownBlock(ch, ch * 2)\n",
    "        self.conv_block3 = DownBlock(ch * 2, ch * 4)\n",
    "\n",
    "        # Instead of flattening (and then having to unflatten) out our feature map and \n",
    "        # putting it through a linear layer we can just use a conv layer\n",
    "        # where the kernal is the same size as the feature map \n",
    "        # (in practice it's the same thing)\n",
    "        self.conv_mu = nn.Conv2d(4 * ch, z, 4, 1)\n",
    "        self.conv_logvar = nn.Conv2d(4 * ch, z, 4, 1)\n",
    "\n",
    "    # This function will sample from our distribution\n",
    "    def sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.conv_1(x))\n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "\n",
    "        mu = self.conv_mu(x)\n",
    "        logvar = self.conv_logvar(x)\n",
    "        x = self.sample(mu, logvar)\n",
    "        \n",
    "        return x, mu, logvar\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels, ch = 32, z = 32):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(z, 4 * ch, 4, 1)\n",
    "        \n",
    "        self.conv_block1 = UpBlock(4 * ch, 2 * ch)\n",
    "        self.conv_block2 = UpBlock(2 * ch, ch)\n",
    "        self.conv_block3 = UpBlock(ch, ch)\n",
    "        self.conv_out = nn.Conv2d(ch, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = F.elu(self.conv_block3(x))\n",
    "\n",
    "        return torch.tanh(self.conv_out(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-TN2KYN6IeH"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, channel_in, ch=16, z=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(channels=channel_in, ch=ch, z=z)\n",
    "        self.decoder = Decoder(channels=channel_in, ch=ch, z=z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding, mu, logvar = self.encoder(x)\n",
    "        \n",
    "        # Only sample during training or when we want to generate new images\n",
    "        # just use mu otherwise\n",
    "        if self.training:\n",
    "            x = self.decoder(encoding)\n",
    "        else:\n",
    "            x = self.decoder(mu)\n",
    "            \n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6489,
     "status": "ok",
     "timestamp": 1570409783350,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "74l6KlI06IeK",
    "outputId": "8d7a863d-8de8-4ae0-c4c1-403ddb67eae5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a test image\n",
    "dataiter = iter(test_loader)\n",
    "test_images = next(dataiter)[0]\n",
    "\n",
    "# View the shape\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7133,
     "status": "ok",
     "timestamp": 1570409784001,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "GBE2TPmy6IeN",
    "outputId": "8344a523-4b6a-4adb-ecdd-c59fd617b137"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21dd3822890>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABj0AAAD6CAYAAAD6OoWmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6SUlEQVR4nO3deXBVZZ74/8/NdkkguSRkZw2QoBAIW9hXkSgqCrj16NA4dtvSKjWM7dijzIwZywGlZiytchmZdgQXGnVwQQeBqBC1kUUgrLKELQESwhKTELKQ5Pz+6B/5Gp/PCfeGhOSevF9VVLXvfEge6JuTc/Nw87gsy7IEAAAAAAAAAADAzwW09gIAAAAAAAAAAACaA5seAAAAAAAAAADAEdj0AAAAAAAAAAAAjsCmBwAAAAAAAAAAcAQ2PQAAAAAAAAAAgCOw6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIbHoAAAAAAAAAAABHYNMDAAAAAAAAAAA4ApseAAAAAAAAAADAEVps0+O1116TpKQk6dChgwwbNky+/fbblvpQAAAAAAAAAAAAEtQS7/T999+X+fPny2uvvSZjx46VN954Q6ZNmyb79u2THj16NPp76+rq5NSpUxIeHi4ul6sllgcAAAAAAAAAAPyEZVlSVlYmiYmJEhDQ+Gs5XJZlWc29gJEjR8rQoUPl9ddfr2/XX3+9zJgxQxYtWtTo7z1x4oR07969uZcEAAAAAAAAAAD8WH5+vnTr1q3RmWb/8VbV1dWybds2ycjIaNAzMjJk48aNxnxVVZWUlpbW/2qBPRgAAAAAAAAAAODnwsPDrzjT7JseZ8+eldraWomLi2vQ4+LipLCw0JhftGiReDye+l9X+vFXAAAAAAAAAACg/fHmSIwWO8j8lx/csix1QU899ZSUlJTU/8rPz2+pJQEAAAAAAAAAAAdr9oPMo6OjJTAw0HhVR1FRkfHqDxERt9stbre7uZcBAAAAAAAAAADamWZ/pUdISIgMGzZMsrKyGvSsrCwZM2ZMc384AAAAAAAAAAAAEWmBV3qIiDz++OMye/ZsGT58uIwePVqWLFkieXl5Mnfu3Jb4cAAAAAAAAAAAAC2z6XHvvffKuXPn5Nlnn5WCggJJTU2V1atXS8+ePVviwwEAAAAAAAAAAIjLsiyrtRfxc6WlpeLxeFp7GQAAAAAAAAAAoA0pKSmRiIiIRmea/UwPAAAAAAAAAACA1sCmBwAAAAAAAAAAcAQ2PQAAAAAAAAAAgCOw6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIbHoAAAAAAAAAAABHYNMDAAAAAAAAAAA4ApseAAAAAAAAAADAEdj0AAAAAAAAAAAAjsCmBwAAAAAAAAAAcAQ2PQAAAAAAAAAAgCOw6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIbHoAAAAAAAAAAABHYNMDAAAAAAAAAAA4ApseAAAAAAAAAADAEdj0AAAAAAAAAAAAjsCmBwAAAAAAAAAAcISg1l4AADRF37591d6rVy+jhYWFqbNdu3Y1WkpKik/r2Llzp9EOHz6szh44cEDtRUVFPn1MAAAAAAAAADpe6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIbHoAAAAAAAAAAABHYNMDAAAAAAAAAAA4ApseAAAAAAAAAADAEYJaewEA0JihQ4eqfebMmWpPS0szWseOHdXZrl27Gi0lJcWH1Ynk5OQYbe/everse++9p/avv/7aaNXV1T6tA2hLwsLCjDZu3Dh1NiIiQu0//PCD0Y4dO3ZV6wJ+zuPxGC09PV2dve6664xWUlKizn7//fdqP378uNEuXbrU2BIBAAAAAE3AKz0AAAAAAAAAAIAjsOkBAAAAAAAAAAAcgU0PAAAAAAAAAADgCGx6AAAAAAAAAAAAR2DTAwAAAAAAAAAAOEJQay8AAC7r37+/0R566CF1dvr06WpPSEgwWk1NjTpbVlZmtMOHD6uzoaGhah8wYIDRevbsqc7m5eWpff/+/UY7duyYOgv4g8TERKM98MAD6mxMTIzaS0pKjMbnBZoiODhY7b179zbak08+qc5OmDDBaKdPn1ZnX3jhBbWvXLnS6/cBNKZDhw5q79Gjh9Hs7l927tzZrGtqTiEhIWrv1KmT2svLy41WVVXVrGsCvNW5c2ejpaSkqLNBQfq3Y3Jzc41WVFR0VeuC/7F7fPTq1ctoHo9HnT179qzaCwsLjeak62ZgYKDatc9P7WuIiEhlZWVzLglAK+CVHgAAAAAAAAAAwBHY9AAAAAAAAAAAAI7ApgcAAAAAAAAAAHAENj0AAAAAAAAAAIAjcJA5gGvO7gDOefPmGW3mzJnqbGRkpNq1Q2Hz8/PV2UOHDhnN7mBP7WBmEZE777zTaPHx8ers4MGD1d6vXz+jcWAz/Jl2mK7d52xxcbHatQMWgca4XC61R0VFqX348OFGmzx5stcfr1u3bmpPS0tT+3fffWc0DjJHU3Tt2lXtDzzwgNFiYmLU2Yceeqg5l9SsoqOj1Z6enq72EydOGC0nJ0edra2tbfK6gJ8LCND//ejo0aON9sQTT6iz4eHhan/llVeM9u6776qzdXV1dkuEn7B7LGnPEUVEHnzwQaP16dNHnV27dq3aP/zwQ6P540HmwcHBak9NTVW79nx806ZN6qz2vQIRkZqaGu8Wh1YXFhamdrvnBtrj6fjx4+os117/wCs9AAAAAAAAAACAI7DpAQAAAAAAAAAAHIFNDwAAAAAAAAAA4AhsegAAAAAAAAAAAEdg0wMAAAAAAAAAADhCUGsvwOmCg4O97pZl+fS+a2pqjFZbW6vOau/b148H+CogQN9Xvf7669U+YcIEo0VFRamz+fn5al+6dKnRlixZos6WlJQYrbKyUp3t1KmT2rt37260KVOmqLO+XA8Af+ByudSelJRkNLvPoe3bt6t99+7dTV8YHC8wMNBoERER6uz48ePVPm/evGZd02Xa2kREQkJCjGZ3/be7n6urq2v6wuAYffv2VXtKSorRysrKWno5zc7u3u+mm25Se8eOHY32yCOPqLPl5eVNXxjwMx6PR+3333+/0UaMGKHOhoaGqv2OO+4w2ooVK9TZ6upquyXCT0RHR6s9MzNT7VOnTjXa0aNH1dmgIP1bfmfPnvVucW2Idn+lPRcX0b8nICISExNjtAULFqizJ0+eVHtpaanNCnEt2H2PSXuumZ6ers4+8MADao+NjTXar371K3W2uLjYZoVoS3ilBwAAAAAAAAAAcAQ2PQAAAAAAAAAAgCOw6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIPm96fPPNNzJ9+nRJTEwUl8sln3zySYO3W5YlmZmZkpiYKKGhoTJp0iTZu3dvc60XAAAAAAAAAABAFeTrbygvL5e0tDT5u7/7O7nzzjuNty9evFhefPFFWbp0qaSkpMhzzz0nU6dOlQMHDkh4eHizLLotioiIUHtGRoba77rrLqOVlpaqsxcvXlT79u3bjZaTk6POFhYWGq2oqEidBZpLQIC+rxobG6v24OBgoxUUFKizr776qtrffvtto507d06dtSzLaG63W52dM2eO2ocMGWK0jh07qrO5ublqP3TokNqBti4pKUnt06dPN1pNTY06u2vXrmZdE9qHESNGGG3GjBnq7KRJk9SekpLSjCv6f/r27av222+/3Wg9e/ZUZ/fs2aP2AwcONH1h8Dt291HJyclq1x57O3bsaNY1XQt2z4mOHz+udu1rTlhYmDpr97xKuycEGjNw4EC19+7d22h2j0e75wabN282WnV1tQ+rQ1sVGRlptH/9139VZ8eMGaN27R7h9ddfV2c//PBDH1bXtsXExBjt97//vTrbp08ftb/yyitGy87OVmftvhahddlde2fPnm20WbNmqbMJCQlqLysrM5rH41FntcdHbW2tOovW4/Omx7Rp02TatGnq2yzLkpdeekkWLFhQ/+BatmyZxMXFyfLly+Xhhx++utUCAAAAAAAAAADYaNYzPY4ePSqFhYUNXt3gdrtl4sSJsnHjRvX3VFVVSWlpaYNfAAAAAAAAAAAAvmrWTY/LP0IpLi6uQY+Li1N/vJKIyKJFi8Tj8dT/6t69e3MuCQAAAAAAAAAAtBPNuulxmcvlavDflmUZ7bKnnnpKSkpK6n/l5+e3xJIAAAAAAAAAAIDD+XymR2Pi4+NF5K+v+Pj5wTBFRUXGqz8uc7vdtgcH+xO7g5n/8Ic/qL1fv35GsztAr66uTu0zZ840mt2PBztz5ozR/HWDSTsM1+6wa7uDu/bv32+0qqqqq1sYDHYHF9v9uLsHH3zQaHafF0eOHFH7+fPnvX4fmqAg/bJ4yy23qD06OtpogYGB6qzdwVYceAV/1bVrV7VrBw3u27dPnbU7sBlozM9/lOplc+bMUWc7duyodrtr9dUaMmSI2rXDp8+ePavOrl+/Xu0vvfSS0Y4dO+b12uBftIPJRUTS09PVrh10vGXLlmZd07Vgdy+mHQAsoh9Ya/d5r90ninAvBnvBwcFqv/vuu9WelJRkNLt/AGr3fOb777/3cnVoqwIC9H9frH0faOzYseqs9jxTROSdd94x2ldffaXOal8X2rqwsDC1Dx482Gj333+/Ort161a1r1q1yminTp3yfnG4ZiIiItQ+Y8YMtWvXZLsDy+2eA3Tq1Mlo//AP/6DOvvLKK0Y7evSoOmv3fTG0vGZ9pUdSUpLEx8dLVlZWfauurpbs7GwZM2ZMc34oAAAAAAAAAACABnx+pceFCxckNze3/r+PHj0qOTk5EhUVJT169JD58+fLwoULJTk5WZKTk2XhwoUSFhYm9913X7MuHAAAAAAAAAAA4Od83vT44YcfZPLkyfX//fjjj4vIX3+kwNKlS+XJJ5+UiooKeeSRR6S4uFhGjhwp69atk/Dw8OZbNQAAAAAAAAAAwC/4vOkxadKkRn8+vsvlkszMTMnMzLyadQEAAAAAAAAAAPikWc/0AAAAAAAAAAAAaC0+v9IDupKSErV//PHHak9OTjbamTNn1FmPx6P2bt26Ga1Xr17q7IABA7xqIvZ/lsjISKMFBPi2b1ZXV2e0qqoqdba2tlbtYWFhRistLVVnL1y4oPaTJ096vQ40v7KyMrVv3brVaHavLKupqVG79hizExERYbTbbrtNnR04cKDaQ0NDjXbq1Cl19uDBg2q3+9wH2rpp06apXfvcsnv85+fnN+ua4Cz33HOP2m+66SajxcTEqLONvUL5aly8eFHtdvcT2udFly5d1FntnktEvwf63e9+Z7dE+JH4+HijPfjgg+psWlqa2jdu3Gi0NWvWXN3CWkFUVJTa7f7cbrfbaL4+RwFE/vpTK36pZ8+e6mx6erratcev3deh48ePq33Xrl12S0QbY3etsfv6PmnSJKPFxsaqs+vWrVN7VlaW0fzx+WSnTp3UPmHCBLVf/tH6P9e5c2d1dv369Wo/cuSI0SorK21WiNY0btw4tY8dO1btCQkJRgsK0r/lbXdNDg4ONtqsWbPU2T59+hjtiy++UGfXrl2r9sOHD3u9NjQNd4MAAAAAAAAAAMAR2PQAAAAAAAAAAACOwKYHAAAAAAAAAABwBDY9AAAAAAAAAACAI7DpAQAAAAAAAAAAHEE/yh4+++mnn9S+YsUKtUdFRRmttLRUnQ0NDfX6fXTt2lWdTUpKMlqXLl3U2ePHj6s9OTnZaIGBgeqsndraWqMVFxers/Hx8WqfPXu20Tp27KjO2v3dBQSw39cWVVVVtcj7DQ8PV/uoUaOMNnfuXHU2JiZG7ZWVlUb78ssv1dktW7aovaysTO1AW9G5c2e1jx49Wu2dOnUy2vnz59XZCxcuNHldcI5u3bqpffLkyWrv3bt3i6xDu08REfnhhx+Mtm/fPnXW7rEeFxdntPT0dHW2X79+ap8wYYLRRowYoc5u27ZN7XZ/RrSuqVOnGu2GG25QZ48dO6b2VatWGS0vL++q1tXSgoLMp6OJiYnq7JAhQ9Tucrm8asCVaI9Hu3udhIQEtQcHBxutsLBQnT169KjaS0pK7JaINsbuHvnee+9V+1133WW06upqdfbzzz9X+86dO43mj1/b7Z5fa8/RRUQGDx5stE2bNqmzn332mdrt7tHQurTv19x4443q7IABA9SufV/G7v/vuro6tffs2dNodvck2uPX7jF97tw5tZ84ccJo2p8DTcd3fgEAAAAAAAAAgCOw6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIbHoAAAAAAAAAAABH4CDzZnLp0iW12x0e2FKHCtodLB4REeFVExEpKipSu3aAj68HgmsHbGkHxomI3HTTTWqvqakxmt0BRXaHeF68eNFuiXCgyMhItWsHE9odnGb3ubV3716jrV69Wp3dv3+/2u0O0gJag/ZYtztU2e5gN+0xbXcoG4//9kc76P6ee+5RZ+0O+tYOPLQsS521u0crLi42Wm5urjr75ptvGm3Pnj3qbGlpqdq1Q2/tDi/t1auX2rt06WK0uXPnqrNLlixR+65du4zGfVHzsztIOykpSe3Tp083WnR0tDr7v//7v2rXDnRt64fbdu3a1WjDhg1TZ+3u5w4dOmQ0u685dtcJQER/XjpmzBh1VvtaJqLf19g9J92+fbsPq0NbZPc4GD9+vNpTU1ONZnc/oV3bRPT7l7bC7vtD2gHRGRkZ6qzd311ZWZnRXn31VXXW7u/U7p4Qratbt25G0z5XRETcbrfas7OzjbZ+/Xp1tkOHDmqfMmWK0QYNGqTOejweo9mt+dZbb1W7dpC5di8non8fFFfGKz0AAAAAAAAAAIAjsOkBAAAAAAAAAAAcgU0PAAAAAAAAAADgCGx6AAAAAAAAAAAAR2DTAwAAAAAAAAAAOEJQay8Azau2tlbtxcXFXrXGHD58uElr+rnAwECjjRo1Sp0dN26c2mtqaoy2YcMGdfb7779Xe3l5uc0K4e/i4uKMNnbsWHV2xIgRRnO5XOrsyZMn1f7pp58abcuWLepsSUmJ2oG2RLtOp6WlqbNhYWFqP3DggNGKioqubmHwO3bX0y5duhjt7rvvVmcHDBig9uDgYKNVVlaqs7m5uWrPzs422qZNm9TZVatWGc3Xe4lTp04ZrXfv3urs6NGj1d6vXz+j3XnnnepsUJB+m79w4UKj2f0dafdc8I72GBURycjIUPvQoUONdvr0aXV2//79av/pp5+8W1wrsPv7GDJkiNFuuOEGddbuz7du3Tqj2d1z1dXV2awQ7UlAgP5vP6Ojo402bNgwdTY0NFTt2nVz69at6uy2bdvslgg/YVnWVXe3263OXnfddWqvqKgw2vnz59VZu361OnfurPaYmBi1T5kyxWi33XabOtu3b1+1a59HK1eutFkh/ElCQoLRIiIi1Fm7e9bly5cbbcWKFeqsx+NR+65du4y2YMECdXbQoEFGs3tubPe9zT179hht+/bt6iz35E3DKz0AAAAAAAAAAIAjsOkBAAAAAAAAAAAcgU0PAAAAAAAAAADgCGx6AAAAAAAAAAAAR2DTAwAAAAAAAAAAOEJQay8A7UuXLl2MduONN6qz06ZNU/vp06eN9uc//1mdLS4uVrtlWXZLhJ8ICND3bMePH2+0f/zHf1Rnr7/+eqNVVFSosytXrlT7u+++a7T8/Hx1FmhLXC6X2sPDw42Wlpamztp9Hn755ZdG27dvnw+rg5MFBgYarVOnTupsUJB+q3rp0iWjHTx4UJ3VrtMiIkuWLDHahQsX1NnmUF5ebrTNmzers2vXrlV7r169jGb3d/c3f/M3av/ggw+MlpeXp87W1NSoHVfmdrvVPmnSJLVHRUUZ7aOPPlJn/fF6GhcXp/bhw4cbbcCAAeqs3eP0nXfeMdrFixd9WB3am7CwMLUPHTrUaNrzBRGRDh06qL2goMBox48fV2fPnz9vt0T4Cbuvk2fPnlV7dXW10eweY/Pnz1d7UVGR0fbs2aPO5uTkqP1q9e/fX+0DBw5Uu3Zd79y5szp7+PBhtX/11VfeLQ5+R7sHCgkJUWft7oEOHTpkNLvv+/30009qX79+vdHmzp2rztbW1nr98ezuScrKyozGvXfz4pUeAAAAAAAAAADAEdj0AAAAAAAAAAAAjsCmBwAAAAAAAAAAcAQ2PQAAAAAAAAAAgCNwkDlahN3htqNHjzaadvC0iP3hpadOnTLakSNH1Nm6ujq7JcLPRUZGqr1fv35G69OnjzqrHeR88uRJdfbNN99UuzbP4w7+wO4Qz/T0dKPdfvvt6qzdIY3btm0zWmFhoQ+rAxqnHQq7YsUKdXbZsmVqb8lDy7114sQJtW/atEntGRkZRrM79NmOdsC2drg8vKfd93o8HnV20KBBag8ODjbat99+q87m5ub6sLq2YfDgwWofOXKk1+/jwIEDat+xY0dTloR2LDQ0VO2pqalGs3tea3dg7YYNG4z2448/er84+JXi4mK1v/fee2ofNWqU0Xr16qXOJiYmqj0pKclo2vdZRPTDlkWu/rBku9+v3WOI6N/bsfu7s7sHevfdd71cHfzNiBEjjBYTE6PO9u3bV+0pKSlG2759uzprd98bHh5utOuvv16dtTtoXVNUVKT2goICo126dMnr94sr45UeAAAAAAAAAADAEdj0AAAAAAAAAAAAjsCmBwAAAAAAAAAAcAQ2PQAAAAAAAAAAgCOw6QEAAAAAAAAAABwhqLUXAGfq06eP2mfOnGm0sWPHqrPbt29X+9NPP220w4cPq7N1dXV2S4Sfu+eee9T+t3/7t0br2LGjOnvu3DmjffbZZ+rskSNH1F5dXW23RKBNCwsLU/uoUaOM1qVLF3V27969ai8uLjZabW2tD6tDexMQoP87HLv+P//zP0Z7+eWX1dmqqqqmL6yVuFwutWt/H3Z/R3buv/9+o9ndR+Xk5Pj0vtsr7f8vt9utzsbGxqp948aNRisoKLi6hbUhUVFRao+MjDTahQsX1Nnjx48365rQfnXo0EHtAwYMMFpgYKA6W15ervavv/7aaPv37/dhdfAnlZWVat+6davaZ8yYYbTrrrtOndXuyUVEZs2aZbR+/fqps3aPvW+//Vbt3lq7dq3a//jHP6p9/PjxRjtz5ow6u3v3brUXFhZ6uTr4m+zsbKNNnjxZnU1MTFR7//79jTZ48GB1NjU1Ve1TpkwxWlxcnDqrPb+wu38fMmSI2rU/4+bNm9XZU6dOqR2N45UeAAAAAAAAAADAEdj0AAAAAAAAAAAAjsCmBwAAAAAAAAAAcAQ2PQAAAAAAAAAAgCOw6QEAAAAAAAAAABwhqLUXAGcaOHCg2nv37m204uJidXbz5s1q37lzp9Hq6up8WB38yYABA9Q+evRotffo0cNoJSUl6mx2drbRXn/9dXW2srLSbomAX+rQoYPar7vuOqMFBgaqsxs3blT76dOnm74wOEZISIja77jjDqNFRESos3Zf37VrclVVlQ+raxu6deum9pEjR6q9V69eRrP7O7Lrq1atMlpeXp7NCuEN7e/6woUL6uzhw4fVHhUVZbSYmBh11u5zq7q62m6J10zHjh3VnpaWpnbta86RI0fU2X379jV9YWi3tK8vdtfYjIwMowUHB6uzds9hCwsLjVZWVtbYEuFANTU1aj916pTRzp8/r85q3/cQEXn//feNZndfb/cc1u5rlLfsPp7d16EzZ84Y7YsvvlBnV65cqXbLsrxcHfzNtm3bjHbixAl1VrtvEBH5zW9+Y7S7775bnXW73WoPCwsz2p49e9TZN954w2gzZsxQZ8ePH6/24cOHG23y5Mnq7Hvvvad2NI5XegAAAAAAAAAAAEdg0wMAAAAAAAAAADgCmx4AAAAAAAAAAMARfNr0WLRokaSnp0t4eLjExsbKjBkz5MCBAw1mLMuSzMxMSUxMlNDQUJk0aZLs3bu3WRcNAAAAAAAAAADwSz4dZJ6dnS2PPvqopKenS01NjSxYsEAyMjJk37599YfWLV68WF588UVZunSppKSkyHPPPSdTp06VAwcOSHh4eIv8IdB6goL0h5DdQeaJiYlG27Vrlzq7evVqtV/toVtou0JDQ4128803q7PDhg1Tu3YoVVFRkTqrHYh5/PhxddbuQFjAH2iHcMbFxamzgwYNMprdweQbNmxQu93nHNoXu3uEG2+80WhOukfUvpaJ6IdSa38XIiI33XST2rWvcXYHe5aWlqq9oKDAaBUVFeosvKP9f3Dx4kV19tixY2ofM2aM0X7729+qs7GxsWo/ePCgzQq9ExgYqPauXbuqXXtMx8fHq7MTJ05Uu3YYrt0BwHZ/p0BjtMeY3T1QZGSk0Vwulzq7f/9+tWsHNnMAMy7TnlPaXdvsuvYYu9YefPBBtWvf7xER2bhxo9E+//xzddbuAGs419mzZ422fPlyddbj8ah97NixRrO7JykpKVF7VlaW0f70pz+ps9rh6507d1Zn+/btq/ZevXoZbeTIkersF198ofbz58+rHX/l06bHmjVrGvz3W2+9JbGxsbJt2zaZMGGCWJYlL730kixYsEBmzZolIiLLli2TuLg4Wb58uTz88MPNt3IAAAAAAAAAAICfuaozPS7vjkVFRYmIyNGjR6WwsFAyMjLqZ9xut0ycOFHd2QUAAAAAAAAAAGguPr3S4+csy5LHH39cxo0bJ6mpqSIiUlhYKCLmy0Xj4uJsf2RMVVWVVFVV1f+33UvxAQAAAAAAAAAAGtPkV3o89thjsmvXLvnzn/9svO2XP/fSsizbn4W5aNEi8Xg89b+6d+/e1CUBAAAAAAAAAIB2rEmbHvPmzZNVq1bJ+vXrpVu3bvX98iExl1/xcVlRUZHtYWFPPfWUlJSU1P/Kz89vypIAAAAAAAAAAEA759OPt7IsS+bNmycff/yxbNiwQZKSkhq8PSkpSeLj4yUrK0uGDBkiIiLV1dWSnZ0tL7zwgvo+3W63uN3uJi4frW3kyJFqHzVqlNq1V/xs2bJFnd22bVvTFwa/NHbsWKNNnTpVne3Ro4faz507ZzS7M4W++OILo9XV1TW2RMAvdenSxWijR49WZ7t27Wo0u+u03T9U+PmPrUT7FRCg/9ua2NhYowUHB7f0cq6Z4cOHq33atGlGmzJlijrbu3dvtdfW1hpN+7onIrJixQq15+bmGq26ulqdRdPZXQdXrVql9s6dOxtt0KBB6mxiYqLaz549693ibNi9Mj8kJETt4eHhRouOjlZntc97O8XFxWrfvXu31+8DuCwyMtJoAwYM8Pr3V1ZWqv3rr79W+y//ASjgL+zu2/r372+0u+66S529/A+hf2nt2rVGO3LkiDqr3evA2Wpqaoy2fv16dTY0NFTtRUVFRrP7npHdc9g33njDaH/5y1/UWe0+b926dersL793ftn06dONZvc84sYbb1T7Bx98oHb8lU+bHo8++qgsX75cPv30UwkPD6//gu7xeCQ0NFRcLpfMnz9fFi5cKMnJyZKcnCwLFy6UsLAwue+++1rkDwAAAAAAAAAAACDi46bH66+/LiIikyZNatDfeusteeCBB0RE5Mknn5SKigp55JFHpLi4WEaOHCnr1q1T/yUQAAAAAAAAAABAc/H5x1tdicvlkszMTMnMzGzqmgAAAAAAAAAAAHzWpIPMAQAAAAAAAAAA2ho2PQAAAAAAAAAAgCP49OOt0D64XC61d+/e3Wi//vWv1dnU1FS179y502h/+ctf1Nni4mK7JcKhZs2aZbS0tDR1NiwsTO1bt2412ooVK9TZ7du3+7A6wH9FRkYaze5zy+12Gy0/P1+dvXTp0tUtDPBjkydPVrv2tUxE5OabbzZar169fPqYFRUVRsvNzVVn33rrLbXn5eUZrba21qd14Mrsro9r165Vu/b/7ZgxY9TZHj16qD00NNTL1elqamrUrt2/i+iPm1GjRqmznTt3VnvHjh2NVlpaqs4ePHhQ7YCISFCQ/q2NpKQko02YMEGd1X6c9/nz59XZjRs3qv3MmTN2SwTaNLvPoRtuuMFogwYNUmftrt9HjhwxGt/vQWPOnj2r9tWrV6v98OHDRuvatatP7/vbb781mi/Pd3/88Ue1f/XVV2rv37+/0QYOHKjOZmRkqP2TTz4xWnV1tc0K2x9e6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIbHoAAAAAAAAAAABHYNMDAAAAAAAAAAA4AgeZw6AdKCgicueddxrtlltuUWftDkPPzs42GodJ47Lhw4cbLTo62qf3oR2StnnzZq9/f3BwsE8fT2N3IGxdXZ3X7yMgQN+TDgwMVLv2OWd3GF337t3Vrh1g7Svt8Drt0FwR3/4+cHXCw8ON1rt3b3VWe/weP35cna2qqrq6hQFXYHc/oT2mY2Nj1VmPx6P2vn37Gi0kJMTrtf3ud79Tu3YooYj+9czuz2dHu8Z+9tln6uyhQ4fU7suBjGg67UBkEfvDMz/66COjfffdd+psfHy82u3u4b1ld5D5tm3b1K59HZ87d646GxMTo/bIyEij2d0DdejQQe3l5eVqR/tid63v16+f0a6//np1VvscOHjwoDp79OhRtVdUVNgtEWjT7J47jhs3zmhhYWHqrHYQtIjIDz/8YLSSkhIfVgf8ld19lF2/luy+v7F79261a9+nsvv6lJqaqnbteceuXbt8Wp+T8UoPAAAAAAAAAADgCGx6AAAAAAAAAAAAR2DTAwAAAAAAAAAAOAKbHgAAAAAAAAAAwBHY9AAAAAAAAAAAAI4Q1NoLQOsJCwtTe3p6utoffvhho0VGRqqzWVlZat+0aZPRzpw5Y7dEwGexsbFGGzlypDpbVlbWIms4e/as2ktLS9VuWZbRIiIi1NmYmBi1u1wuo3Xq1EmdvfXWW9Vu9/nsi927dxvtT3/6kzpbWFh41R8PDQUF6V/WExMTjZaWlqbO1tbWGm3Hjh3q7MWLF31YHdob7domIlJcXGw07XEnol/bREQGDx5stLvvvludTUlJUfv9999vNF+ugwEB+r8dsvtz23VNdXW12g8ePGi0//iP//D6/cK/FBUV+dTbAu3zW8S3rxcdOnRQe1RUlNrLy8u9ft9wrujoaLX36NHD6/dRWVlptLVr16qzdvf1gD8IDAw0mvY8WkRk+PDhXv1+EZHs7Gy1a/cvQHtx5MgRtWvfN01NTVVn7b6nNXv2bKM9++yz6mxJSYndEh2LV3oAAAAAAAAAAABHYNMDAAAAAAAAAAA4ApseAAAAAAAAAADAEdj0AAAAAAAAAAAAjsCmBwAAAAAAAAAAcISg1l4Arg2Xy2W0Xr16qbMvvfSS2vv06WO0Y8eOqbMffPCB2nNyctQOiIjU1NQYzbIsn97Hrbfe6lVrSVu2bFH7gQMH1K79GVNSUtTZUaNGNX1hV6D9/dfW1qqzdv+/9O3b12gnT55UZ9966y0fVgdvxMfHqz01NdVoHo9HnS0rKzOa9tgQ8f3zE+1LdXW12j/88EOjJScnq7OhoaFqnzlzpldNxLfHaV1dndezvr4P7Xp66dIldTY3N1ftq1atavrCgGtAe87hay8vL1dn8/Pzm74wOF6HDh3UHhERYTS7+9uioiKjrVixQp0tLi72YXVA2xIZGWm0e++9V52Ni4szmvZ8QcT++0N8vgCm3bt3G23NmjXq7MSJE9X+29/+1mgrV65UZ7dv3672yspKuyX6PV7pAQAAAAAAAAAAHIFNDwAAAAAAAAAA4AhsegAAAAAAAAAAAEdg0wMAAAAAAAAAADgCB5m3E9pBoNrB5CIi/fv3V3tgYKDRlixZos5+8803arc78AoQEdm6davRevbsqc7aHdjcFgwdOlTtgwcP9vp9BAToe9J2B/JqBzL6eiCvdrCV3WFXZ86c8brv2LHDp3Wg6ew+X4YMGWK0qqoqdXbPnj1G27x5szpbUVHhw+rQ3tgd0v3ll18a7de//rU6GxMTo3a7A2vbArt7nRMnThjtk08+UWffffddtRcWFjZ5XcC1YHef4ku3mwUa061bN7Vrz23tvj4VFBQYrbS0VJ21Owwd8AfaQeb33HOPOhscHGw07Xm7iEhRUZHaQ0JCjNapUyd11uVyqf3cuXNqB/yV9phev369Omv33EA7yPzv//7v1dl//ud/Vvvhw4eN5uv3ktoqXukBAAAAAAAAAAAcgU0PAAAAAAAAAADgCGx6AAAAAAAAAAAAR2DTAwAAAAAAAAAAOAKbHgAAAAAAAAAAwBGCWnsBaF5hYWFqv+GGG4z2L//yL+psTU2N2t944w2jffrpp+rs6dOn7ZYI2Hr++eeNlpubq84mJyer3eVyef3x3G630Xr16qXOTpw4Ue1BQeZlVGsiIrt371b7rl27jFZSUqLOXrhwQe3ff/+90fLy8tTZ2tpatV+8eNGrJmJ/ndB6RUWFOoums3uM9e7dW+2pqalGKysrU2e/+uoro9k9DizLslsiYPv4yM/PN9rixYvV2UmTJqn95ptvNlq/fv28X1wLysrKUvu7775rtM2bN6uz58+fV7vd9RtoK4KDg9UeGBio9rq6OqPZ3WMAIvb3+pGRkWqPi4szmva4ExGprKw02qVLl3xYHeAftM+jkJAQr39/dXW12ocMGaL26dOnG61r167q7Hfffaf2t99+28vVAf5Le54kIrJ69Wq1a59bU6dOVWe15yIiIqdOnTJaeXm53RL9Cq/0AAAAAAAAAAAAjsCmBwAAAAAAAAAAcAQ2PQAAAAAAAAAAgCOw6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIQa29ADSvpKQktWdkZBht4MCB6uylS5fUvn79eqMVFBSoszU1NXZLBGxpj6cPP/xQnQ0PD7/qjxcYGGi0Tp06qbOJiYlqd7lcXn+88+fPe92rq6vVWbvPraKiIqOVl5d7vTb4F8uyfOolJSVG27Vrlzr7/vvvG62qqsqH1QGN065j33zzjTr7448/qn3Hjh1GmzJlijo7atQotWvX9ZycHHVWW19dXZ06+8MPP6h9y5YtRtOu3YA/S0tLU3t8fLzatc+5DRs2NOOK0F6UlZWp/cyZM0aLiopSZ7XnInbXeqA9s7u36tu3r9q1z62NGzeqszt37mz6wgA/d/HiRbVv3bpV7c8++6zRXn75ZXX21ltvVfvBgweNduDAAbsl+hVe6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIbHoAAAAAAAAAAABHYNMDAAAAAAAAAAA4AgeZ+zHtALbx48ers9rhnm63W521O0BZO2yZA8vR0goLC33qQHtQW1urdu2gZBGRxYsXG83uAGXtIDMO8URLKykp8amfO3fOaPv27VNn16xZo/bOnTsbLS8vT53dv3+/0ew+L+wO07XrQHvw/fffq/2rr74y2pdfftnSy4EfsyxL7Xv37lX7p59+ajTtubGIfnCr3T0X4M9++ukno33yySfq7MyZM41m9zwiJyfH6759+3Z1Njc3V+1Ae2D3NU577iMisnr1aqPNnTtXnR07dqza161bZzS750QVFRVqb6t4pQcAAAAAAAAAAHAENj0AAAAAAAAAAIAjsOkBAAAAAAAAAAAcgU0PAAAAAAAAAADgCD5terz++usyaNAgiYiIkIiICBk9erR88cUX9W+3LEsyMzMlMTFRQkNDZdKkSbYHigEAAAAAAAAAADSnIF+Gu3XrJs8//7z07dtXRESWLVsmd9xxh+zYsUMGDBggixcvlhdffFGWLl0qKSkp8txzz8nUqVPlwIEDEh4e3iJ/gPasR48eRktPT1dnk5OTjVZbW6vOlpaWqr2qqspolmU1tkQAwDV08OBBnzrgr4qLi71qIiLbt29v6eUA+P+tW7dO7TU1NWrfvXu30c6ePdusa0L7cPz4cbWvXLnSaIcOHVJnT5w4YTS7xy7gz7R7pnfeeUedzc/PN9rp06fV2ZycHLUXFBQY7dKlS+os32MCTHV1dWo/d+6c0d5880119rHHHlN7z549jebxeNTZiooKuyW2ST690mP69Olyyy23SEpKiqSkpMi///u/S6dOnWTTpk1iWZa89NJLsmDBApk1a5akpqbKsmXL5OLFi7J8+fKWWj8AAAAAAAAAAICIXMWZHrW1tbJixQopLy+X0aNHy9GjR6WwsFAyMjLqZ9xut0ycOFE2btxo+36qqqqktLS0wS8AAAAAAAAAAABf+bzpsXv3bunUqZO43W6ZO3eufPzxx9K/f38pLCwUEZG4uLgG83FxcfVv0yxatEg8Hk/9r+7du/u6JAAAAAAAAAAAAN83Pfr16yc5OTmyadMm+f3vfy9z5syRffv21b/d5XI1mLcsy2g/99RTT0lJSUn9L+3nBQIAAAAAAAAAAFyJTweZi4iEhITUH2Q+fPhw2bp1q7z88svyxz/+UURECgsLJSEhoX6+qKjIePXHz7ndbnG73b4uAwAAAAAAAAAAoAGfNz1+ybIsqaqqkqSkJImPj5esrCwZMmSIiIhUV1dLdna2vPDCC1e9UJiio6ONFhsbq85almW0U6dOqbOff/652rUfU1ZbW9vYEgEAAAC0E1lZWa29BLRTly5dUvuhQ4e8akB7on2+HDx4UJ216wBaX3V1tdGWLl2qztq9IOHEiRNGq6uru6p1tRU+bXo8/fTTMm3aNOnevbuUlZXJihUrZMOGDbJmzRpxuVwyf/58WbhwoSQnJ0tycrIsXLhQwsLC5L777mup9QMAAAAAAAAAAIiIj5sep0+fltmzZ0tBQYF4PB4ZNGiQrFmzRqZOnSoiIk8++aRUVFTII488IsXFxTJy5EhZt26dhIeHt8jiAQAAAAAAAAAALvNp0+PNN99s9O0ul0syMzMlMzPzatYEAAAAAAAAAADgs4DWXgAAAAAAAAAAAEBzuOqDzNF6SktLjXbkyBF1dvPmzUbbsmWLOvtv//Zvai8rK/NhdQAAAAAAAACAa6GyslLtdt/rdTJe6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIbHoAAAAAAAAAAABHYNMDAAAAAAAAAAA4ApseAAAAAAAAAADAEVyWZVmtvYifKy0tFY/H09rLAAAAAAAAAAAAbUhJSYlEREQ0OsMrPQAAAAAAAAAAgCOw6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIbHoAAAAAAAAAAABHYNMDAAAAAAAAAAA4ApseAAAAAAAAAADAEdj0AAAAAAAAAAAAjsCmBwAAAAAAAAAAcAQ2PQAAAAAAAAAAgCOw6QEAAAAAAAAAAByBTQ8AAAAAAAAAAOAIbHoAAAAAAAAAAABHYNMDAAAAAAAAAAA4ApseAAAAAAAAAADAEdj0AAAAAAAAAAAAjtDmNj0sy2rtJQAAAAAAAAAAgDbGm/2DNrfpUVZW1tpLAAAAAAAAAAAAbYw3+wcuq429tKKurk5OnTol4eHhUlZWJt27d5f8/HyJiIho7aUBcKjS0lKuNQBaHNcaANcC1xoA1wLXGgDXAtca/JxlWVJWViaJiYkSEND4azmCrtGavBYQECDdunUTERGXyyUiIhERETywAbQ4rjUArgWuNQCuBa41AK4FrjUArgWuNbjM4/F4NdfmfrwVAAAAAAAAAABAU7DpAQAAAAAAAAAAHKFNb3q43W555plnxO12t/ZSADgY1xoA1wLXGgDXAtcaANcC1xoA1wLXGjRVmzvIHAAAAAAAAAAAoCna9Cs9AAAAAAAAAAAAvMWmBwAAAAAAAAAAcAQ2PQAAAAAAAAAAgCOw6QEAAAAAAAAAAByhTW96vPbaa5KUlCQdOnSQYcOGybffftvaSwLgpzIzM8XlcjX4FR8fX/92y7IkMzNTEhMTJTQ0VCZNmiR79+5txRUD8AfffPONTJ8+XRITE8Xlcsknn3zS4O3eXFuqqqpk3rx5Eh0dLR07dpTbb79dTpw4cQ3/FADauitdax544AHjPmfUqFENZrjWAGjMokWLJD09XcLDwyU2NlZmzJghBw4caDDDfQ2Aq+XNtYb7GjSHNrvp8f7778v8+fNlwYIFsmPHDhk/frxMmzZN8vLyWntpAPzUgAEDpKCgoP7X7t2769+2ePFiefHFF+WVV16RrVu3Snx8vEydOlXKyspaccUA2rry8nJJS0uTV155RX27N9eW+fPny8cffywrVqyQ7777Ti5cuCC33Xab1NbWXqs/BoA27krXGhGRm2++ucF9zurVqxu8nWsNgMZkZ2fLo48+Kps2bZKsrCypqamRjIwMKS8vr5/hvgbA1fLmWiPCfQ2agdVGjRgxwpo7d26Ddt1111n/9E//1EorAuDPnnnmGSstLU19W11dnRUfH289//zz9a2ystLyeDzWf/3Xf12jFQLwdyJiffzxx/X/7c215aeffrKCg4OtFStW1M+cPHnSCggIsNasWXPN1g7Af/zyWmNZljVnzhzrjjvusP09XGsA+KqoqMgSESs7O9uyLO5rALSMX15rLIv7GjSPNvlKj+rqatm2bZtkZGQ06BkZGbJx48ZWWhUAf3fo0CFJTEyUpKQk+dWvfiVHjhwREZGjR49KYWFhg2uO2+2WiRMncs0B0GTeXFu2bdsmly5dajCTmJgoqampXH8A+GTDhg0SGxsrKSkp8tBDD0lRUVH927jWAPBVSUmJiIhERUWJCPc1AFrGL681l3Ffg6vVJjc9zp49K7W1tRIXF9egx8XFSWFhYSutCoA/GzlypLz99tuydu1a+e///m8pLCyUMWPGyLlz5+qvK1xzADQnb64thYWFEhISIpGRkbYzAHAl06ZNk/fee0++/vpr+c///E/ZunWr3HDDDVJVVSUiXGsA+MayLHn88cdl3LhxkpqaKiLc1wBoftq1RoT7GjSPoNZeQGNcLleD/7Ysy2gA4I1p06bV/++BAwfK6NGjpU+fPrJs2bL6A7G45gBoCU25tnD9AeCLe++9t/5/p6amyvDhw6Vnz57yf//3fzJr1izb38e1BoDmsccek127dsl3331nvI37GgDNxe5aw30NmkObfKVHdHS0BAYGGrtzRUVFxr8qAICm6NixowwcOFAOHTok8fHxIiJccwA0K2+uLfHx8VJdXS3FxcW2MwDgq4SEBOnZs6ccOnRIRLjWAPDevHnzZNWqVbJ+/Xrp1q1bfee+BkBzsrvWaLivQVO0yU2PkJAQGTZsmGRlZTXoWVlZMmbMmFZaFQAnqaqqkh9//FESEhIkKSlJ4uPjG1xzqqurJTs7m2sOgCbz5toybNgwCQ4ObjBTUFAge/bs4foDoMnOnTsn+fn5kpCQICJcawBcmWVZ8thjj8lHH30kX3/9tSQlJTV4O/c1AJrDla41Gu5r0BRt9sdbPf744zJ79mwZPny4jB49WpYsWSJ5eXkyd+7c1l4aAD/0xBNPyPTp06VHjx5SVFQkzz33nJSWlsqcOXPE5XLJ/PnzZeHChZKcnCzJycmycOFCCQsLk/vuu6+1lw6gDbtw4YLk5ubW//fRo0clJydHoqKipEePHle8tng8HvnNb34jf/jDH6RLly4SFRUlTzzxhAwcOFBuvPHG1vpjAWhjGrvWREVFSWZmptx5552SkJAgx44dk6efflqio6Nl5syZIsK1BsCVPfroo7J8+XL59NNPJTw8vP4VHR6PR0JDQ716zsS1BsCVXOlac+HCBe5r0DysNuzVV1+1evbsaYWEhFhDhw61srOzW3tJAPzUvffeayUkJFjBwcFWYmKiNWvWLGvv3r31b6+rq7OeeeYZKz4+3nK73daECROs3bt3t+KKAfiD9evXWyJi/JozZ45lWd5dWyoqKqzHHnvMioqKskJDQ63bbrvNysvLa4U/DYC2qrFrzcWLF62MjAwrJibGCg4Otnr06GHNmTPHuI5wrQHQGO0aIyLWW2+9VT/DfQ2Aq3Wlaw33NWguLsuyrGu5yQIAAAAAAAAAANAS2uSZHgAAAAAAAAAAAL5i0wMAAAAAAAAAADgCmx4AAAAAAAAAAMAR2PQAAAAAAAAAAACOwKYHAAAAAAAAAABwBDY9AAAAAAAAAACAI7DpAQAAAAAAAAAAHIFNDwAAAAAAAAAA4AhsegAAAAAAAAAAAEdg0wMAAAAAAAAAADgCmx4AAAAAAAAAAMAR2PQAAAAAAAAAAACO8P8BajnCz5MpJ+MAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the data!!!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuZmm4Jx6IeQ"
   },
   "outputs": [],
   "source": [
    "# Create our network\n",
    "vae_net = VAE(channel_in=1, z=latent_size).to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(vae_net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Create loss logger\n",
    "loss_log = []\n",
    "train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7126,
     "status": "ok",
     "timestamp": 1570409784003,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "iteROyrA6IeT",
    "outputId": "51104e6b-af60-44b9-a7f6-5885749d92af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass through a test image to make sure everything is working\n",
    "recon_data, mu, logvar = vae_net(test_images.to(device))\n",
    "\n",
    "# View the Latent vector shape\n",
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1570410601202,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "bVFA6W6L6IeY",
    "outputId": "99cd69e0-8ff6-466b-fb59-4e4ba9f33271"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a15142f69d48be818f58b0c48c42d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83aa1822c34c43af8f5046f5f939757e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000021DC77BEB60>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1479, in __del__\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1443, in _shutdown_workers\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\multiprocessing\\process.py\", line 149, in join\n",
      "  File \"C:\\Users\\admin\\anaconda3\\Lib\\multiprocessing\\popen_spawn_win32.py\", line 110, in wait\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Take a training step\u001b[39;00m\n\u001b[0;32m     22\u001b[0m vae_net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = trange(0, nepoch, leave=False, desc=\"Epoch\")   \n",
    "vae_net.train()\n",
    "train_loss = 0\n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % (train_loss/len(train_loader)))\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(tqdm(train_loader, leave=False, desc=\"Training\")):\n",
    "\n",
    "        image = data[0].to(device)\n",
    "\n",
    "        # Forward pass the image in the data tuple\n",
    "        recon_data, mu, logvar = vae_net(image)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = vae_loss(recon_data, image, mu, logvar)\n",
    "        \n",
    "        # Log the loss\n",
    "        loss_log.append(loss.item())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Take a training step\n",
    "        vae_net.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0ZrSDsR6Ief"
   },
   "source": [
    "## Results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 824609,
     "status": "ok",
     "timestamp": 1570410601500,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "vTr64GEm6Iej",
    "outputId": "4d715e03-42e0-4277-ec5a-1e5577c2b240"
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(loss_log[1000:])\n",
    "_ = plt.title(\"VAE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_net.eval()\n",
    "recon_data, mu, logvar = vae_net(test_images.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GW3ruNa6Ieo"
   },
   "source": [
    "Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 825552,
     "status": "ok",
     "timestamp": 1570410602450,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "yqC3cmVx6Ieo",
    "outputId": "8665a80d-bc7d-4be4-d8fa-89716e7391c7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPRXvoNr6Ies"
   },
   "source": [
    "Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 825546,
     "status": "ok",
     "timestamp": 1570410602451,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "kX95LD-u6Iet",
    "outputId": "40c30d2b-837b-4143-a540-024b14789389"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(recon_data.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xBrRk9Hp6Iev"
   },
   "source": [
    "Random Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 826092,
     "status": "ok",
     "timestamp": 1570410603004,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "5Dm8T70o6Iex",
    "outputId": "dd0e50e3-780b-4694-d550-cbd3c258ba2b"
   },
   "outputs": [],
   "source": [
    "rand_samp = vae_net.decoder(mu + 1 * torch.randn_like(mu))\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(rand_samp.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_samp = vae_net.decoder(torch.randn_like(mu))\n",
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(rand_samp.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-_CImBn6Ie0"
   },
   "source": [
    "### Interpolation in Latent Space\n",
    "Now that we've trained our VAE we can explore the \"MNIST Latent Space\" it has created. <br>\n",
    "We first use our validation images and class labels to find the mean latent vector for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 830953,
     "status": "ok",
     "timestamp": 1570410607873,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "7_ZEGuhF6Ie1",
    "outputId": "2ca91ef6-c10c-4b05-f27f-b74a366aaeaa"
   },
   "outputs": [],
   "source": [
    "# Initialise the class means to 0\n",
    "class_means = torch.zeros(10, latent_size)\n",
    "\n",
    "vae_net.eval()\n",
    "# Loop through all the data in the validation set\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        images, labels = data\n",
    "        recon_data, mu, _ = vae_net(images.to(device))\n",
    "        \n",
    "        # For each batch sum up the latent vectors of the same class\n",
    "        # (Use a matrix of one hot coded vectors to make it easy)\n",
    "        class_matrix = F.one_hot(labels, 10).t().type(torch.FloatTensor)\n",
    "        class_means += torch.matmul(class_matrix, mu.squeeze().detach().cpu())\n",
    "\n",
    "# In the validation set each class has 1000 images so to find the mean vectors we divide by 1000\n",
    "class_means /= 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFTYB8Xg6Ie5"
   },
   "source": [
    "Recondstruct the means using the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3qMLKiO6Ie7"
   },
   "outputs": [],
   "source": [
    "# Reshape the mean classes to the appropriate shape \n",
    "class_means = class_means.view(10, latent_size, 1, 1)\n",
    "\n",
    "# We only need to pass the latent vectors through our decoder\n",
    "recon_means = vae_net.decoder(class_means.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhGtytFf6IfD"
   },
   "source": [
    "Plot out our class means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 832301,
     "status": "ok",
     "timestamp": 1570410609232,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "1mZo8Y6_6IfJ",
    "outputId": "e80d67df-b7de-42f8-9bdc-3116d4a1f4e1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "out = vutils.make_grid(recon_means.detach().cpu(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuiSJmX76IfW"
   },
   "source": [
    "Interpolate between two class means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZl3Vg5t6IfZ"
   },
   "outputs": [],
   "source": [
    "# Pick the two classes to move between\n",
    "start_class = 4\n",
    "end_class = 7\n",
    "\n",
    "# Number of interpolation steps\n",
    "num_steps = 100\n",
    "steps = torch.linspace(0,1,num_steps)\n",
    "\n",
    "# Get the vector pointing from one class to the other\n",
    "diff_vector = class_means[end_class] - class_means[start_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vh1pWb866Ifi"
   },
   "source": [
    "Take \"num_steps\" from the \"start_class\" to the \"end_class\" along the \"diff_vector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBiC3_WZ6Ifk"
   },
   "outputs": [],
   "source": [
    "latent_steps = class_means[start_class] + (steps.view(num_steps, 1, 1, 1) * diff_vector.view(1, latent_size, 1, 1))\n",
    "recon_steps = vae_net.decoder(latent_steps.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1VigySL6Ifr"
   },
   "source": [
    "Visualize the interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 832289,
     "status": "ok",
     "timestamp": 1570410609233,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "dvkc6DZX6Ift",
    "outputId": "9bb9921f-8945-4528-d7a1-38c31e993daf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(recon_steps.detach().cpu(), normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZZDqGWP6Ifw"
   },
   "source": [
    "Animate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1570410637193,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "XkoT838E6Ifx",
    "outputId": "0151c9e0-3ce5-4d23-b1d3-e42572ca13d6"
   },
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    plt.imshow(((recon_steps[i, 0] + 1) / 2).detach().cpu())\n",
    "    plt.pause(0.01)\n",
    "    clear_output(True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VAE.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
