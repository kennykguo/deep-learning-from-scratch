{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05196f-035c-49d6-8922-d823e8fb9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedfdf34-73d5-4c25-9115-ae5dca9d7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of epochs for training\n",
    "nepochs = 20\n",
    "\n",
    "# Batch size for data loaders\n",
    "batch_size = 128\n",
    "\n",
    "# Maximum sequence length for text inputs\n",
    "max_len = 128\n",
    "\n",
    "# Root directory of the dataset\n",
    "data_set_root = \"datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c763a040-2929-4594-83c9-c4d449115b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using the AG News Dataset\n",
    "# Which contains a short news article and a single label to classify the \"type\" of article\n",
    "# Note that for torchtext these datasets are NOT torch dataset classes \"AG_NEWS\" is a function that returns a Pytorch data pipe!\n",
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "\n",
    "# Good blog on the difference between DataSet and DataPipe\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58\n",
    "dataset_train = AG_NEWS(root=data_set_root, split=\"train\")\n",
    "data = next(iter(dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "462bda6a-59f4-4ca0-b4fa-e78c8e08ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=datasets/datasets/AG_NEWS/data.txt --model_prefix=spm_ag_news --vocab_size=20000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: datasets/datasets/AG_NEWS/data.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_ag_news\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: datasets/datasets/AG_NEWS/data.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 120000 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=28222889\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9756% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=48\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999756\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 120000 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 152602 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 120000\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 120631\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 120631 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=62443 obj=11.2717 num_tokens=276733 num_tokens/piece=4.43177\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=49790 obj=9.0969 num_tokens=277388 num_tokens/piece=5.57116\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=37337 obj=9.0515 num_tokens=288412 num_tokens/piece=7.72456\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=37324 obj=9.03806 num_tokens=288579 num_tokens/piece=7.73173\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=27993 obj=9.07602 num_tokens=309208 num_tokens/piece=11.0459\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=27993 obj=9.06489 num_tokens=309177 num_tokens/piece=11.0448\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=22000 obj=9.12194 num_tokens=328649 num_tokens/piece=14.9386\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=21999 obj=9.10707 num_tokens=328625 num_tokens/piece=14.9382\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: spm_ag_news.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: spm_ag_news.vocab\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.functional import generate_sp_model\n",
    "\n",
    "with open(os.path.join(data_set_root, \"datasets/AG_NEWS/train.csv\")) as f:\n",
    "    with open(os.path.join(data_set_root, \"datasets/AG_NEWS/data.txt\"), \"w\") as f2:\n",
    "        for i, line in enumerate(f):\n",
    "            text_only = \"\".join(line.split(\",\")[1:])\n",
    "            filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' ')) # remove newline characters\n",
    "            filtered = filtered.replace(' #39;', \"'\")\n",
    "            filtered = filtered.replace(' #38;', \"&\")\n",
    "            filtered = filtered.replace(' #36;', \"$\")\n",
    "            filtered = filtered.replace(' #151;', \"-\")\n",
    "\n",
    "            f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "generate_sp_model(os.path.join(data_set_root, \"datasets/AG_NEWS/data.txt\"), \n",
    "                  vocab_size=20000, model_prefix='spm_ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5bfec94-e193-441c-bd5d-a0ee4114a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGNews(Dataset):\n",
    "    \"\"\"\n",
    "    The AGNews class is a custom Dataset for handling the AG News dataset.\n",
    "    This dataset consists of news articles categorized into four classes.\n",
    "    The class loads the data from CSV files, preprocesses the text by cleaning and combining\n",
    "    relevant columns, and provides an interface to access individual samples along with their\n",
    "    corresponding class labels.\n",
    "    \n",
    "    Attributes:\n",
    "        df (pd.DataFrame): The DataFrame containing the preprocessed dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_datapoints, test_train=\"train\"):\n",
    "        # Load the dataset from the specified CSV file\n",
    "        self.df = pd.read_csv(os.path.join(data_set_root, \"datasets/AG_NEWS/\" + test_train + \".csv\"),\n",
    "                              names=[\"Class\", \"Title\", \"Content\"])\n",
    "        \n",
    "        # Fill any missing values with empty strings\n",
    "        self.df.fillna('', inplace=True)\n",
    "        \n",
    "        # Combine the Title and Content columns into a single Article column\n",
    "        self.df['Article'] = self.df['Title'] + \" : \" + self.df['Content']\n",
    "        \n",
    "        # Drop the now redundant Title and Content columns\n",
    "        self.df.drop(['Title', 'Content'], axis=1, inplace=True)\n",
    "        \n",
    "        # Clean the Article column by removing unwanted characters and replacing HTML codes\n",
    "        self.df['Article'] = self.df['Article'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "        self.df['Article'] = self.df['Article'].replace({' #39;': \"'\", \n",
    "                                                         ' #38;': \"&\", \n",
    "                                                         ' #36;': \"$\",\n",
    "                                                         ' #151;': \"-\"}, \n",
    "                                                        regex=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve the article text and convert it to lowercase\n",
    "        text = self.df.loc[index][\"Article\"].lower()\n",
    "        \n",
    "        # Retrieve the class label and convert it to an integer\n",
    "        class_index = int(self.df.loc[index][\"Class\"]) - 1\n",
    "\n",
    "        # Return a tuple of the class index and the article text\n",
    "        return class_index, text\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the number of data points in the dataset\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fc4a963-b8a1-4b73-b81d-2352794c73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing datasets\n",
    "dataset_train = AGNews(num_datapoints=data_set_root, test_train=\"train\")\n",
    "dataset_test = AGNews(num_datapoints=data_set_root, test_train=\"test\")\n",
    "\n",
    "# Create data loaders for the training and testing datasets\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdc1832f-a16b-4b45-8152-86754e96e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            # Yield the token from the first column (split by tab)\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "# Build a vocabulary from the tokens yielded by the yield_tokens function\n",
    "# We will also add \"special\" tokens that we'll use to signal something to our model\n",
    "# <pad> is a padding token that is added to the end of a sentence to ensure \n",
    "# the length of all sequences in a batch is the same\n",
    "# <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "# <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "# <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"spm_ag_news.vocab\"), \n",
    "                                  specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "                                  special_first=True)\n",
    "\n",
    "# Set the default index for unknown tokens to the index of the '<unk>' token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b0ddb0c-f8e9-4491-8aa1-4a1a6d5d84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transform to turn text into vocab tokens\n",
    "text_tranform = T.Sequential(\n",
    "    # Tokeniz with pre-existing Tokenizer\n",
    "    T.SentencePieceTokenizer(\"spm_ag_news.model\"),\n",
    "    ## converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "    # 1 as seen in previous section\n",
    "    T.AddToken(1, begin=True),\n",
    "    # Crop the sentance if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "    # 2 as seen in previous section\n",
    "    T.AddToken(2, begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length!\n",
    "    T.ToTensor(padding_value=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1beb384c-6a4a-423c-91fb-f3cb75fe9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\"For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    \n",
    "    Args:\n",
    "        prob (float): probability of dropping a token\n",
    "        pad_token (int): index for the <pad> token\n",
    "        num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p=prob\n",
    "        # to create a mask where 1 means we will replace that token\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # only replace if the token is not a special token\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        mask = mask * can_drop\n",
    "        \n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        \n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc2310-f2c2-4f54-820a-f29519b492c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal positional embeds\n",
    "# Produce unique vector for the integer input vector\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "# \"Encoder-Only\" Style Transformer\n",
    "class NanoTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a simplified Transformer model for sequence classification. \n",
    "    It uses an embedding layer for tokens, sinusoidal positional embeddings, \n",
    "    a single Transformer block, and a final linear layer for prediction.\n",
    "\n",
    "    Args:\n",
    "      num_emb: The number of unique tokens in the vocabulary.\n",
    "      output_size: The size of the output layer (number of classes).\n",
    "      hidden_size: The dimension of the hidden layer in the Transformer block (default: 128).\n",
    "      num_heads: The number of heads in the multi-head attention layer (default: 4).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_emb, output_size, hidden_size=128, num_heads=4):\n",
    "        super(NanoTransformer, self).__init__()\n",
    "\n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Multi-layer perception\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                 nn.LayerNorm(hidden_size),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(hidden_size, hidden_size))\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        bs, l = input_seq.shape\n",
    "        input_embs = self.embedding(input_seq)\n",
    "\n",
    "        # Add a unique embedding to each token embedding depending on it's position in the sequence\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, -1).expand(bs, l, -1)\n",
    "        embs = input_embs + pos_emb\n",
    "        output, attn_map = self.multihead_attn(embs, embs, embs)\n",
    "\n",
    "        output = self.mlp(output)\n",
    "\n",
    "        return self.fc_out(output), attn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "561a2785-752b-4f53-ba90-da82428e3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Embedding size\n",
    "hidden_size = 256\n",
    "\n",
    "# Create the Transformer model\n",
    "tf_classifier = NanoTransformer(num_emb=len(vocab), output_size=4, \n",
    "                                hidden_size=hidden_size, num_heads=4).to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(tf_classifier.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Cosine annealing scheduler to decay the learning rate\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                    T_max=nepochs,\n",
    "                                                    eta_min=0)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Custom transform to randomly replace a token with <pad>\n",
    "td = TokenDrop(prob=0.5)\n",
    "\n",
    "# Loggers for training and testing loss\n",
    "training_loss_logger = []\n",
    "test_loss_logger = []\n",
    "\n",
    "# Loggers for training and testing accuracy\n",
    "training_acc_logger = []\n",
    "test_acc_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60012da2-236e-46c0-b5ff-dc55a21257d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-This Model Has 5517060 (Approximately 5 Million) Parameters!\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_classifier.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf14c176-ef03-441d-9616-1b055316995a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea516c04cda24c8f9453618aa97dfdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29942abb13e14a269c425b2473326e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'AGNews' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Loop over each batch in the training dataset\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, text \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader_train, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     17\u001b[0m     bs \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Transform the text to tokens and move to the GPU\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize progress bar for tracking epochs\n",
    "pbar = trange(0, nepochs, leave=False, desc=\"Epoch\")\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "\n",
    "# Loop over each epoch\n",
    "for epoch in pbar:\n",
    "    # Update the progress bar with current training and testing accuracy\n",
    "    pbar.set_postfix_str('Accuracy: Train %.2f%%, Test %.2f%%' % (train_acc * 100, test_acc * 100))\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    tf_classifier.train()\n",
    "    steps = 0\n",
    "    \n",
    "    # Loop over each batch in the training dataset\n",
    "    for label, text in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        bs = label.shape[0]\n",
    "        \n",
    "        # Transform the text to tokens and move to the GPU\n",
    "        text_tokens = text_tranform(list(text)).to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Randomly drop tokens to aid in regularization\n",
    "        text_tokens = td(text_tokens)\n",
    "\n",
    "        # Get the model predictions\n",
    "        pred, _ = tf_classifier(text_tokens)\n",
    "\n",
    "        # Compute the loss using cross-entropy loss\n",
    "        loss = loss_fn(pred[:, 0, :], label)\n",
    "        \n",
    "        # Backpropagation and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training loss\n",
    "        training_loss_logger.append(loss.item())\n",
    "        \n",
    "        # Update training accuracy\n",
    "        train_acc += (pred[:, 0, :].argmax(1) == label).sum()\n",
    "        steps += bs\n",
    "    \n",
    "    # Calculate average training accuracy\n",
    "    train_acc = (train_acc / steps).item()\n",
    "    training_acc_logger.append(train_acc)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    tf_classifier.eval()\n",
    "    steps = 0\n",
    "    \n",
    "    # Loop over each batch in the testing dataset\n",
    "    with torch.no_grad():\n",
    "        for label, text in tqdm(data_loader_test, desc=\"Testing\", leave=False):\n",
    "            bs = label.shape[0]\n",
    "            \n",
    "            # Transform the text to tokens and move to the GPU\n",
    "            text_tokens = text_tranform(list(text)).to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Get the model predictions\n",
    "            pred, _ = tf_classifier(text_tokens)\n",
    "\n",
    "            # Compute the loss using cross-entropy loss\n",
    "            loss = loss_fn(pred[:, 0, :], label)\n",
    "            test_loss_logger.append(loss.item())\n",
    "\n",
    "            # Update testing accuracy\n",
    "            test_acc += (pred[:, 0, :].argmax(1) == label).sum()\n",
    "            steps += bs\n",
    "\n",
    "        # Calculate average testing accuracy\n",
    "        test_acc = (test_acc / steps).item()\n",
    "        test_acc_logger.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d7b89-977b-4822-a80c-64e7d6e5081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_loss_logger)), training_loss_logger)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_loss_logger)), test_loss_logger)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae485636-eaae-4123-b8dc-200ccfedf863",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_acc_logger)), training_acc_logger)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_acc_logger)), test_acc_logger)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Accuracy\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Accuracy\")\n",
    "print(\"Max Test Accuracy %.2f%%\" % (np.max(test_acc_logger) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58c1f111-2c6e-4552-ba07-9c0215745739",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m ag_news_classes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorld\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSports\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBusiness\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScience/Technology\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m     label, text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(data_loader_train))\n\u001b[1;32m     10\u001b[0m     text_tokens \u001b[38;5;241m=\u001b[39m text_tranform(\u001b[38;5;28mlist\u001b[39m(text))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     pred, attention_map \u001b[38;5;241m=\u001b[39m tf_classifier(text_tokens)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader_train' is not defined"
     ]
    }
   ],
   "source": [
    "ag_news_classes = [\n",
    "    \"World\",\n",
    "    \"Sports\",\n",
    "    \"Business\",\n",
    "    \"Science/Technology\"\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    label, text = next(iter(data_loader_train))\n",
    "    text_tokens = text_tranform(list(text)).to(device)\n",
    "    pred, attention_map = tf_classifier(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "02e987ca-bc6f-4c6b-b6c9-d956aee4e3a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m test_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Choose a text index smaller than the total number in the batch!\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_index \u001b[38;5;241m<\u001b[39m label\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Select the attention map for a single sample and the first attention head\u001b[39;00m\n\u001b[1;32m      7\u001b[0m att_map \u001b[38;5;241m=\u001b[39m attention_map[test_index, \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "test_index = 3\n",
    "\n",
    "# Choose a text index smaller than the total number in the batch!\n",
    "assert test_index < label.shape[0]\n",
    "\n",
    "# Select the attention map for a single sample and the first attention head\n",
    "att_map = attention_map[test_index, 0]\n",
    "pred_class = ag_news_classes[pred[test_index, -1].argmax().item()]\n",
    "top5 = att_map.argsort(descending=True)[:5]\n",
    "top5_tokens = vocab.lookup_tokens(text_tokens[test_index, top5].cpu().numpy())\n",
    "\n",
    "\n",
    "print(\"Article:\")\n",
    "print(text[test_index])\n",
    "print(\"\\nPredicted label:\")\n",
    "print(pred_class)\n",
    "print(\"True label:\")\n",
    "print(ag_news_classes[label[test_index].item()])\n",
    "\n",
    "print(\"\\nTop 5 Tokens:\")\n",
    "print(top5_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e3dbd933-329f-4fa1-a5fd-bab5ac498b9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'att_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mplot(att_map\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m      2\u001b[0m _ \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention score per token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'att_map' is not defined"
     ]
    }
   ],
   "source": [
    "_ = plt.plot(att_map.cpu().numpy())\n",
    "_ = plt.title(\"Attention score per token\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
