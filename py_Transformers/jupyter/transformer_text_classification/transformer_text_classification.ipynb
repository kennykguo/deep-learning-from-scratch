{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31010ea-a0b2-4ad9-99d5-824ac3387b0a",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d05196f-035c-49d6-8922-d823e8fb9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.data.functional import generate_sp_model, load_sp_model, sentencepiece_tokenizer, sentencepiece_numericalizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "708aeccf-e502-47c2-894f-237215dbab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/data/main/torchdata.datapipes.iter.html\n",
    "# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3c39fc1-a25f-4b57-9e32-9f727014b8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"2\"', '\"Nets get Carter from Raptors\"', '\"INDIANAPOLIS -- All-Star Vince Carter was traded by the Toronto Raptors to the New Jersey Nets for Alonzo Mourning', ' Eric Williams', ' Aaron Williams', ' and a pair of first-round draft picks yesterday.\"\\n']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "root = \"data\"\n",
    "with open(os.path.join(root, \"train.csv\")) as f:\n",
    "        with open(os.path.join(root, \"data.txt\"), \"w\") as f2:\n",
    "            for line in f:\n",
    "                text_only = \"\".join(line.split(\",\")[1:])\n",
    "                filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' '))\n",
    "                filtered = filtered.replace(' #39;', \"'\")\n",
    "                filtered = filtered.replace(' #38;', \"&\")\n",
    "                filtered = filtered.replace(' #36;', \"$\")\n",
    "                filtered = filtered.replace(' #151;', \"-\")\n",
    "                f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "# Example line\n",
    "print(line.split(\",\"))\n",
    "print(type(line.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a3cbf43-85b6-4905-a1bf-f29e03a3874b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=data/data.txt --model_prefix=SentencePiece/transformer --vocab_size=20000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/data.txt\n",
      "  input_format: \n",
      "  model_prefix: SentencePiece/transformer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: data/data.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 120000 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=28222889\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9756% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=48\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999756\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 120000 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 152602 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 120000\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 120631\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 120631 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=62443 obj=11.2717 num_tokens=276733 num_tokens/piece=4.43177\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=49790 obj=9.0969 num_tokens=277388 num_tokens/piece=5.57116\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=37337 obj=9.0515 num_tokens=288412 num_tokens/piece=7.72456\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=37324 obj=9.03806 num_tokens=288579 num_tokens/piece=7.73173\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=27993 obj=9.07602 num_tokens=309208 num_tokens/piece=11.0459\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=27993 obj=9.06489 num_tokens=309177 num_tokens/piece=11.0448\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=22000 obj=9.12194 num_tokens=328649 num_tokens/piece=14.9386\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=21999 obj=9.10707 num_tokens=328625 num_tokens/piece=14.9382\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: SentencePiece/transformer.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: SentencePiece/transformer.vocab\n"
     ]
    }
   ],
   "source": [
    "# Generate the SentencePiece tokenizer\n",
    "    # Text tokenizer and detokenizer\n",
    "    # Unsupervised text tokenizer. It will tokenize words into subpieces instead of words\n",
    "    # This function will create a set of subtokens to fit the set vocabulary size\n",
    "    # There will always be enough subwords to subtokenize a dataset if you think about it :) -> max 2 length pairs = 26!\n",
    "    # Saved in the home directory\n",
    "generate_sp_model(os.path.join(root, \"data.txt\"), vocab_size=20000, model_prefix='SentencePiece/transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97e9b8c6-f18f-4cd0-a686-de839cbe2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class must inherit from Dataset to use DataLoader from torch\n",
    "class AGNews(Dataset):\n",
    "    def __init__(self, num_datapoints, set=\"train\"):\n",
    "\n",
    "        # Reads the file into a pandas DataFrame\n",
    "        self.df = pd.read_csv(os.path.join(root, set + \".csv\"), names=[\"Class\", \"Title\", \"Content\"])\n",
    "\n",
    "        # Replaces empty entries with a space\n",
    "        self.df.fillna('', inplace=True)\n",
    "\n",
    "        # Concatenates title and content into one heading 'article'\n",
    "        self.df['Article'] = self.df['Title'] + \" : \" + self.df['Content']\n",
    "        self.df.drop(['Title', 'Content'], axis=1, inplace=True)\n",
    "        self.df['Article'] = self.df['Article'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "\n",
    "        # Clean up 'Article' column text\n",
    "        self.df['Article'] = self.df['Article'].replace({' #39;': \"'\", ' #38;': \"&\", ' #36;': \"$\", ' #151;': \"-\"}, regex=True)\n",
    "\n",
    "        # Gets the first num_datapoints datapoints\n",
    "        if num_datapoints is not None:\n",
    "            self.df = self.df.head(num_datapoints)\n",
    "\n",
    "    # To use for DataLoader\n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.loc[index][\"Article\"].lower()\n",
    "        # See if we can remove array indexing\n",
    "        class_index = int(self.df.loc[index][\"Class\"]) - 1\n",
    "        return class_index, text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "# Create training and testing datasets from train.csv, test.csv, using AGNews class\n",
    "train_dataset = AGNews(num_datapoints=10000, set=\"train\")\n",
    "test_dataset = AGNews(num_datapoints=10000, set=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "c57c0c02-ff0b-4d53-8754-bf7c0be6c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "db1b0de6-ecdb-41e4-8b78-28eb4c06d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99d0d48a-2913-497f-9ab9-34350dfd5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for the training and testing datasets\n",
    "# Dataloaders allow for batching, shuffling\n",
    "\n",
    "batch_size = 128  # Example batch size\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e1088bb3-c7a2-4b83-9016-27b4293696b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize individual entries\n",
    "# x = list(sp_tokenizer([text_batch[0]]))[0]\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ddd4cab-8ebe-481a-b9e6-bcea113890c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenization and numericalization\n",
    "\n",
    "# # Load the trained SentencePiece model\n",
    "# sp_model = load_sp_model(\"SentencePiece/transformer.model\")\n",
    "\n",
    "# # Create a tokenizer function from file\n",
    "# sp_tokenizer = sentencepiece_tokenizer(sp_model)\n",
    "\n",
    "# # Create a numericalizer function from file\n",
    "# sp_numericalizer = sentencepiece_numericalizer(sp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "47863551-0b36-471a-9962-9bf732969868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdc1832f-a16b-4b45-8152-86754e96e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            # Yield the token from the first column (split by tab)\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "# Build a vocabulary from the tokens yielded by the yield_tokens function\n",
    "    # <pad> is a padding token that is added to the end of a sentence to ensure the length of all sequences in a batch is the same\n",
    "    # <sos> signals the \"Start-Of-Sentence\" aka the start of the sequence\n",
    "    # <eos> signal the \"End-Of-Sentence\" aka the end of the sequence\n",
    "    # <unk> \"unknown\" token is used if a token is not contained in the vocab\n",
    "# From torchtext library (build_vocab_from_iterator)\n",
    "# Builds a generator object, that is treated like an iterator\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"SentencePiece/transformer.vocab\"), specials=['<pad>', '<sos>', '<eos>', '<unk>'], special_first=True)\n",
    "\n",
    "# Set the default index for unknown tokens to the index of the '<unk>' token\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b607f19d-1f63-4040-af39-2db2615c668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocab['<pad>'])  # Should print the index of <pad>\n",
    "# print(vocab['<sos>'])  # Should print the index of <sos>\n",
    "# print(vocab['<eos>'])  # Should print the index of <eos>\n",
    "# print(vocab['<unk>'])  # Should print the index of <unk>\n",
    "# print(vocab['‚ñÅthe'])   # Should print the index of ‚ñÅthe (if present)\n",
    "# print(vocab['unknown_token'])  # Should print the index of <unk> as it's not in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b0ddb0c-f8e9-4491-8aa1-4a1a6d5d84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length for text inputs\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "# Data transform to turn text into vocab token\n",
    "# Text transformation pipeline aliased T\n",
    "text_transform = T.Sequential(\n",
    "    # Tokenize with pretrained Tokenizer\n",
    "    T.SentencePieceTokenizer(\"SentencePiece/transformer.model\"),\n",
    "    # Converts the sentences to indices based on given vocabulary\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    # Add <sos> at beginning of each sentence. \n",
    "    # 1 because the index for <sos> in vocabulary is 1 as seen in previous section\n",
    "    T.AddToken(vocab['<sos>'], begin=True),\n",
    "    # Crop the sentence if it is longer than the max length\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    ## Add <eos> at beginning of each sentence. \n",
    "    # 2 because the index for <eos> in vocabulary is 2\n",
    "    T.AddToken(vocab['<eos>'], begin=False),\n",
    "    # Convert the list of lists to a tensor, this will also\n",
    "    # Pad a sentence with the <pad> token if it is shorter than the max length\n",
    "    # This ensures all sentences are the same length\n",
    "    # T.ToTensor(padding_value=vocab['<pad>']),\n",
    ")\n",
    "\n",
    "# Custom padding function\n",
    "def pad_sequence(seq, max_len, padding_value):\n",
    "    if len(seq) < max_len:\n",
    "        seq += [padding_value] * (max_len - len(seq))\n",
    "    return seq[:max_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "598a72b7-adef-48ef-8a98-59b24373bcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  3686,     3,  1556, 11578,  5040,  9237, 16819,    50,     2,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,  4555,  4555,     3,  4555,     3,  4555,     3,  4555,     3,\n",
      "          4555,     3,  4555,     3,  4555,     2,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    1,  5521,  5521,  5521,  5521,  5521,  5521,  5521,     2,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "torch.Size([3, 128])\n"
     ]
    }
   ],
   "source": [
    "# Example text data\n",
    "example_texts = [\"This is an example sentence.\", \"a a, a, a, a, a, a, a\", \"b b b b b b b\"]\n",
    "\n",
    "padding_idx = 0\n",
    "\n",
    "# Loop through all examples, separated by a comma\n",
    "transformed_texts = [pad_sequence(text_transform([text])[0], max_len, padding_idx) for text in example_texts]\n",
    "\n",
    "# Convert to tensor and stack into a batch\n",
    "batch_tensor = torch.stack([torch.tensor(seq, dtype=torch.long) for seq in transformed_texts])\n",
    "\n",
    "print(batch_tensor)\n",
    "print(batch_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1beb384c-6a4a-423c-91fb-f3cb75fe9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    \"\"\" For a batch of tokens indices, randomly replace a non-specical token with <pad>.\n",
    "    prob (float): probability of dropping a token\n",
    "    pad_token (int): index for the <pad> token\n",
    "    num_special (int): Number of special tokens, assumed to be at the start of the vocab\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # Randomly sample a bernoulli distribution with p = prob\n",
    "        # Create a mask where 1 means we will replace that token\n",
    "        # Discrete probability distribution\n",
    "        # Here we want to treat the ones as the indexes to drop\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        \n",
    "        # Only replace if the token is not a special token\n",
    "        # Ones or zeros. If cannot drop, 0, if can drop, 1\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        # Multiply together to get the corresponding tokens to be dropped and not dropped\n",
    "        # Here, 1 represents drop, 0 represents do not drop\n",
    "        mask = mask * can_drop\n",
    "\n",
    "        # Make a mask of pad_token to use for replacing dropped indices with the pad_token\n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        # Sample is the original sample\n",
    "        # THe mask indicates what tokens can be replaced (0 to not be replaced, 1 to be replaced)\n",
    "        # Replace_with is a list of of the pad_token tokens\n",
    "        # Here, (1-mask) creates the complement mask. (now, 0 indicates drop, 1 indicates to not drop)\n",
    "        # 1-1 = 0, 1-0 = 0\n",
    "        # Multiplying by sample, retains the original tokens that are not to be kept, and applies the mask on the sample\n",
    "        # Here, mask * replace_with does elementwise multiplication and adds the corresponding pad_token to the tokens replaced\n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        \n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ec49e3c-8b79-4bf7-854c-f39fd7a9a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SinusoidalPosEmb, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the device of the input tensor\n",
    "        device = x.device\n",
    "        \n",
    "        # Calculate half of the hidden size\n",
    "        half_dim = self.hidden_size // 2\n",
    "        \n",
    "        # Compute the scaling factor\n",
    "        emb_scale = math.log(10000) / (half_dim - 1)\n",
    "        \n",
    "        # Generate the sinusoidal frequencies\n",
    "        emb_frequencies = torch.exp(torch.arange(half_dim, device=device) * -emb_scale)\n",
    "        \n",
    "        # Apply frequencies to the positions\n",
    "        emb = x[:, :, None] * emb_frequencies[None, None, :]\n",
    "        \n",
    "        # Concatenate sin and cos functions to form positional embeddings\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        \n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9cc2310-f2c2-4f54-820a-f29519b492c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        This class implements a simplified Transformer model for sequence classification. \n",
    "        It uses an embedding layer for tokens, sinusoidal positional embeddings, \n",
    "        a Transformer, and a Linear layer.\n",
    "        \n",
    "        num_emb: The number of unique tokens in the vocabulary. (vocab_size)\n",
    "        output_size: The size of the output layer (number of classes). (4)\n",
    "        hidden_size: The dimension of the hidden layer in the Transformer block (default: 128)\n",
    "        num_heads: The number of heads in the multi-head attention layer (default: 4).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_emb, output_size, hidden_size=128, num_heads=4):\n",
    "        \n",
    "        # Inherits from nn.Module's attributes\n",
    "        super(NanoTransformer, self).__init__()\n",
    "\n",
    "        # Create an embedding for each token\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size) # (vocab_size, 128)\n",
    "        \n",
    "        # Scaling down the embedding weights\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size) # (128)\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads = num_heads, batch_first = True)\n",
    "\n",
    "        # Linear layer\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size), # (batch_size, 128, 128)\n",
    "                                 nn.LayerNorm(hidden_size), # (batch_size, 128, 128)\n",
    "                                 nn.ELU(), # (batch_size, 128, 128)\n",
    "                                 nn.Linear(hidden_size, hidden_size)) # (batch_size, 128, 128)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, output_size) # (batch_size, 128, 128)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # batch_size, time_steps\n",
    "        batch_size, l = input_seq.shape # (32, 160)\n",
    "\n",
    "        input_embs = self.embedding(input_seq) # (32, 160) -> (32, 160, 128)\n",
    "        \n",
    "        # Add a unique embedding to each token embedding depending on it's position in the sequence\n",
    "        seq_indx = torch.arange(l) # (160)\n",
    "        \n",
    "        pos_emb = self.pos_emb(seq_indx.repeat(batch_size, 1)) # (1, 160, 128) -> (32, 160, 128)\n",
    "        \n",
    "        embs = input_embs + pos_emb # (32, 160, 128) + (32, 160, 128)\n",
    "        \n",
    "        output, attn_map = self.multihead_attn(embs, embs, embs) # (32, 160, 128)\n",
    "        \n",
    "        output = self.mlp(output) # (32, 160, 128)\n",
    "\n",
    "        return self.fc_out(output), attn_map # (32, 160, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "561a2785-752b-4f53-ba90-da82428e3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Embedding size\n",
    "hidden_size = 256\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Number of epochs for training\n",
    "nepochs = 20\n",
    "\n",
    "output_size = 4\n",
    "\n",
    "num_heads = 4\n",
    "\n",
    "# Create the Transformer model\n",
    "tf_classifier = NanoTransformer(num_emb=len(vocab), output_size=4, hidden_size = hidden_size, num_heads = 4).to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.Adam(tf_classifier.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Cosine annealing scheduler to decay the learning rate\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=nepochs, eta_min=0)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Custom transform to randomly replace a token with <pad>\n",
    "td = TokenDrop(prob=0.5)\n",
    "\n",
    "# Training and testing loss\n",
    "training_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "# Training and testing accuracy\n",
    "training_acc_list = []\n",
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60012da2-236e-46c0-b5ff-dc55a21257d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-This Model Has 5517060 (Approximately 5 Million) Parameters!\n"
     ]
    }
   ],
   "source": [
    "# Number of model parameters\n",
    "num_model_params = 0\n",
    "for param in tf_classifier.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56458b80-63f3-46d3-a2df-9a108c56ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: Train %.2f%%, Test %.2f%%' %(train_acc * 100, test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "297d8fdd-3d42-425f-9b0c-2d72d004b23b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Train 0.00%, Test 0.00%\n",
      "Accuracy: Train 0.00%, Test 0.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Transform the text to tokens\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m [pad_sequence(text_transform([text])[\u001b[38;5;241m0\u001b[39m], max_len, padding_idx) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m     21\u001b[0m batch_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mtensor(seq, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m transformed_texts])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[80], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Transform the text to tokens\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m transformed_texts \u001b[38;5;241m=\u001b[39m [pad_sequence(text_transform([text])[\u001b[38;5;241m0\u001b[39m], max_len, padding_idx) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m     21\u001b[0m batch_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mtensor(seq, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m transformed_texts])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchtext/transforms.py:1033\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m:param input: Input sequence or batch. The input type must be supported by the first transform in the sequence.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124;03m:type input: `Any`\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m-> 1033\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize training and testing accuracy lists\n",
    "training_acc_list = []\n",
    "test_acc_list = []\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "\n",
    "# Loop over each epoch\n",
    "for epoch in range(nepochs):\n",
    "    \n",
    "    tf_classifier.train()\n",
    "    train_steps = 0\n",
    "\n",
    "    print('Accuracy: Train %.2f%%, Test %.2f%%' %(train_acc * 100, test_acc * 100))\n",
    "    \n",
    "    # Loop over each batch in the training dataset\n",
    "    for batch_idx, (labels, texts) in enumerate(dataloader_train):\n",
    "        batch_size = labels.shape[0]\n",
    "        \n",
    "        # Transform the text to tokens\n",
    "        transformed_texts = [pad_sequence(text_transform([text])[0], max_len, padding_idx) for text in texts]\n",
    "        batch_tensor = torch.stack([torch.tensor(seq, dtype=torch.long) for seq in transformed_texts]).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # TokenDrop\n",
    "        batch_tensor = td(batch_tensor)\n",
    "        \n",
    "        # Get the model predictions\n",
    "        pred, _ = tf_classifier(batch_tensor)\n",
    "        \n",
    "        # Compute the loss using cross-entropy loss\n",
    "        loss = loss_fn(pred[:, 0, :], labels)\n",
    "        \n",
    "        # Backpropagation and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training loss\n",
    "        training_loss_list.append(loss.item())\n",
    "        \n",
    "        # Update training accuracy\n",
    "        train_acc += (pred[:, 0, :].argmax(1) == labels).sum().item()\n",
    "        train_steps += batch_size\n",
    "    \n",
    "    # Calculate average training accuracy\n",
    "    train_acc /= train_steps\n",
    "    training_acc_list.append(train_acc)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    tf_classifier.eval()\n",
    "    test_steps = 0\n",
    "    \n",
    "    # Loop over each batch in the testing dataset\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, texts) in enumerate(dataloader_test):\n",
    "            batch_size = labels.shape[0]\n",
    "            \n",
    "            # Transform the text to tokens\n",
    "            transformed_texts = [pad_sequence(text_transform([text])[0], max_len, padding_idx) for text in texts]\n",
    "            batch_tensor = torch.stack([torch.tensor(seq, dtype=torch.long) for seq in transformed_texts]).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get the model predictions\n",
    "            pred, _ = tf_classifier(batch_tensor)\n",
    "            \n",
    "            # Compute the loss using cross-entropy loss\n",
    "            loss = loss_fn(pred[:, 0, :], labels)\n",
    "            test_loss_list.append(loss.item())\n",
    "            \n",
    "            # Update testing accuracy\n",
    "            test_acc += (pred[:, 0, :].argmax(1) == labels).sum().item()\n",
    "            test_steps += batch_size\n",
    "        \n",
    "        # Calculate average testing accuracy\n",
    "        test_acc /= test_steps\n",
    "        test_acc_list.append(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14eb779-05ff-4031-8590-d41a8ee68a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_loss_list)), training_loss_list)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_loss_list)), test_loss_list)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0881a29-93c0-4ce4-bb5b-e006cabe3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_acc_list)), training_acc_list)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_acc_list)), test_acc_list)\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Accuracy\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Accuracy\")\n",
    "print(\"Max Test Accuracy %.2f%%\" % (np.max(test_acc_list) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34fda3-ca89-4375-ad66-5ff5bb0f103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_classes = [\n",
    "    \"World\",\n",
    "    \"Sports\",\n",
    "    \"Business\",\n",
    "    \"Science/Technology\"\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    label, text = next(iter(dataloader_train))\n",
    "    text_tokens = text_transform(list(text)).to(device)\n",
    "    pred, attention_map = tf_classifier(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3115b-5ed4-41da-ada1-f7672290b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 3\n",
    "\n",
    "# Choose a text index smaller than the total number in the batch!\n",
    "assert test_index < label.shape[0]\n",
    "\n",
    "# Select the attention map for a single sample and the first attention head\n",
    "att_map = attention_map[test_index, 0]\n",
    "pred_class = ag_news_classes[pred[test_index, -1].argmax().item()]\n",
    "top5 = att_map.argsort(descending=True)[:5]\n",
    "top5_tokens = vocab.lookup_tokens(text_tokens[test_index, top5].cpu().numpy())\n",
    "\n",
    "\n",
    "print(\"Article:\")\n",
    "print(text[test_index])\n",
    "print(\"\\nPredicted label:\")\n",
    "print(pred_class)\n",
    "print(\"True label:\")\n",
    "print(ag_news_classes[label[test_index].item()])\n",
    "print(\"\\nTop 5 Tokens:\")\n",
    "print(top5_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185764d-21f2-41db-b5bd-3fff6d0f712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(att_map.cpu().numpy())\n",
    "_ = plt.title(\"Attention score per token\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
