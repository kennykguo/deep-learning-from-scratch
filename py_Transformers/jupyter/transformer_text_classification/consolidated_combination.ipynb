{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b7de76-3109-41eb-a2ed-ceb684df6b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torchtext/transforms.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torchtext/functional.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.functional import generate_sp_model, load_sp_model, sentencepiece_tokenizer, sentencepiece_numericalizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f66ca2e-7e31-43e0-b7f7-2d7eb265c9f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=data/data.txt --model_prefix=SentencePiece/transformer --vocab_size=20000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/data.txt\n",
      "  input_format: \n",
      "  model_prefix: SentencePiece/transformer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: data/data.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 120000 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=28222889\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9756% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=48\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999756\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 120000 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 152602 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 120000\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 120631\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 120631 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=62443 obj=11.2717 num_tokens=276733 num_tokens/piece=4.43177\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=49790 obj=9.0969 num_tokens=277388 num_tokens/piece=5.57116\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=37337 obj=9.0515 num_tokens=288412 num_tokens/piece=7.72456\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=37324 obj=9.03806 num_tokens=288579 num_tokens/piece=7.73173\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=27993 obj=9.07602 num_tokens=309208 num_tokens/piece=11.0459\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=27993 obj=9.06489 num_tokens=309177 num_tokens/piece=11.0448\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=22000 obj=9.12194 num_tokens=328649 num_tokens/piece=14.9386\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=21999 obj=9.10707 num_tokens=328625 num_tokens/piece=14.9382\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: SentencePiece/transformer.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: SentencePiece/transformer.vocab\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data preprocessing\n",
    "root = \"data\"\n",
    "with open(os.path.join(root, \"train.csv\")) as f:\n",
    "    with open(os.path.join(root, \"data.txt\"), \"w\") as f2:\n",
    "        for line in f:\n",
    "            text_only = \"\".join(line.split(\",\")[1:])\n",
    "            filtered = re.sub(r'\\\\|\\\\n|;', ' ', text_only.replace('\"', ' ').replace('\\n', ' '))\n",
    "            filtered = filtered.replace(' #39;', \"'\")\n",
    "            filtered = filtered.replace(' #38;', \"&\")\n",
    "            filtered = filtered.replace(' #36;', \"$\")\n",
    "            filtered = filtered.replace(' #151;', \"-\")\n",
    "            f2.write(filtered.lower() + \"\\n\")\n",
    "\n",
    "generate_sp_model(os.path.join(root, \"data.txt\"), vocab_size=20000, model_prefix='SentencePiece/transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53cc1198-01d4-4979-8bd7-3565a865822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class AGNews(Dataset):\n",
    "    def __init__(self, num_datapoints, set=\"train\"):\n",
    "        self.df = pd.read_csv(os.path.join(root, set + \".csv\"), names=[\"Class\", \"Title\", \"Content\"])\n",
    "        self.df.fillna('', inplace=True)\n",
    "        self.df['Article'] = self.df['Title'] + \" : \" + self.df['Content']\n",
    "        self.df.drop(['Title', 'Content'], axis=1, inplace=True)\n",
    "        self.df['Article'] = self.df['Article'].str.replace(r'\\\\n|\\\\|\\\\r|\\\\r\\\\n|\\n|\"', ' ', regex=True)\n",
    "        self.df['Article'] = self.df['Article'].replace({' #39;': \"'\", ' #38;': \"&\", ' #36;': \"$\", ' #151;': \"-\"}, regex=True)\n",
    "        if num_datapoints is not None:\n",
    "            self.df = self.df.head(num_datapoints)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.loc[index][\"Article\"].lower()\n",
    "        class_index = int(self.df.loc[index][\"Class\"]) - 1\n",
    "        return class_index, text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "train_dataset = AGNews(num_datapoints=12000, set=\"train\")\n",
    "test_dataset = AGNews(num_datapoints=10000, set=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e07dbd6-40fc-4ce3-9e2d-7ab90db86fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31447274-68b3-424e-9c45-04d825546d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield [line.split(\"\\t\")[0]]\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\"SentencePiece/transformer.vocab\"), specials=['<pad>', '<sos>', '<eos>', '<unk>'], special_first=True)\n",
    "vocab.set_default_index(vocab['<unk>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1838e8c-b600-45c5-b4dd-afe62b1ef8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform = T.Sequential(\n",
    "    T.SentencePieceTokenizer(\"SentencePiece/transformer.model\"),\n",
    "    T.VocabTransform(vocab=vocab),\n",
    "    T.AddToken(vocab['<sos>'], begin=True),\n",
    "    T.Truncate(max_seq_len=max_len),\n",
    "    T.AddToken(vocab['<eos>'], begin=False),\n",
    "    T.ToTensor(padding_value=vocab['<pad>'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8bb072da-0781-42cf-8cb5-22845b6ebb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDrop(nn.Module):\n",
    "    def __init__(self, prob=0.1, pad_token=0, num_special=4):\n",
    "        super().__init__()\n",
    "        self.prob = prob\n",
    "        self.num_special = num_special\n",
    "        self.pad_token = pad_token\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
    "        can_drop = (sample >= self.num_special).long()\n",
    "        mask = mask * can_drop\n",
    "        replace_with = (self.pad_token * torch.ones_like(sample)).long()\n",
    "        sample_out = (1 - mask) * sample + mask * replace_with\n",
    "        return sample_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e06f792-d838-42ab-b2fd-3b9013e2ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SinusoidalPosEmb, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.hidden_size // 2\n",
    "        emb_scale = math.log(10000) / (half_dim - 1)\n",
    "        emb_frequencies = torch.exp(torch.arange(half_dim, device=device) * -emb_scale)\n",
    "        emb = x[:, None] * emb_frequencies[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3a4aab4-dcd1-4a0e-bffe-b60eb46ff6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoTransformer(nn.Module):\n",
    "    def __init__(self, num_emb, output_size, hidden_size=128, num_heads=4):\n",
    "        super(NanoTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                                 nn.LayerNorm(hidden_size),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(hidden_size, hidden_size))\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size, l = input_seq.shape\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, -1).expand(batch_size, l, -1)\n",
    "        embs = input_embs + pos_emb\n",
    "        output, attn_map = self.multihead_attn(embs, embs, embs)\n",
    "        output = self.mlp(output)\n",
    "        return self.fc_out(output), attn_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a46c5c70-4207-4208-87e1-0ad14dbacd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Model Has 5517060 (Approximately 5.0 Million) Parameters!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "hidden_size = 256\n",
    "learning_rate = 1e-4\n",
    "nepochs = 20\n",
    "batch_size = 128\n",
    "max_len = 128\n",
    "output_size = 4\n",
    "num_heads = 4\n",
    "\n",
    "tf_classifier = NanoTransformer(num_emb=len(vocab), output_size=4, hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "optimizer = optim.Adam(tf_classifier.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=nepochs, eta_min=0)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "td = TokenDrop(prob=0.5)\n",
    "\n",
    "training_loss_list = []\n",
    "test_loss_list = []\n",
    "training_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "num_model_params = sum(p.numel() for p in tf_classifier.parameters())\n",
    "print(f\"This Model Has {num_model_params} (Approximately {num_model_params // 1e6} Million) Parameters!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e610da1-f112-4894-9cb1-d37a7ec8d8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "Epoch 1/20\n",
      "Training Accuracy: 25.87%\n",
      "Testing Accuracy: 24.38%\n",
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m batch_tensor \u001b[38;5;241m=\u001b[39m td(batch_tensor)\n\u001b[0;32m---> 12\u001b[0m pred, _ \u001b[38;5;241m=\u001b[39m tf_classifier(batch_tensor)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred[:, \u001b[38;5;241m0\u001b[39m, :], labels)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 20\u001b[0m, in \u001b[0;36mNanoTransformer.forward\u001b[0;34m(self, input_seq)\u001b[0m\n\u001b[1;32m     18\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb(seq_indx)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, l, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, l, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m embs \u001b[38;5;241m=\u001b[39m input_embs \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[0;32m---> 20\u001b[0m output, attn_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attn(embs, embs, embs)\n\u001b[1;32m     21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(output)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output), attn_map\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py:1266\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1253\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1264\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1267\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1270\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1271\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1272\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1273\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1274\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1275\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1276\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:5483\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5481\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m attn_output_weights\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, tgt_len, src_len)\n\u001b[1;32m   5482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average_attn_weights:\n\u001b[0;32m-> 5483\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m attn_output_weights\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   5486\u001b[0m     \u001b[38;5;66;03m# squeeze the output if input was unbatched\u001b[39;00m\n\u001b[1;32m   5487\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(nepochs):\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    tf_classifier.train()\n",
    "    train_steps = 0\n",
    "    for batch_idx, (labels, texts) in enumerate(dataloader_train):\n",
    "        batch_size = labels.shape[0]\n",
    "        batch_tensor = text_transform(list(texts)).to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_tensor = td(batch_tensor)\n",
    "        pred, _ = tf_classifier(batch_tensor)\n",
    "        loss = loss_fn(pred[:, 0, :], labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss_list.append(loss.item())\n",
    "        train_acc += (pred[:, 0, :].argmax(1) == labels).sum().item()\n",
    "        train_steps += batch_size\n",
    "\n",
    "        if (batch_idx % 40 == 0):\n",
    "            print(\"hello\")\n",
    "            \n",
    "    train_acc /= train_steps\n",
    "    training_acc_list.append(train_acc)\n",
    "    lr_scheduler.step()\n",
    "    tf_classifier.eval()\n",
    "    test_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, texts) in enumerate(dataloader_test):\n",
    "            batch_size = labels.shape[0]\n",
    "            batch_tensor = text_transform(list(texts)).to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred, _ = tf_classifier(batch_tensor)\n",
    "            loss = loss_fn(pred[:, 0, :], labels)\n",
    "            test_loss_list.append(loss.item())\n",
    "            test_acc += (pred[:, 0, :].argmax(1) == labels).sum().item()\n",
    "            test_steps += batch_size\n",
    "    test_acc /= test_steps\n",
    "    test_acc_list.append(test_acc)\n",
    "    \n",
    "    # Print out the results for this epoch\n",
    "    print(f'Epoch {epoch+1}/{nepochs}')\n",
    "    print(f'Training Accuracy: {train_acc*100:.2f}%')\n",
    "    print(f'Testing Accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb62b79-9f19-4a82-b5b1-079b0acdb762",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_loss_list)), training_loss_list)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_loss_list)), test_loss_list)\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Loss\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")\n",
    "\n",
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(training_acc_list)), training_acc_list)\n",
    "_ = plt.plot(np.linspace(0, nepochs, len(test_acc_list)), test_acc_list)\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "_ = plt.title(\"Training Vs Test Accuracy\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Accuracy\")\n",
    "print(\"Max Test Accuracy %.2f%%\" % (np.max(test_acc_list) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
