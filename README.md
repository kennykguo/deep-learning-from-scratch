# Neural network implementation using only Python, NumPy, and Calculus

The mathematics behind backpropogation were learned through these resources:

https://www.youtube.com/watch?time_continue=13581&v=Ixl3nykKG9M&embeds_referring_euri=https%3A%2F%2Fadamdhalla.com%2F&source_ve_path=MjM4NTE&feature=emb_title&ab_channel=AdamDhalla

http://neuralnetworksanddeeplearning.com/

The neural network with the ReLU activation function reached an accuracy of 85%, with 2 layers of 10 neurons. It may be improved through adding layers, adjusting learning rates, using different activation functions, and other finetuning techniques.

The neural network with the sigmoid activation function utilized stochastic gradient descent, and reached an accuracy of 96%, over the validation set.