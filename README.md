# Neural network implementation using only Python, NumPy, and Calculus

The mathematics behind backpropogation were learned through these resources:

https://www.youtube.com/watch?time_continue=13581&v=Ixl3nykKG9M&embeds_referring_euri=https%3A%2F%2Fadamdhalla.com%2F&source_ve_path=MjM4NTE&feature=emb_title&ab_channel=AdamDhalla

http://neuralnetworksanddeeplearning.com/

https://arxiv.org/pdf/1802.01528

The neural network with the ReLU activation function reached a maximum accuracy of 85%, with 2 layers of 10 neurons. It may be improved through adding layers, adjusting learning rates, using different activation functions, and other finetuning techniques. You may also improve its efficiency by implementing stochastic gradient descent, instead of backpropogating over all 41,000 training examples.

The neural network with the sigmoid activation function and stochastic gradient descent reached an accuracy of 96%, over the validation set. 