# Neural network implementation using only Python, NumPy, and Calculus

The mathematics behind backpropogation were learned in this lecture: https://www.youtube.com/watch?time_continue=13581&v=Ixl3nykKG9M&embeds_referring_euri=https%3A%2F%2Fadamdhalla.com%2F&source_ve_path=MjM4NTE&feature=emb_title&ab_channel=AdamDhalla

All the necessary concepts to understand the mathematics behind backpropogation are summarized in the pdf file, which are notes taken from the lecture.

The implementation of the neural network can be found in the .ipynb file, and to use file, run all the cells from top to bottom. 

The neural network reached an accuracy of 85%, with 2 layers of 10 neurons. It may be improved by adding one more layer of neurons, increasing the training examples by adding more data, and adjusting the learning rate.

This project involves Samson Zhang's test cases, which can be found within this video: https://www.youtube.com/watch?v=w8yWXqWQYmU&ab_channel=SamsonZhang.
