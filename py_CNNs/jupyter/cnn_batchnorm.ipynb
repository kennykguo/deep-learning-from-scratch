{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f74cad-a18b-4772-a59a-074086702076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "data = pd.read_csv('../../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0624881d-5967-451c-a759-0bc6805e5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 40000)\n",
      "(40000,)\n",
      "(28, 28, 1000)\n",
      "(1000,)\n",
      "(28, 28, 1000)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "\n",
    "np.random.shuffle(data) # Shuffles all the individual rows\n",
    "\n",
    "data_dev = data[0:1000].T #Take the first 1000 rows, and transpose the matrix to get 1000 examples as column vectors\n",
    "Y_dev = data_dev[0] #Takes the first row, which contains all of the answers to the numbers (the Y is what we want)\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev.reshape(28,28,1000)\n",
    "\n",
    "data_train = data[2000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train= data_train[1:n] #Takes all of the data corresponding to all of the entries\n",
    "X_train = X_train.reshape(28,28,-1)\n",
    "\n",
    "data_test = data[1000:2000].T\n",
    "Y_test = data_test[0]\n",
    "X_test= data_test[1:n] #Takes all of the data corresponding to all of the entries\n",
    "X_test = X_test.reshape(28,28,-1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5864e0c-acf3-4def-b894-6913bcc0d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_forward(x, gamma, beta, eps=1e-5):\n",
    "    mean = np.mean(x, axis=0)\n",
    "    variance = np.var(x, axis=0)\n",
    "    x_normalized = (x - mean) / np.sqrt(variance + eps)\n",
    "    out = gamma * x_normalized + beta\n",
    "    cache = (x, x_normalized, mean, variance, gamma, beta, eps)\n",
    "    return out, cache\n",
    "\n",
    "def batch_norm_backward(dout, cache):\n",
    "    x, x_normalized, mean, variance, gamma, beta, eps = cache\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgamma = np.sum(dout * x_normalized, axis=0)\n",
    "    \n",
    "    dx_normalized = dout * gamma\n",
    "    dvariance = np.sum(dx_normalized * (x - mean) * -0.5 * np.power(variance + eps, -1.5), axis=0)\n",
    "    dmean = np.sum(dx_normalized * -1 / np.sqrt(variance + eps), axis=0) + dvariance * np.sum(-2 * (x - mean), axis=0) / N\n",
    "    \n",
    "    dx = dx_normalized / np.sqrt(variance + eps) + dvariance * 2 * (x - mean) / N + dmean / N\n",
    "    return dx, dgamma, dbeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fb5c539-b2e6-4cea-ab1d-9f7580cd0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(input_data):\n",
    "    # 24, 24, 2\n",
    "    input_height, input_width, input_depth = input_data.shape\n",
    "\n",
    "    # Calculate the output dimensions\n",
    "    output_height = input_height // 2 # 12\n",
    "    output_width = input_width // 2 # 12\n",
    "    output_depth = input_depth # 2 - depth stays the same\n",
    "\n",
    "    # Initialize the output array and array to store indices\n",
    "    output_data = np.zeros((output_height, output_width, output_depth))\n",
    "    indices = np.zeros((output_height, output_width, output_depth, 2), dtype=int)\n",
    "\n",
    "    # Apply max pooling\n",
    "    for h in range(output_height):\n",
    "        for w in range(output_width):\n",
    "            for d in range(output_depth):\n",
    "                # Extract the 2x2 region of interest from the input data\n",
    "                region = input_data[h*2:(h+1)*2, w*2:(w+1)*2, d]\n",
    "                # Compute the maximum value in the region\n",
    "                max_val = np.max(region)\n",
    "                output_data[h, w, d] = max_val\n",
    "                # Find the indices of the maximum value in the region\n",
    "                max_indices = np.unravel_index(np.argmax(region), region.shape)\n",
    "                # Store the indices relative to the region and convert to global indices\n",
    "                indices[h, w, d] = [h*2 + max_indices[0], w*2 + max_indices[1]]\n",
    "\n",
    "    return output_data, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec77b82a-5105-48bd-bba6-eea0ae38b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_ReLU(Z):\n",
    "  return Z > 0\n",
    "\n",
    "def ReLU2(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "def ReLU(Z): # Takes in a scalar, returns a scalar\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU2(Z): # Takes in a scalar, returns a scalar\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "  return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Compute the sigmoid function element-wise\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def softmax(Z):\n",
    "    # Apply softmax column-wise\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0))  # Subtracting the maximum value in each column to avoid overflow\n",
    "    return exp_Z / np.sum(exp_Z, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce95ada-3428-48ba-9156-6bcb650d8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params():\n",
    "    layer_weights = np.random.randn(5, 5, 2) * np.sqrt(2. / 5)\n",
    "    layer_bias = np.zeros((24, 24, 2))\n",
    "    layer_output = np.zeros((24, 24, 2)) #(24, 24, 2)\n",
    "    fc_weights = np.random.randn(10, 288) * np.sqrt(2. / 288)\n",
    "    fc_bias = np.zeros((10, 1))\n",
    "    gamma_conv = np.ones((24, 24, 2))\n",
    "    beta_conv = np.zeros((24, 24, 2))\n",
    "    gamma_fc = np.ones((10, 1))\n",
    "    beta_fc = np.zeros((10, 1))\n",
    "    return layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "\n",
    "\n",
    "def forward_propagation(layer_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc, dropout_rate=0.5):\n",
    "    # Convolution\n",
    "    for i in range(2): # 2 filters in total\n",
    "        layer_output[:,:,i] = signal.correlate2d(layer_input, layer_weights[:,:,i], mode='valid')\n",
    "    layer_output = layer_output + layer_bias   # (24, 24, 2)\n",
    "    \n",
    "    # Batch Normalization for Convolutional Layer\n",
    "    layer_output, bn_cache_conv = batch_norm_forward(layer_output, gamma_conv, beta_conv)\n",
    "    \n",
    "    # Activation layer\n",
    "    layer_output = ReLU(layer_output)  # (24, 24, 2)\n",
    "    \n",
    "    # Pool layer\n",
    "    layer_pool, layer_indices = max_pooling(layer_output)  # (12, 12, 2)\n",
    "    \n",
    "    # Flattening\n",
    "    layer_pool = layer_pool.reshape(288, 1) # (288, 1)\n",
    "    \n",
    "    # Dropout\n",
    "    dropout_mask = (np.random.rand(*layer_pool.shape) < dropout_rate) / dropout_rate\n",
    "    layer_pool *= dropout_mask\n",
    "    \n",
    "    # Fully connected layer\n",
    "    final_output = fc_weights.dot(layer_pool)  # (10, 288) (288, 1) = (10, 1)\n",
    "    final_output = final_output + fc_bias # (10, 1) + (10, 1) = (10, 1)\n",
    "    \n",
    "    # Batch Normalization for Fully Connected Layer\n",
    "    final_output, bn_cache_fc = batch_norm_forward(final_output, gamma_fc, beta_fc)\n",
    "    \n",
    "    final_output = softmax(final_output) # (10, 1)\n",
    "    \n",
    "    return layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask\n",
    "\n",
    "\n",
    "def create(Y):\n",
    "  column_Y = np.zeros((10, 1))\n",
    "  column_Y[Y] = 1\n",
    "  column_Y = column_Y.T\n",
    "  return column_Y.reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea97843b-5871-4db2-bc7d-1baae6ec752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(layer_input, layer_output, layer_pool, layer_indices, final_output, label, layer_weights, layer_bias, fc_weights, fc_bias, bn_cache_conv, bn_cache_fc, dropout_mask):\n",
    "    # Initialize parameters\n",
    "    delta_conv_weights = np.zeros_like(layer_weights)\n",
    "    delta_conv_bias = np.zeros_like(layer_bias)\n",
    "    delta_fc_weights = np.zeros_like(fc_weights)\n",
    "    delta_fc_bias = np.zeros_like(fc_bias)\n",
    "    delta_fc_bias = delta_fc_bias.reshape(10, 1)\n",
    "    \n",
    "    # Backpropagate cost\n",
    "    x = create(label)\n",
    "    dZ = (final_output - x)  # (10, 1) - (10, 1) = (10, 1)\n",
    "    \n",
    "    # Backpropagate through Batch Normalization for Fully Connected Layer\n",
    "    dZ, dgamma_fc, dbeta_fc = batch_norm_backward(dZ, bn_cache_fc)\n",
    "    \n",
    "    # Backpropagate weights and biases for Fully Connected Layer\n",
    "    delta_fc_weights = dZ.dot(layer_pool.T)  # (10, 1) (1, 288) = (10, 288)\n",
    "    delta_fc_bias = dZ\n",
    "    \n",
    "    # Backpropagate error\n",
    "    dZ_pool_output = np.dot(fc_weights.T, dZ) * der_ReLU(layer_pool)  # (288, 10) (10, 1) = (288, 1)\n",
    "    \n",
    "    # Undo Dropout\n",
    "    dZ_pool_output *= dropout_mask\n",
    "    \n",
    "    # Unflattening\n",
    "    dZ_pool_output = dZ_pool_output.reshape(12, 12, 2)\n",
    "    \n",
    "    # Unpooling\n",
    "    dZ_pool_input = np.zeros((24, 24, 2))\n",
    "    for i in range(12):  # height\n",
    "        for j in range(12):  # width\n",
    "            for k in range(2):  # depth\n",
    "                # Get the global indices from layer_indices\n",
    "                x_index, y_index = layer_indices[i, j, k]\n",
    "                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n",
    "                dZ_pool_input[x_index, y_index, k] = dZ_pool_output[i, j, k]\n",
    "    \n",
    "    # Backpropagate through ReLU activation\n",
    "    dZ_pool_input *= der_ReLU(layer_output)\n",
    "    \n",
    "    # Backpropagate through Batch Normalization\n",
    "    dZ_pool_input, dgamma_conv, dbeta_conv = batch_norm_backward(dZ_pool_input, bn_cache_conv)\n",
    "    \n",
    "    # Backpropagate Conv layer\n",
    "    for i in range(2):  # For each filter in the kernel\n",
    "        delta_conv_weights[:, :, i] = signal.correlate(layer_input, dZ_pool_input[:, :, i], mode=\"valid\")\n",
    "    delta_conv_bias = dZ_pool_input\n",
    "    \n",
    "    return delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias, dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc\n",
    "\n",
    " \n",
    "def update_params(layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc,\n",
    "                  delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias,\n",
    "                  dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc, learning_rate):\n",
    "    layer_weights -= learning_rate * delta_conv_weights\n",
    "    layer_bias -= learning_rate * delta_conv_bias\n",
    "    fc_weights -= learning_rate * delta_fc_weights\n",
    "    fc_bias -= learning_rate * delta_fc_bias\n",
    "    gamma_conv -= learning_rate * dgamma_conv\n",
    "    beta_conv -= learning_rate * dbeta_conv\n",
    "    gamma_fc -= learning_rate * dgamma_fc\n",
    "    beta_fc -= learning_rate * dbeta_fc\n",
    "    return layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "\n",
    "def get_prediction(A2):\n",
    "  return np.argmax(A2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82019a37-ccc9-4530-86e4-4ef9272aaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X_train, X_dev, Y_train, Y_dev, epochs, learning_rate, batch_size):\n",
    "    # Initialize parameters\n",
    "    layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = params()\n",
    "    gamma_conv = np.ones((24, 24, 2))\n",
    "    beta_conv = np.zeros((24, 24, 2))\n",
    "    gamma_fc = np.ones((10, 1))\n",
    "    beta_fc = np.zeros((10, 1))\n",
    "    \n",
    "    num_examples = X_train.shape[2]\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch:\", i + 1)\n",
    "        \n",
    "        # Generate a random permutation of indices\n",
    "        permuted_indices = np.random.permutation(X_train.shape[2])\n",
    "        \n",
    "        # Shuffle both X_train and Y_train using the same permutation of indices\n",
    "        X_train_shuffled = X_train[:, :, permuted_indices]\n",
    "        Y_train_shuffled = Y_train[permuted_indices]\n",
    "        \n",
    "        for batch_start in range(0, int(X_train_shuffled.shape[2]/100), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, num_examples)\n",
    "            batch_gradients = [0, 0, 0, 0, 0, 0, 0, 0]  # Accumulate gradients over the batch\n",
    "            for j in range(batch_start, batch_end):\n",
    "                # Get a single training example\n",
    "                layer_input = X_train_shuffled[:, :, j]\n",
    "                label = Y_train_shuffled[j]\n",
    "                \n",
    "                # Forward propagation\n",
    "                layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask = forward_propagation(\n",
    "                    layer_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "                )\n",
    "                \n",
    "                # Back propagation\n",
    "                delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias, dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc = back_prop(\n",
    "                    layer_input, layer_output, layer_pool, layer_indices, final_output, label,\n",
    "                    layer_weights, layer_bias, fc_weights, fc_bias, bn_cache_conv, bn_cache_fc, dropout_mask\n",
    "                )\n",
    "                \n",
    "                # Accumulate gradients\n",
    "                batch_gradients[0] += delta_conv_weights\n",
    "                batch_gradients[1] += delta_conv_bias\n",
    "                batch_gradients[2] += delta_fc_weights\n",
    "                batch_gradients[3] += delta_fc_bias\n",
    "                batch_gradients[4] += dgamma_conv\n",
    "                batch_gradients[5] += dbeta_conv\n",
    "                batch_gradients[6] += dgamma_fc\n",
    "                batch_gradients[7] += dbeta_fc\n",
    "            \n",
    "            # Average gradients after processing the batch\n",
    "            batch_gradients = [grad / batch_size for grad in batch_gradients]\n",
    "            \n",
    "            # Update parameters after processing the batch\n",
    "            layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = update_params(\n",
    "                layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc,\n",
    "                *batch_gradients,  # Use averaged gradients\n",
    "                learning_rate\n",
    "            )\n",
    "        \n",
    "        # Get training accuracy\n",
    "        counter = 0\n",
    "        for j in range(int(X_train_shuffled.shape[2]/100)):\n",
    "            test_input = X_train_shuffled[:, :, j]\n",
    "            layer_output, layer_pool, layer_indices, final_output, _, _, _ = forward_propagation(\n",
    "                test_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "            )\n",
    "            prediction = get_prediction(final_output)\n",
    "            predicted_label = prediction[0]\n",
    "            if Y_train_shuffled[j] == predicted_label:\n",
    "                counter += 1\n",
    "        print(\"Training Accuracy:\", counter / int(X_train_shuffled.shape[2]/100))\n",
    "        counter = 0\n",
    "        for j in range(500):\n",
    "            test_input = X_dev[:, :, j]\n",
    "            layer_output, layer_pool, layer_indices, final_output, _, _, _ = forward_propagation(\n",
    "                test_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "            )\n",
    "            prediction = get_prediction(final_output)\n",
    "            predicted_label = prediction[0]\n",
    "            if Y_dev[j] == predicted_label:\n",
    "                counter += 1\n",
    "        print(\"Validation Accuracy:\", counter / 500)\n",
    "    return layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88043d28-1660-42f8-b569-968412d07f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Accuracy: 0.33\n",
      "Validation Accuracy: 0.326\n",
      "Epoch: 2\n",
      "Training Accuracy: 0.455\n",
      "Validation Accuracy: 0.374\n",
      "Epoch: 3\n",
      "Training Accuracy: 0.5975\n",
      "Validation Accuracy: 0.574\n",
      "Epoch: 4\n",
      "Training Accuracy: 0.6625\n",
      "Validation Accuracy: 0.62\n",
      "Epoch: 5\n",
      "Training Accuracy: 0.7025\n",
      "Validation Accuracy: 0.69\n",
      "Epoch: 6\n",
      "Training Accuracy: 0.725\n",
      "Validation Accuracy: 0.744\n",
      "Epoch: 7\n",
      "Training Accuracy: 0.7725\n",
      "Validation Accuracy: 0.764\n",
      "Epoch: 8\n",
      "Training Accuracy: 0.7475\n",
      "Validation Accuracy: 0.768\n",
      "Epoch: 9\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.79\n",
      "Epoch: 10\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.786\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = stochastic_gradient_descent(\n",
    "    X_train, X_dev, Y_train, Y_dev, 10, 0.2, 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df4f1a6-6227-4556-8395-3a9d1a2d20e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"layer weights\", layer_weights)\n",
    "# print(\"layer bias\", layer_bias)\n",
    "# print(\"layer output\", layer_output)\n",
    "# print(\"fc weights\", fc_weights)\n",
    "# print(\"fc_bias\", fc_bias)\n",
    "plt.imshow(layer_output[:,:,1], cmap='gray')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()\n",
    "plt.imshow(layer_output[:,:,0], cmap='gray')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e98a1531-056b-41dd-9bb5-550c39b421fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.7 %\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in range(1000):\n",
    "    test = X_dev[:,:,i]\n",
    "    label = Y_dev[i]\n",
    "    # print(label)\n",
    "    # plt.imshow(test, cmap='gray')\n",
    "    # plt.axis('off')  # Turn off axis\n",
    "    # plt.show()\n",
    "    # print(\"Testing model:\")\n",
    "    _, _, _, final_output, _, _, _ = forward_propagation(\n",
    "                    test, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "                )\n",
    "    # print(\"Model prediction: \")\n",
    "    prediction = get_prediction(final_output)\n",
    "    if (prediction == label):\n",
    "        counter += 1\n",
    "print(\"Accuracy:\", counter / 10,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
