{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51874045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:10:05.026499Z",
     "iopub.status.busy": "2024-06-03T04:10:05.025839Z",
     "iopub.status.idle": "2024-06-03T04:10:05.034184Z",
     "shell.execute_reply": "2024-06-03T04:10:05.033239Z",
     "shell.execute_reply.started": "2024-06-03T04:10:05.026466Z"
    }
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88415812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:10:06.078553Z",
     "iopub.status.busy": "2024-06-03T04:10:06.077779Z",
     "iopub.status.idle": "2024-06-03T04:10:07.561556Z",
     "shell.execute_reply": "2024-06-03T04:10:07.560499Z",
     "shell.execute_reply.started": "2024-06-03T04:10:06.078517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array on GPU:\n",
      "[[ 1.41887935 -1.24628621 -1.5654162 ]\n",
      " [ 0.29132549 -2.01655029 -0.248686  ]\n",
      " [-1.05822349 -0.43371707 -0.71966692]]\n",
      "\n",
      "Result array on GPU:\n",
      "[[ 1.1398161  -0.62896253 -0.99460543]\n",
      " [ 1.24508613 -1.33342415  0.72310608]\n",
      " [-0.38106456  0.48716333  0.09289109]]\n",
      "\n",
      "Result array on CPU:\n",
      "[[ 1.1398161  -0.62896253 -0.99460543]\n",
      " [ 1.24508613 -1.33342415  0.72310608]\n",
      " [-0.38106456  0.48716333  0.09289109]]\n"
     ]
    }
   ],
   "source": [
    "# Generate a random array on GPU\n",
    "x_gpu = cp.random.randn(3, 3)\n",
    "\n",
    "# Perform some operations on the array\n",
    "y_gpu = cp.sin(x_gpu) + cp.cos(x_gpu)\n",
    "\n",
    "# Transfer the result back to CPU\n",
    "y_cpu = cp.asnumpy(y_gpu)\n",
    "\n",
    "# Print the result\n",
    "print(\"Original array on GPU:\")\n",
    "print(x_gpu)\n",
    "print(\"\\nResult array on GPU:\")\n",
    "print(y_gpu)\n",
    "print(\"\\nResult array on CPU:\")\n",
    "print(y_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = '/kaggle/input/training/dog.3.jpg' \n",
    "# image = Image.open(image_path)\n",
    "\n",
    "# # Resize the image\n",
    "# resized_image = image.resize((224, 224))\n",
    "\n",
    "# # If the image is not in RGB mode, convert it to RGB\n",
    "# if resized_image.mode != 'RGB':\n",
    "#     resized_image = resized_image.convert('RGB')\n",
    "\n",
    "# # Convert the resized image to a numpy array\n",
    "# image_array = np.array(resized_image)\n",
    "\n",
    "# # Display the resized image\n",
    "# plt.imshow(image_array)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5251e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:12:12.545634Z",
     "iopub.status.busy": "2024-06-03T04:12:12.545202Z",
     "iopub.status.idle": "2024-06-03T04:12:32.547962Z",
     "shell.execute_reply": "2024-06-03T04:12:32.546940Z",
     "shell.execute_reply.started": "2024-06-03T04:12:12.545598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3200\n",
      "Validation set size: 800\n",
      "Example image shape: (224, 224, 3)\n",
      "Example label: [[1 0]]\n"
     ]
    }
   ],
   "source": [
    "dogs_num = 2000\n",
    "cats_num = 2000\n",
    "image_size = (224, 224)\n",
    "train_dir = '/kaggle/input/training'\n",
    "validation_split = 0.2\n",
    "\n",
    "# Function to load and process images\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    resized_image = image.resize(image_size)\n",
    "    if resized_image.mode != 'RGB':\n",
    "        resized_image = resized_image.convert('RGB')\n",
    "    image_array = cp.array(resized_image)  # Convert to CuPy array\n",
    "    return image_array\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(dir_path, dogs_num, cats_num):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(dogs_num):\n",
    "        image_path = os.path.join(dir_path, f'dog.{i}.jpg')\n",
    "        image_array = load_image(image_path)\n",
    "        label = cp.array([[1, 0]])  # Dog label, converted to CuPy array\n",
    "        data.append((image_array, label))\n",
    "        \n",
    "    for i in range(cats_num):\n",
    "        image_path = os.path.join(dir_path, f'cat.{i}.jpg')\n",
    "        image_array = load_image(image_path)\n",
    "        label = cp.array([[0, 1]])  # Cat label, converted to CuPy array\n",
    "        data.append((image_array, label))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load training data\n",
    "train_data = create_dataset(train_dir, dogs_num, cats_num)\n",
    "\n",
    "# Convert the list of tuples to CuPy arrays\n",
    "train_images = cp.array([image for image, label in train_data])\n",
    "train_labels = cp.array([label for image, label in train_data])\n",
    "\n",
    "# Shuffle data\n",
    "cp.random.shuffle(train_images)\n",
    "\n",
    "# Manually split into training and validation sets\n",
    "split_index = int(len(train_data) * (1 - validation_split))\n",
    "train_set = list(zip(train_images, train_labels))[:split_index]\n",
    "val_set = list(zip(train_images, train_labels))[split_index:]\n",
    "\n",
    "# Example of accessing data\n",
    "print(f'Training set size: {len(train_set)}')\n",
    "print(f'Validation set size: {len(val_set)}')\n",
    "\n",
    "# Example: Access an image and label from the training set\n",
    "example_image, example_label = train_set[0]\n",
    "print(f'Example image shape: {example_image.shape}')\n",
    "print(f'Example label: {example_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "170e04cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:12:38.648347Z",
     "iopub.status.busy": "2024-06-03T04:12:38.647951Z",
     "iopub.status.idle": "2024-06-03T04:12:38.655788Z",
     "shell.execute_reply": "2024-06-03T04:12:38.654687Z",
     "shell.execute_reply.started": "2024-06-03T04:12:38.648306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(train_set[4][0], cp.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c499144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:12:41.681386Z",
     "iopub.status.busy": "2024-06-03T04:12:41.681002Z",
     "iopub.status.idle": "2024-06-03T04:12:41.709822Z",
     "shell.execute_reply": "2024-06-03T04:12:41.708651Z",
     "shell.execute_reply.started": "2024-06-03T04:12:41.681354Z"
    }
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "# Setting up functions in forward propagation\n",
    "\n",
    "# We will forward propagate one piece of data at a time, then average over the examples, to simulate batches\n",
    "def batch_norm_forward(x, gamma, beta, eps=1e-5):\n",
    "    mean = cp.mean(x, axis=0)\n",
    "    variance = cp.var(x, axis=0)  # std_dev ** 2\n",
    "    x_normalized = (x - mean) / cp.sqrt(variance + eps)\n",
    "    # gamma and beta are learned\n",
    "    out = gamma * x_normalized + beta\n",
    "    cache = (x, x_normalized, mean, variance, gamma, beta, eps)\n",
    "    return out, cache\n",
    "\n",
    "def batch_norm_backward(dout, cache):\n",
    "    x, x_normalized, mean, variance, gamma, beta, eps = cache\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    dbeta = cp.sum(dout, axis=0)\n",
    "    dgamma = cp.sum(dout * x_normalized, axis=0)\n",
    "    \n",
    "    dx_normalized = dout * gamma\n",
    "    dvariance = cp.sum(dx_normalized * (x - mean) * -0.5 * cp.power(variance + eps, -1.5), axis=0)\n",
    "    dmean = cp.sum(dx_normalized * -1 / cp.sqrt(variance + eps), axis=0) + dvariance * cp.sum(-2 * (x - mean), axis=0) / N\n",
    "    \n",
    "    dx = dx_normalized / cp.sqrt(variance + eps) + dvariance * 2 * (x - mean) / N + dmean / N\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "def max_pooling(input_data):\n",
    "    input_height, input_width, input_depth = input_data.shape\n",
    "\n",
    "    # Calculate the output dimensions\n",
    "    output_height = input_height // 2 \n",
    "    output_width = input_width // 2\n",
    "    output_depth = input_depth\n",
    "\n",
    "    # Initialize the output array and array to store indices\n",
    "    output_data = cp.zeros((output_height, output_width, output_depth))\n",
    "    indices = cp.zeros((output_height, output_width, output_depth, 2), dtype=int)\n",
    "\n",
    "    # Apply max pooling\n",
    "    for h in range(output_height):\n",
    "        for w in range(output_width):\n",
    "            for d in range(output_depth):\n",
    "                # Extract the 2x2 region of interest from the input data\n",
    "                region = input_data[h*2:(h+1)*2, w*2:(w+1)*2, d]\n",
    "                # Compute the maximum value in the region\n",
    "                max_val = cp.max(region)\n",
    "                output_data[h, w, d] = max_val\n",
    "                # Find the indices of the maximum value in the region\n",
    "                max_indices = cp.unravel_index(cp.argmax(region), region.shape)\n",
    "                # Store the indices relative to the region and convert to global indices\n",
    "                indices[h, w, d] = [h*2 + max_indices[0], w*2 + max_indices[1]]\n",
    "\n",
    "    return output_data, indices\n",
    "\n",
    "def correlate(input_image, kernel):\n",
    "    # Dimensions of the input and kernel\n",
    "    input_height, input_width, input_channels = input_image.shape\n",
    "    filter_height, filter_width, filter_channels, num_filters = kernel.shape\n",
    "    \n",
    "    # Output dimensions\n",
    "    output_height = input_height - filter_height + 1  # (H - S + 1)\n",
    "    output_width = input_width - filter_width + 1  # (W - S + 1)\n",
    "\n",
    "    # Initialize the output\n",
    "    output = cp.zeros((output_height, output_width, num_filters))\n",
    "    \n",
    "    # Loop through each filter\n",
    "    for filter in range(num_filters):\n",
    "        # Loop through height and width of the output\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                # Extract the patch from the input image\n",
    "                input_patch = input_image[i:i+filter_height, j:j+filter_width, :]\n",
    "                # Perform the correlation operation\n",
    "                output[i, j, filter] = cp.sum(input_patch * kernel[:, :, :, filter])\n",
    "    return output\n",
    "\n",
    "def convolve(input_image, kernel):\n",
    "    # Dimensions of the input and kernel\n",
    "    input_height, input_width, input_channels = input_image.shape\n",
    "    filter_height, filter_width, filter_channels, num_filters = kernel.shape\n",
    "    \n",
    "    # Padding size (pad_height and pad_width are half the kernel size, rounded down)\n",
    "    pad_height = filter_height - 1\n",
    "    pad_width = filter_width - 1\n",
    "    \n",
    "    # Output dimensions\n",
    "    output_height = input_height + pad_height\n",
    "    output_width = input_width + pad_width\n",
    "\n",
    "    # Initialize the output\n",
    "    output = cp.zeros((output_height, output_width, num_filters))\n",
    "    \n",
    "    # Pad the input image with zeros\n",
    "    padded_input = cp.pad(input_image, ((pad_height, pad_height), (pad_width, pad_width), (0, 0)), mode='constant')\n",
    "    \n",
    "    # Flip kernel horizontally and vertically\n",
    "    rotated_kernel = cp.flip(kernel, axis=(0, 1))\n",
    "    \n",
    "    # Loop through each filter\n",
    "    for filter in range(num_filters):\n",
    "        # Loop through the height and width of the output\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                # Extract the patch from the padded input image\n",
    "                input_patch = padded_input[i:i+filter_height, j:j+filter_width, :]\n",
    "                # Perform the convolution operation\n",
    "                output[i, j, filter] = cp.sum(input_patch * rotated_kernel[:, :, :, filter])\n",
    "    return output\n",
    "\n",
    "def der_ReLU(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def ReLU(Z):\n",
    "    return cp.maximum(Z, 0)\n",
    "\n",
    "def ReLU2(Z, alpha=0.01):\n",
    "    return cp.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Compute the sigmoid function element-wise\n",
    "    return 1.0 / (1.0 + cp.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def softmax(Z):\n",
    "    exp_Z = cp.exp(Z - cp.max(Z, axis=1, keepdims=True))\n",
    "    sum_exp_Z = cp.sum(exp_Z, axis=1, keepdims=True)\n",
    "    return exp_Z / sum_exp_Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a09771f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:12:44.695962Z",
     "iopub.status.busy": "2024-06-03T04:12:44.695170Z",
     "iopub.status.idle": "2024-06-03T04:12:44.997780Z",
     "shell.execute_reply": "2024-06-03T04:12:44.996833Z",
     "shell.execute_reply.started": "2024-06-03T04:12:44.695927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "# Convolutional layer -> ReLU activation -> Batch normalization -> Dropout -> Max pooling\n",
    "\n",
    "# Layer 1\n",
    "conv1_1_kernel = cp.random.randn(3, 3, 3, 64)\n",
    "conv1_1_bias = cp.zeros((222, 222, 64))\n",
    "layer1_1_output = cp.zeros((222, 222, 64))\n",
    "\n",
    "conv1_2_kernel = cp.random.randn(3, 3, 64, 64)\n",
    "conv1_2_bias = cp.zeros((220, 220, 64))\n",
    "layer1_2_output = cp.zeros((220, 220, 64))\n",
    "\n",
    "gamma1_conv = cp.ones((220, 220, 64))\n",
    "beta1_conv = cp.zeros((220, 220, 64))\n",
    "\n",
    "layer1_pool = cp.zeros((110, 110, 64))\n",
    "\n",
    "# Layer 2\n",
    "conv2_1_kernel = cp.random.randn(3, 3, 64, 128)\n",
    "conv2_1_bias = cp.zeros((108, 108, 128))\n",
    "layer2_1_output = cp.zeros((108, 108, 128))\n",
    "\n",
    "conv2_2_kernel = cp.random.randn(3, 3, 128, 128)\n",
    "conv2_2_bias = cp.zeros((106, 106, 128))\n",
    "layer2_2_output = cp.zeros((106, 106, 128))\n",
    "\n",
    "conv2_3_kernel = cp.random.randn(3, 3, 128, 128)\n",
    "conv2_3_bias = cp.zeros((104, 104, 128))\n",
    "layer2_3_output = cp.zeros((104, 104, 128))\n",
    "\n",
    "gamma2_conv = cp.ones((104, 104, 128))\n",
    "beta2_conv = cp.zeros((104, 104, 128))\n",
    "\n",
    "layer2_pool = cp.zeros((52, 52, 128))\n",
    "\n",
    "# Layer 3\n",
    "conv3_1_kernel = cp.random.randn(3, 3, 128, 256)\n",
    "conv3_1_bias = cp.zeros((50, 50, 256))\n",
    "layer3_1_output = cp.zeros((50, 50, 256))\n",
    "\n",
    "conv3_2_kernel = cp.random.randn(3, 3, 256, 256)\n",
    "conv3_2_bias = cp.zeros((48, 48, 256))\n",
    "layer3_2_output = cp.zeros((48, 48, 256))\n",
    "\n",
    "gamma3_conv = cp.ones((48, 48, 256))\n",
    "beta3_conv = cp.zeros((48, 48, 256))\n",
    "\n",
    "layer3_pool = cp.zeros((24, 24, 256))\n",
    "\n",
    "# Layer 4\n",
    "conv4_1_kernel = cp.random.randn(3, 3, 256, 512)\n",
    "conv4_1_bias = cp.zeros((22, 22, 512))\n",
    "layer4_1_output = cp.zeros((22, 22, 512))\n",
    "\n",
    "conv4_2_kernel = cp.random.randn(3, 3, 512, 512)\n",
    "conv4_2_bias = cp.zeros((20, 20, 512))\n",
    "layer4_2_output = cp.zeros((20, 20, 512))\n",
    "\n",
    "conv4_3_kernel = cp.random.randn(3, 3, 512, 512)\n",
    "conv4_3_bias = cp.zeros((18, 18, 512))\n",
    "layer4_3_output = cp.zeros((18, 18, 512))\n",
    "\n",
    "gamma4_conv = cp.ones((18, 18, 512))\n",
    "beta4_conv = cp.zeros((18, 18, 512))\n",
    "\n",
    "layer4_pool = cp.zeros((9, 9, 512))\n",
    "\n",
    "# Layer 5\n",
    "conv5_1_kernel = cp.random.randn(3, 3, 512, 512)\n",
    "conv5_1_bias = cp.zeros((7, 7, 512))\n",
    "layer5_1_output = cp.zeros((7, 7, 512))\n",
    "\n",
    "conv5_2_kernel = cp.random.randn(3, 3, 512, 512)\n",
    "conv5_2_bias = cp.zeros((5, 5, 512))\n",
    "layer5_2_output = cp.zeros((5, 5, 512))\n",
    "\n",
    "gamma5_conv = cp.ones((5, 5, 512))\n",
    "beta5_conv = cp.zeros((5, 5, 512))\n",
    "\n",
    "# Layer 6\n",
    "fc1_weights = cp.random.randn(12800, 4096)\n",
    "fc1_bias = cp.zeros((1, 4096))\n",
    "layer6_1_output = cp.zeros((1, 4096))\n",
    "\n",
    "fc2_weights = cp.random.randn(4096, 2)\n",
    "fc2_bias = cp.zeros((1, 2))\n",
    "layer6_2_output = cp.zeros((1, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10472c4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:12:47.564942Z",
     "iopub.status.busy": "2024-06-03T04:12:47.564218Z",
     "iopub.status.idle": "2024-06-03T04:12:47.589405Z",
     "shell.execute_reply": "2024-06-03T04:12:47.588439Z",
     "shell.execute_reply.started": "2024-06-03T04:12:47.564904Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(\n",
    "    layer_input, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool,\n",
    "    conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool,\n",
    "    conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool,\n",
    "    conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool,\n",
    "    conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, \n",
    "    fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output, dropout_rate = 0.25):\n",
    "    \n",
    "    layer1_1_output = correlate(layer_input, conv1_1_kernel)\n",
    "    layer1_1_output += conv1_1_bias\n",
    "    layer1_1_output = ReLU(layer1_1_output)\n",
    "    \n",
    "    layer1_2_output = correlate(layer1_1_output, conv1_2_kernel)\n",
    "    layer1_2_output += conv1_2_bias\n",
    "    layer1_2_output = ReLU(layer1_2_output)\n",
    "\n",
    "    layer1_2_output, layer1_cache = batch_norm_forward(layer1_2_output, gamma1_conv, beta1_conv)\n",
    "    \n",
    "    dropout1_mask = (cp.random.rand(*layer1_2_output.shape) < dropout_rate) / dropout_rate\n",
    "    layer1_2_output *= dropout1_mask\n",
    "\n",
    "    layer1_pool, layer1_indices = max_pooling(layer1_2_output)\n",
    "\n",
    "    layer2_1_output = correlate(layer1_pool, conv2_1_kernel)\n",
    "    layer2_1_output += conv2_1_bias\n",
    "    layer2_1_output = ReLU(layer2_1_output)\n",
    "    \n",
    "    layer2_2_output = correlate(layer2_1_output, conv2_2_kernel)\n",
    "    layer2_2_output += conv2_2_bias\n",
    "    layer2_2_output = ReLU(layer2_2_output)\n",
    "\n",
    "    layer2_3_output = correlate(layer2_2_output, conv2_3_kernel)\n",
    "    layer2_3_output += conv2_3_bias\n",
    "    layer2_3_output = ReLU(layer2_3_output)\n",
    "\n",
    "    layer2_3_output, layer2_cache = batch_norm_forward(layer2_3_output, gamma2_conv, beta2_conv)\n",
    "    \n",
    "    dropout2_mask = (cp.random.rand(*layer2_3_output.shape) < dropout_rate) / dropout_rate \n",
    "    layer2_3_output *= dropout2_mask\n",
    "\n",
    "    layer2_pool, layer2_indices = max_pooling(layer2_3_output)\n",
    "\n",
    "    layer3_1_output = correlate(layer2_pool, conv3_1_kernel)\n",
    "    layer3_1_output += conv3_1_bias\n",
    "    layer3_1_output = ReLU(layer3_1_output)\n",
    "    \n",
    "    layer3_2_output = correlate(layer3_1_output, conv3_2_kernel)\n",
    "    layer3_2_output += conv3_2_bias\n",
    "    layer3_2_output = ReLU(layer3_2_output)\n",
    "\n",
    "    layer3_2_output, layer3_cache = batch_norm_forward(layer3_2_output, gamma3_conv, beta3_conv)\n",
    "    \n",
    "    dropout3_mask = (cp.random.rand(*layer3_2_output.shape) < dropout_rate) / dropout_rate\n",
    "    layer3_2_output *= dropout3_mask\n",
    "\n",
    "    layer3_pool, layer3_indices = max_pooling(layer3_2_output)\n",
    "\n",
    "    layer4_1_output = correlate(layer3_pool, conv4_1_kernel)\n",
    "    layer4_1_output += conv4_1_bias\n",
    "    layer4_1_output = ReLU(layer4_1_output)\n",
    "    \n",
    "    layer4_2_output = correlate(layer4_1_output, conv4_2_kernel)\n",
    "    layer4_2_output += conv4_2_bias\n",
    "    layer4_2_output = ReLU(layer4_2_output)\n",
    "\n",
    "    layer4_3_output = correlate(layer4_2_output, conv4_3_kernel)\n",
    "    layer4_3_output += conv4_3_bias\n",
    "    layer4_3_output = ReLU(layer4_3_output)\n",
    "\n",
    "    layer4_3_output, layer4_cache = batch_norm_forward(layer4_3_output, gamma4_conv, beta4_conv)\n",
    "    \n",
    "    dropout4_mask = (cp.random.rand(*layer4_3_output.shape) < dropout_rate) / dropout_rate \n",
    "    layer4_3_output *= dropout4_mask\n",
    "\n",
    "    layer4_pool, layer4_indices = max_pooling(layer4_3_output)\n",
    "    \n",
    "    layer5_1_output = correlate(layer4_pool, conv5_1_kernel)\n",
    "    layer5_1_output += conv5_1_bias\n",
    "    layer5_1_output = ReLU(layer5_1_output)\n",
    "    \n",
    "    layer5_2_output = correlate(layer5_1_output, conv5_2_kernel)\n",
    "    layer5_2_output += conv5_2_bias\n",
    "    layer5_2_output = ReLU(layer5_2_output)\n",
    "\n",
    "    layer5_2_output, layer5_cache = batch_norm_forward(layer5_2_output, gamma5_conv, beta5_conv)\n",
    "    \n",
    "    dropout5_mask = (cp.random.rand(*layer5_2_output.shape) < dropout_rate) / dropout_rate\n",
    "    layer5_2_output *= dropout5_mask\n",
    "        \n",
    "    layer6_1_output = (layer5_2_output.reshape(1, -1)) @ fc1_weights\n",
    "    layer6_1_output += fc1_bias\n",
    "    layer6_1_output = ReLU(layer6_1_output)\n",
    "    \n",
    "    layer6_2_output = layer6_1_output @ fc2_weights\n",
    "    layer6_2_output += fc2_bias\n",
    "    print(layer6_2_output)\n",
    "    layer6_2_output = softmax(layer6_2_output)\n",
    "    print(layer6_2_output)\n",
    "    \n",
    "    return (layer1_indices, layer1_cache, dropout1_mask, layer2_indices, layer2_cache, dropout2_mask, layer3_indices, layer3_cache, dropout3_mask, layer4_indices, layer4_cache, dropout4_mask, layer5_cache, dropout5_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cae0ea61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:12:49.312644Z",
     "iopub.status.busy": "2024-06-03T04:12:49.312234Z",
     "iopub.status.idle": "2024-06-03T04:12:49.402270Z",
     "shell.execute_reply": "2024-06-03T04:12:49.401225Z",
     "shell.execute_reply.started": "2024-06-03T04:12:49.312610Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_pass(layer_input, label, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool, layer1_indices, layer1_cache, dropout1_mask, \n",
    "            conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool, layer2_indices, layer2_cache, dropout2_mask,\n",
    "            conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool, layer3_indices, layer3_cache, dropout3_mask,\n",
    "            conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool, layer4_indices, layer4_cache, dropout4_mask,\n",
    "            conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, layer5_cache, dropout5_mask,\n",
    "            fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output):\n",
    "\n",
    "    dconv5_2_kernel = cp.zeros_like(conv5_2_kernel)\n",
    "    dlayer5_2_output = cp.zeros_like(layer5_2_output)\n",
    "    \n",
    "    dconv5_1_kernel = cp.zeros_like(conv5_1_kernel)\n",
    "    dlayer5_1_output = cp.zeros_like(layer5_1_output)\n",
    "    \n",
    "    dlayer4_pool = cp.zeros_like(layer4_pool)\n",
    "\n",
    "    dconv4_1_kernel = cp.zeros_like(conv4_1_kernel)\n",
    "    dlayer4_1_output = cp.zeros_like(layer4_1_output)\n",
    "    \n",
    "    dconv4_2_kernel = cp.zeros_like(conv4_2_kernel)\n",
    "    dlayer4_2_output = cp.zeros_like(layer4_2_output)\n",
    "    \n",
    "    dconv4_3_kernel = cp.zeros_like(conv4_3_kernel)\n",
    "    dlayer4_3_output = cp.zeros_like(layer4_3_output)\n",
    "\n",
    "    dlayer3_pool = cp.zeros_like(layer3_pool)\n",
    "\n",
    "    dconv3_2_kernel = cp.zeros_like(conv3_2_kernel)\n",
    "    dlayer3_2_output = cp.zeros_like(layer3_2_output)\n",
    "    \n",
    "    dconv3_1_kernel = cp.zeros_like(conv3_1_kernel)\n",
    "    dlayer3_1_output = cp.zeros_like(layer3_1_output)\n",
    "    \n",
    "    dlayer2_pool = cp.zeros_like(layer2_pool)\n",
    "\n",
    "    dconv2_1_kernel = cp.zeros_like(conv2_1_kernel)\n",
    "    dlayer2_1_output = cp.zeros_like(layer2_1_output)\n",
    "    \n",
    "    dconv2_2_kernel = cp.zeros_like(conv2_2_kernel)\n",
    "    dlayer2_2_output = cp.zeros_like(layer2_2_output)\n",
    "    \n",
    "    dconv2_3_kernel = cp.zeros_like(conv2_3_kernel)\n",
    "    dlayer2_3_output = cp.zeros_like(layer2_3_output)\n",
    "\n",
    "    dlayer1_pool = cp.zeros_like(layer1_pool)\n",
    "\n",
    "    dconv1_2_kernel = cp.zeros_like(conv1_2_kernel)\n",
    "    dlayer1_2_output = cp.zeros_like(layer1_2_output)\n",
    "    \n",
    "    dconv1_1_kernel = cp.zeros_like(conv1_1_kernel)\n",
    "    dlayer1_1_output = cp.zeros_like(layer1_1_output)\n",
    "\n",
    "     ### Layer 6 ----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Layer 6 - Softmax\n",
    "    dlayer6_2_output = layer6_2_output - label # (1,2)\n",
    "    \n",
    "    # fc2\n",
    "    dfc2_bias = dlayer6_2_output # (1,2)\n",
    "    \n",
    "    dfc2_weights = layer6_1_output.T @ dlayer6_2_output # (4096, 1) @ (1, 2) = (4096, 2)\n",
    "    \n",
    "    dlayer6_1_output = (dlayer6_2_output @ fc2_weights.T) * der_ReLU(layer6_1_output) # (1, 2) @ (2, 4096) = (1, 4096)\n",
    "    \n",
    "    # fc1\n",
    "    dfc1_bias = dlayer6_1_output\n",
    "    \n",
    "    dfc1_weights = (layer5_2_output.reshape(1, 12800).T) @ dlayer6_1_output # (12800, 1) @ (1, 4096) = (12800, 4096)\n",
    "    \n",
    "    dlayer5_2_output = (dlayer6_1_output @ fc1_weights.T) # (1, 4096) @ (4096, 12800) = (1, 12800)\n",
    "\n",
    "    \n",
    "    ### Layer 5 ----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Unflatten\n",
    "    dlayer5_2_output = dlayer5_2_output.reshape(5, 5, 512)\n",
    "\n",
    "    # Layer 5 - Dropout\n",
    "    dlayer5_2_output *= dropout5_mask\n",
    "    \n",
    "    # Layer 5 - Batch norm\n",
    "    dlayer5_2_output, dgamma5_conv, dbeta5_conv = batch_norm_backward(dlayer5_2_output, layer5_cache)\n",
    "\n",
    "    # Layer 5 - conv2\n",
    "    dlayer5_2_output *= der_ReLU(layer5_2_output) # (5,5,512)\n",
    "    dconv5_2_bias = dlayer5_2_output # (5,5,512)\n",
    "\n",
    "    # Looping over filters\n",
    "    for filter in range(conv5_2_kernel.shape[3]): # 512\n",
    "        temp2 = dlayer5_2_output[:, :, filter].reshape((dlayer5_2_output.shape[0], dlayer5_2_output.shape[0], 1, 1))\n",
    "        for slice in range(conv5_2_kernel.shape[2]): # 512\n",
    "            temp1 = layer5_1_output[:, :, slice].reshape((layer5_1_output.shape[0], layer5_1_output.shape[0], 1))\n",
    "            dconv5_2_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,512,512)\n",
    "    \n",
    "    for filter in range(conv5_2_kernel.shape[3]): # 512\n",
    "        temp1 = dlayer5_2_output[:, :, filter].reshape((dlayer5_2_output.shape[0], dlayer5_2_output.shape[0], 1))\n",
    "        for slice in range(conv5_2_kernel.shape[2]): # 512\n",
    "            temp2 = conv5_2_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            dlayer5_1_output += convolve(temp1, temp2)\n",
    "    \n",
    "     # Layer 5 - BP conv1\n",
    "    dlayer5_1_output *= der_ReLU(layer5_1_output) # (7,7,512)\n",
    "    dconv5_1_bias = dlayer5_1_output # (7,7,512)\n",
    "    \n",
    "    \n",
    "    # Looping over filters\n",
    "    for filter in range(conv5_1_kernel.shape[3]): # 512\n",
    "        temp2 = dlayer5_1_output[:, :, filter].reshape((dlayer5_1_output.shape[0], dlayer5_1_output.shape[0], 1, 1))\n",
    "        for slice in range(conv5_1_kernel.shape[2]): # 512\n",
    "            temp1 = layer4_pool[:, :, slice].reshape((layer4_pool.shape[0], layer4_pool.shape[0], 1))\n",
    "            dconv5_1_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,512,512)\n",
    "\n",
    "    for filter in range(conv5_1_kernel.shape[3]): # 512\n",
    "        temp1 = dlayer5_1_output[:, :, filter].reshape((dlayer5_1_output.shape[0], dlayer5_1_output.shape[0], 1))\n",
    "        for slice in range(conv5_1_kernel.shape[2]): # 512\n",
    "            temp2 = conv5_1_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            layer4_pool += convolve(temp1, temp2)\n",
    "\n",
    "    ### Layer 4 ----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Unpooling - (9, 9, 512) -> (18, 18, 512)\n",
    "    for i in range(dlayer4_pool.shape[0]):  # 9\n",
    "        for j in range(dlayer4_pool.shape[1]):  # 9\n",
    "            for k in range(dlayer4_pool.shape[2]): # 512\n",
    "                # Get the global indices from layer_indices\n",
    "                x_index, y_index = layer4_indices[i, j, k] # -> (18, 18, 512, 2)\n",
    "                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n",
    "                dlayer4_3_output[x_index, y_index, k] = dlayer4_pool[i, j, k]\n",
    "    \n",
    "    # Layer 4 - Dropout\n",
    "    dlayer4_3_output *= dropout4_mask\n",
    "    \n",
    "    # Layer 4 - Batch norm\n",
    "    dlayer4_3_output, dgamma4_conv, dbeta4_conv = batch_norm_backward(dlayer4_3_output, layer4_cache)\n",
    "\n",
    "    # Layer 4 - BP conv3\n",
    "    dlayer4_3_output *= der_ReLU(layer4_3_output) # (18,18,512)\n",
    "    dconv4_3_bias = dlayer4_3_output # (18,18,512)\n",
    "\n",
    "    # Looping over filters\n",
    "    for filter in range(conv4_3_kernel.shape[3]): # 512\n",
    "        temp2 = dlayer4_3_output[:, :, filter].reshape((dlayer4_3_output.shape[0], dlayer4_3_output.shape[0], 1, 1))\n",
    "        for slice in range(conv4_3_kernel.shape[2]): # 512\n",
    "            temp1 = layer4_2_output[:, :, slice].reshape((layer4_2_output.shape[0], layer4_2_output.shape[0], 1))\n",
    "            dconv4_3_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,512,512)\n",
    "    \n",
    "    for filter in range(conv4_3_kernel.shape[3]): # 512\n",
    "        temp1 = dlayer4_3_output[:, :, filter].reshape((dlayer4_3_output.shape[0], dlayer4_3_output.shape[0], 1))\n",
    "        for slice in range(conv4_3_kernel.shape[2]): # 512\n",
    "            temp2 = conv4_3_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            dlayer4_2_output += convolve(temp1, temp2)\n",
    "    \n",
    "    # Layer 4 - BP conv2\n",
    "    dlayer4_2_output *= der_ReLU(layer4_2_output) # (18,18,512)\n",
    "    dconv4_2_bias = dlayer4_2_output # (18,18,512)\n",
    "\n",
    "     # Looping over filters\n",
    "    for filter in range(conv4_2_kernel.shape[3]): # 512\n",
    "        temp2 = dlayer4_2_output[:, :, filter].reshape((dlayer4_2_output.shape[0], dlayer4_2_output.shape[0], 1, 1))\n",
    "        for slice in range(conv4_2_kernel.shape[2]): # 512\n",
    "            temp1 = layer4_1_output[:, :, slice].reshape((layer4_1_output.shape[0], layer4_1_output.shape[0], 1))\n",
    "            dconv4_2_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,512,512)\n",
    "    \n",
    "    for filter in range(conv4_2_kernel.shape[3]): # 512\n",
    "        temp1 = dlayer4_2_output[:, :, filter].reshape((dlayer4_2_output.shape[0], dlayer4_2_output.shape[0], 1))\n",
    "        for slice in range(conv4_2_kernel.shape[2]): # 512\n",
    "            temp2 = conv4_2_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            dlayer4_1_output += convolve(temp1, temp2)\n",
    "    \n",
    "    # Layer 4 - BP conv1\n",
    "    dlayer4_1_output *= der_ReLU(layer4_1_output) # (18,18,512)\n",
    "    dconv4_1_bias = dlayer4_1_output # (18,18,512)\n",
    "\n",
    "    # Looping over filters\n",
    "    for filter in range(conv4_1_kernel.shape[3]): # 512\n",
    "        temp2 = dlayer4_1_output[:, :, filter].reshape((dlayer4_1_output.shape[0], dlayer4_1_output.shape[0], 1, 1))\n",
    "        for slice in range(conv4_1_kernel.shape[2]): # 512\n",
    "            temp1 = layer3_pool[:, :, slice].reshape((layer3_pool.shape[0], layer3_pool.shape[0], 1))\n",
    "            dconv4_1_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,512,512)\n",
    "    \n",
    "    for filter in range(conv4_1_kernel.shape[3]): # 512\n",
    "        temp1 = dlayer4_1_output[:, :, filter].reshape((dlayer4_1_output.shape[0], dlayer4_1_output.shape[0], 1))\n",
    "        for slice in range(conv4_1_kernel.shape[2]): # 512\n",
    "            temp2 = conv4_1_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            dlayer3_pool += convolve(temp1, temp2)\n",
    "\n",
    "\n",
    "    ### Layer 3 ----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Unpooling - (24, 24, 256) -> (48, 48, 256)\n",
    "    for i in range(dlayer3_pool.shape[0]):\n",
    "        for j in range(dlayer3_pool.shape[1]): \n",
    "            for k in range(dlayer3_pool.shape[2]):\n",
    "                # Get the global indices from layer_indices\n",
    "                x_index, y_index = layer3_indices[i, j, k] # -> (18, 18, 512, 2)\n",
    "                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n",
    "                dlayer3_2_output[x_index, y_index, k] = dlayer3_pool[i, j, k]\n",
    "    \n",
    "    # Layer 3 - Dropout\n",
    "    dlayer3_2_output *= dropout3_mask\n",
    "\n",
    "    # Layer 3 - Batch norm\n",
    "    dlayer3_2_output, dgamma3_conv, dbeta3_conv = batch_norm_backward(dlayer3_2_output, layer3_cache)\n",
    "\n",
    "    # Layer 3 - BP conv2\n",
    "    dlayer3_2_output *= der_ReLU(layer3_2_output) # (36,36,256)\n",
    "    dconv3_2_bias = dlayer3_2_output # (36,36,256)\n",
    "\n",
    "    # Looping over filters\n",
    "    for filter in range(conv3_2_kernel.shape[3]): # 256\n",
    "        temp2 = dlayer3_2_output[:, :, filter].reshape((dlayer3_2_output.shape[0], dlayer3_2_output.shape[0], 1, 1))\n",
    "        for slice in range(conv3_2_kernel.shape[2]): # 256\n",
    "            temp1 = layer3_1_output[:, :, slice].reshape((layer3_1_output.shape[0], layer3_1_output.shape[0], 1))\n",
    "            dconv3_2_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,256,256)\n",
    "\n",
    "    for filter in range(conv3_2_kernel.shape[3]): # 256\n",
    "        temp1 = dlayer3_2_output[:, :, filter].reshape((dlayer3_2_output.shape[0], dlayer3_2_output.shape[0], 1))\n",
    "        for slice in range(conv3_2_kernel.shape[2]): # 256\n",
    "            temp2 = conv3_2_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            dlayer3_1_output += convolve(temp1, temp2)\n",
    "    \n",
    "    # Layer 3 - BP conv1\n",
    "    dlayer3_1_output *= der_ReLU(layer3_1_output) # (36,36,256)\n",
    "    dconv3_1_bias = dlayer3_1_output # (36,36,256)\n",
    "\n",
    "    # Looping over filters\n",
    "    for filter in range(conv3_1_kernel.shape[3]): # 256\n",
    "        temp2 = dlayer3_1_output[:, :, filter].reshape((dlayer3_1_output.shape[0], dlayer3_1_output.shape[0], 1, 1))\n",
    "        for slice in range(conv3_1_kernel.shape[2]): # 256\n",
    "            temp1 = layer2_pool[:, :, slice].reshape((layer2_pool.shape[0], layer2_pool.shape[0], 1))\n",
    "            dconv3_1_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,256,256)\n",
    "\n",
    "    for filter in range(conv3_1_kernel.shape[3]): # 256\n",
    "        temp1 = dlayer3_1_output[:, :, filter].reshape((dlayer3_1_output.shape[0], dlayer3_1_output.shape[0], 1))\n",
    "        for slice in range(conv3_1_kernel.shape[2]): # 256\n",
    "            temp2 = conv3_1_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            dlayer2_pool += convolve(temp1, temp2)\n",
    "\n",
    "    ### Layer 2 ----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Unpooling - (24, 24, 256) -> (48, 48, 256)\n",
    "    for i in range(dlayer2_pool.shape[0]):\n",
    "        for j in range(dlayer2_pool.shape[1]): \n",
    "            for k in range(dlayer2_pool.shape[2]):\n",
    "                # Get the global indices from layer_indices\n",
    "                x_index, y_index = layer2_indices[i, j, k] # -> (18, 18, 512, 2)\n",
    "                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n",
    "                dlayer2_3_output[x_index, y_index, k] = dlayer2_pool[i, j, k]\n",
    "    \n",
    "    # Layer 2 - Dropout\n",
    "    dlayer2_3_output *= dropout2_mask\n",
    "\n",
    "    # Layer 2 - Batch norm\n",
    "    dlayer2_3_output, dgamma2_conv, dbeta2_conv = batch_norm_backward(dlayer2_3_output, layer2_cache)\n",
    "\n",
    "    # Layer 2 - BP conv3\n",
    "    dlayer2_3_output *= der_ReLU(layer2_3_output) # (72,72,128)\n",
    "    dconv2_3_bias = dlayer2_3_output # (72,72,128)\n",
    "\n",
    "    # Looping over filters\n",
    "    for filter in range(conv2_3_kernel.shape[3]): # 128\n",
    "        temp2 = dlayer2_3_output[:, :, filter].reshape((dlayer2_3_output.shape[0], dlayer2_3_output.shape[0], 1, 1))\n",
    "        for slice in range(conv2_3_kernel.shape[2]): # 128\n",
    "            temp1 = layer2_2_output[:, :, slice].reshape((layer2_2_output.shape[0], layer2_2_output.shape[0], 1))\n",
    "            dconv2_3_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,128,128)\n",
    "\n",
    "    for filter in range(conv2_3_kernel.shape[3]): # 128\n",
    "        temp1 = dlayer2_3_output[:, :, filter].reshape((dlayer2_3_output.shape[0], dlayer2_3_output.shape[0], 1))\n",
    "        for slice in range(conv2_3_kernel.shape[2]): # 128\n",
    "            temp2 = conv2_3_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            dlayer2_2_output += convolve(temp1, temp2)\n",
    "\n",
    "    # Layer 2 - BP conv2\n",
    "    dlayer2_2_output *= der_ReLU(layer2_2_output) # (72,72,128)\n",
    "    dconv2_2_bias = dlayer2_2_output # (72,72,128)\n",
    "\n",
    "    # Looping over filters\n",
    "    for filter in range(conv2_2_kernel.shape[3]): # 128\n",
    "        temp2 = dlayer2_2_output[:, :, filter].reshape((dlayer2_2_output.shape[0], dlayer2_2_output.shape[0], 1, 1))\n",
    "        for slice in range(conv2_2_kernel.shape[2]): # 128\n",
    "            temp1 = layer2_1_output[:, :, slice].reshape((layer2_1_output.shape[0], layer2_1_output.shape[0], 1))\n",
    "            dconv2_2_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,128,128)\n",
    "     \n",
    "    for filter in range(conv2_2_kernel.shape[3]): # 128\n",
    "        temp1 = dlayer2_2_output[:, :, filter].reshape((dlayer2_2_output.shape[0], dlayer2_2_output.shape[0], 1))\n",
    "        for slice in range (conv2_2_kernel.shape[2]): # 512\n",
    "            # Broadcast the corresponding kernel to the input depth\n",
    "            temp2 = conv2_2_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            dlayer2_1_output += convolve(temp1, temp2)\n",
    "\n",
    "    # Layer 2 - BP conv1\n",
    "    dlayer2_1_output *= der_ReLU(layer2_1_output) # (72,72,128)\n",
    "    dconv2_1_bias = dlayer2_1_output # (72,72,128)\n",
    "\n",
    "    # Looping over filters\n",
    "    for filter in range(conv2_1_kernel.shape[3]): # 128\n",
    "        temp2 = dlayer2_1_output[:, :, filter].reshape((dlayer2_1_output.shape[0], dlayer2_1_output.shape[0], 1, 1))\n",
    "        for slice in range(conv2_1_kernel.shape[2]): # 128\n",
    "            temp1 = layer1_pool[:, :, slice].reshape((layer1_pool.shape[0], layer1_pool.shape[0], 1))\n",
    "            dconv2_1_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,128,128)\n",
    "\n",
    "    for filter in range(conv2_1_kernel.shape[3]): # 128\n",
    "        temp1 = dlayer2_1_output[:, :, filter].reshape((dlayer2_1_output.shape[0], dlayer2_1_output.shape[0], 1))\n",
    "        for slice in range(conv2_1_kernel.shape[2]): # 128\n",
    "            temp2 = conv2_1_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            dlayer1_pool += convolve(temp1, temp2)\n",
    "    \n",
    "\n",
    "    ### Layer 1 ----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Unpooling - (110, 110, 64) -> (220, 220, 64)\n",
    "    for i in range(dlayer1_pool.shape[0]):\n",
    "        for j in range(dlayer1_pool.shape[1]): \n",
    "            for k in range(dlayer1_pool.shape[2]):\n",
    "                # Get the global indices from layer_indices\n",
    "                x_index, y_index = layer1_indices[i, j, k] # -> (18, 18, 512, 2)\n",
    "                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n",
    "                dlayer1_2_output[x_index, y_index, k] = dlayer1_pool[i, j, k]\n",
    "    \n",
    "    # Layer 3 - Dropout\n",
    "    dlayer1_2_output *= dropout1_mask\n",
    "    \n",
    "    # Layer 3 - Batch norm\n",
    "    dlayer1_2_output, dgamma1_conv, dbeta1_conv = batch_norm_backward(dlayer1_2_output, layer1_cache)\n",
    "\n",
    "    # Layer 1 - BP conv2\n",
    "    dlayer1_2_output *= der_ReLU(layer1_2_output) # (144,144,64)\n",
    "    dconv1_2_bias = dlayer1_2_output # (144,144,64)\n",
    "\n",
    "    # Looping over filters\n",
    "    for filter in range(conv1_2_kernel.shape[3]): # 64\n",
    "        temp2 = dlayer1_2_output[:, :, filter].reshape((dlayer1_2_output.shape[0], dlayer1_2_output.shape[0], 1, 1))\n",
    "        for slice in range(conv1_2_kernel.shape[2]): # 64\n",
    "            temp1 = layer1_1_output[:, :, slice].reshape((layer1_1_output.shape[0], layer1_1_output.shape[0], 1))\n",
    "            dconv1_2_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,64,64)\n",
    "\n",
    "    for filter in range(conv1_2_kernel.shape[3]): # 64\n",
    "        temp1 = dlayer1_2_output[:, :, filter].reshape((dlayer1_2_output.shape[0], dlayer1_2_output.shape[0], 1))\n",
    "        for slice in range(conv1_2_kernel.shape[2]): # 64\n",
    "            temp2 = conv1_2_kernel[:, :, slice, filter].reshape((3, 3, 1, 1))\n",
    "            dlayer1_1_output += convolve(temp1, temp2)\n",
    "    \n",
    "    # Layer 1 - BP conv1\n",
    "    dlayer1_1_output *= der_ReLU(layer1_1_output) # (144,144,64)\n",
    "    dconv1_1_bias = dlayer1_1_output # (144,144,64)\n",
    "\n",
    "    # Looping over filters\n",
    "    for filter in range(conv1_1_kernel.shape[3]): # 64\n",
    "        temp2 = dlayer1_1_output[:, :, filter].reshape((dlayer1_1_output.shape[0], dlayer1_1_output.shape[0], 1, 1))\n",
    "        for slice in range(layer_input.shape[2]): # 3\n",
    "            temp1 = layer_input[:, :, slice].reshape((layer_input.shape[0], layer_input.shape[0], 1))\n",
    "            dconv1_1_kernel[:, :, slice, filter] = correlate(temp1, temp2).reshape((3, 3)) # (3,3,3,64)\n",
    "\n",
    "    return (dconv1_1_kernel, dconv1_1_bias, dconv1_2_kernel, dconv1_2_bias, dgamma1_conv, dbeta1_conv,\n",
    "    dconv2_1_kernel, dconv2_1_bias, dconv2_2_kernel, dconv2_2_bias, dconv2_3_kernel, dconv2_3_bias,\n",
    "    dgamma2_conv, dbeta2_conv, dconv3_1_kernel, dconv3_1_bias, dconv3_2_kernel, dconv3_2_bias,\n",
    "    dgamma3_conv, dbeta3_conv, dconv4_1_kernel, dconv4_1_bias, dconv4_2_kernel, dconv4_2_bias,\n",
    "    dconv4_3_kernel, dconv4_3_bias, dgamma4_conv, dbeta4_conv, dconv5_1_kernel, dconv5_1_bias,\n",
    "    dconv5_2_kernel, dconv5_2_bias, dgamma5_conv, dbeta5_conv, dfc1_weights, dfc1_bias, dfc2_weights, dfc2_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "833c5d57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:12:54.321313Z",
     "iopub.status.busy": "2024-06-03T04:12:54.320939Z",
     "iopub.status.idle": "2024-06-03T04:12:54.333737Z",
     "shell.execute_reply": "2024-06-03T04:12:54.332468Z",
     "shell.execute_reply.started": "2024-06-03T04:12:54.321284Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_params(\n",
    "    dconv1_1_kernel, dconv1_1_bias, dconv1_2_kernel, dconv1_2_bias, dgamma1_conv, dbeta1_conv,\n",
    "    dconv2_1_kernel, dconv2_1_bias, dconv2_2_kernel, dconv2_2_bias, dconv2_3_kernel, dconv2_3_bias,\n",
    "    dgamma2_conv, dbeta2_conv, dconv3_1_kernel, dconv3_1_bias, dconv3_2_kernel, dconv3_2_bias,\n",
    "    dgamma3_conv, dbeta3_conv, dconv4_1_kernel, dconv4_1_bias, dconv4_2_kernel, dconv4_2_bias,\n",
    "    dconv4_3_kernel, dconv4_3_bias, dgamma4_conv, dbeta4_conv, dconv5_1_kernel, dconv5_1_bias,\n",
    "    dconv5_2_kernel, dconv5_2_bias, dgamma5_conv, dbeta5_conv, dfc1_weights, dfc1_bias, dfc2_weights, dfc2_bias,\n",
    "    learning_rate\n",
    "):\n",
    "    conv1_1_kernel -= learning_rate * dconv1_1_kernel\n",
    "    conv1_1_bias -= learning_rate * dconv1_1_bias\n",
    "    conv1_2_kernel -= learning_rate * dconv1_2_kernel\n",
    "    conv1_2_bias -= learning_rate * dconv1_2_bias\n",
    "    gamma1_conv -= learning_rate * dgamma1_conv\n",
    "    beta1_conv -= learning_rate * dbeta1_conv\n",
    "\n",
    "    conv2_1_kernel -= learning_rate * dconv2_1_kernel\n",
    "    conv2_1_bias -= learning_rate * dconv2_1_bias\n",
    "    conv2_2_kernel -= learning_rate * dconv2_2_kernel\n",
    "    conv2_2_bias -= learning_rate * dconv2_2_bias\n",
    "    conv2_3_kernel -= learning_rate * dconv2_3_kernel\n",
    "    conv2_3_bias -= learning_rate * dconv2_3_bias\n",
    "    gamma2_conv -= learning_rate * dgamma2_conv\n",
    "    beta2_conv -= learning_rate * dbeta2_conv\n",
    "\n",
    "    conv3_1_kernel -= learning_rate * dconv3_1_kernel\n",
    "    conv3_1_bias -= learning_rate * dconv3_1_bias\n",
    "    conv3_2_kernel -= learning_rate * dconv3_2_kernel\n",
    "    conv3_2_bias -= learning_rate * dconv3_2_bias\n",
    "    gamma3_conv -= learning_rate * dgamma3_conv\n",
    "    beta3_conv -= learning_rate * dbeta3_conv\n",
    "\n",
    "    conv4_1_kernel -= learning_rate * dconv4_1_kernel\n",
    "    conv4_1_bias -= learning_rate * dconv4_1_bias\n",
    "    conv4_2_kernel -= learning_rate * dconv4_2_kernel\n",
    "    conv4_2_bias -= learning_rate * dconv4_2_bias\n",
    "    conv4_3_kernel -= learning_rate * dconv4_3_kernel\n",
    "    conv4_3_bias -= learning_rate * dconv4_3_bias\n",
    "    gamma4_conv -= learning_rate * dgamma4_conv\n",
    "    beta4_conv -= learning_rate * dbeta4_conv\n",
    "\n",
    "    conv5_1_kernel -= learning_rate * dconv5_1_kernel\n",
    "    conv5_1_bias -= learning_rate * dconv5_1_bias\n",
    "    conv5_2_kernel -= learning_rate * dconv5_2_kernel\n",
    "    conv5_2_bias -= learning_rate * dconv5_2_bias\n",
    "    gamma5_conv -= learning_rate * dgamma5_conv\n",
    "    beta5_conv -= learning_rate * dbeta5_conv\n",
    "\n",
    "    fc1_weights -= learning_rate * dfc1_weights\n",
    "    fc1_bias -= learning_rate * dfc1_bias\n",
    "    fc2_weights -= learning_rate * dfc2_weights\n",
    "    fc2_bias -= learning_rate * dfc2_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cafe059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:13:08.877123Z",
     "iopub.status.busy": "2024-06-03T04:13:08.876246Z",
     "iopub.status.idle": "2024-06-03T04:13:08.885252Z",
     "shell.execute_reply": "2024-06-03T04:13:08.884213Z",
     "shell.execute_reply.started": "2024-06-03T04:13:08.877087Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Layer 1\n",
    "    'conv1_1_kernel': conv1_1_kernel,\n",
    "    'conv1_1_bias': conv1_1_bias,\n",
    "    'conv1_2_kernel': conv1_2_kernel,\n",
    "    'conv1_2_bias': conv1_2_bias,\n",
    "    'gamma1_conv': gamma1_conv,\n",
    "    'beta1_conv': beta1_conv,\n",
    "    \n",
    "    # Layer 2\n",
    "    'conv2_1_kernel': conv2_1_kernel,\n",
    "    'conv2_1_bias': conv2_1_bias,\n",
    "    'conv2_2_kernel': conv2_2_kernel,\n",
    "    'conv2_2_bias': conv2_2_bias,\n",
    "    'conv2_3_kernel': conv2_3_kernel,\n",
    "    'conv2_3_bias': conv2_3_bias,\n",
    "    'gamma2_conv': gamma2_conv,\n",
    "    'beta2_conv': beta2_conv,\n",
    "\n",
    "    # Layer 3\n",
    "    'conv3_1_kernel': conv3_1_kernel,\n",
    "    'conv3_1_bias': conv3_1_bias,\n",
    "    'conv3_2_kernel': conv3_2_kernel,\n",
    "    'conv3_2_bias': conv3_2_bias,\n",
    "    'gamma3_conv': gamma3_conv,\n",
    "    'beta3_conv': beta3_conv,\n",
    "\n",
    "    # Layer 4\n",
    "    'conv4_1_kernel': conv4_1_kernel,\n",
    "    'conv4_1_bias': conv4_1_bias,\n",
    "    'conv4_2_kernel': conv4_2_kernel,\n",
    "    'conv4_2_bias': conv4_2_bias,\n",
    "    'conv4_3_kernel': conv4_3_kernel,\n",
    "    'conv4_3_bias': conv4_3_bias,\n",
    "    'gamma4_conv': gamma4_conv,\n",
    "    'beta4_conv': beta4_conv,\n",
    "\n",
    "    # Layer 5\n",
    "    'conv5_1_kernel': conv5_1_kernel,\n",
    "    'conv5_1_bias': conv5_1_bias,\n",
    "    'conv5_2_kernel': conv5_2_kernel,\n",
    "    'conv5_2_bias': conv5_2_bias,\n",
    "    'gamma5_conv': gamma5_conv,\n",
    "    'beta5_conv': beta5_conv,\n",
    "\n",
    "    # Layer 6\n",
    "    'fc1_weights': fc1_weights,\n",
    "    'fc1_bias': fc1_bias,\n",
    "    'fc2_weights': fc2_weights,\n",
    "    'fc2_bias': fc2_bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29568300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-03T04:13:21.946376Z",
     "iopub.status.busy": "2024-06-03T04:13:21.945990Z",
     "iopub.status.idle": "2024-06-03T04:13:21.982035Z",
     "shell.execute_reply": "2024-06-03T04:13:21.980895Z",
     "shell.execute_reply.started": "2024-06-03T04:13:21.946345Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement randomization - currently not used in training\n",
    "def stochastic_gradient_descent(train_set, val_set, epochs, learning_rate, batch_size):\n",
    "    num_examples = len(train_set)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\", epoch + 1)\n",
    "        \n",
    "        for batch_start in range(0, num_examples, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, num_examples)\n",
    "            \n",
    "            # Initialize batch_gradients to accumulate gradients\n",
    "            batch_gradients = [\n",
    "                cp.zeros_like(conv1_1_kernel), cp.zeros_like(conv1_1_bias),\n",
    "                cp.zeros_like(conv1_2_kernel), cp.zeros_like(conv1_2_bias),\n",
    "                cp.zeros_like(gamma1_conv), cp.zeros_like(beta1_conv),\n",
    "                \n",
    "                cp.zeros_like(conv2_1_kernel), cp.zeros_like(conv2_1_bias),\n",
    "                cp.zeros_like(conv2_2_kernel), cp.zeros_like(conv2_2_bias),\n",
    "                cp.zeros_like(conv2_3_kernel), cp.zeros_like(conv2_3_bias),\n",
    "                cp.zeros_like(gamma2_conv), cp.zeros_like(beta2_conv),\n",
    "                \n",
    "                cp.zeros_like(conv3_1_kernel), cp.zeros_like(conv3_1_bias),\n",
    "                cp.zeros_like(conv3_2_kernel), cp.zeros_like(conv3_2_bias),\n",
    "                cp.zeros_like(gamma3_conv), cp.zeros_like(beta3_conv),\n",
    "                \n",
    "                cp.zeros_like(conv4_1_kernel), cp.zeros_like(conv4_1_bias),\n",
    "                cp.zeros_like(conv4_2_kernel), cp.zeros_like(conv4_2_bias),\n",
    "                cp.zeros_like(conv4_3_kernel), cp.zeros_like(conv4_3_bias),\n",
    "                cp.zeros_like(gamma4_conv), cp.zeros_like(beta4_conv),\n",
    "                \n",
    "                cp.zeros_like(conv5_1_kernel), cp.zeros_like(conv5_1_bias),\n",
    "                cp.zeros_like(conv5_2_kernel), cp.zeros_like(conv5_2_bias),\n",
    "                cp.zeros_like(gamma5_conv), cp.zeros_like(beta5_conv),\n",
    "                \n",
    "                cp.zeros_like(fc1_weights), cp.zeros_like(fc1_bias),\n",
    "                cp.zeros_like(fc2_weights), cp.zeros_like(fc2_bias)\n",
    "            ]\n",
    "        \n",
    "            for j in range(batch_start, batch_end):\n",
    "                # Get a single training example\n",
    "                layer_input, label = train_set[j]\n",
    "                \n",
    "                # Forward propagation\n",
    "                layer1_indices, layer1_cache, dropout1_mask, layer2_indices, layer2_cache, dropout2_mask, layer3_indices, layer3_cache, dropout3_mask, layer4_indices, layer4_cache, dropout4_mask, layer5_cache, dropout5_mask = forward_propagation(\n",
    "                    layer_input, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool,\n",
    "                    conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool,\n",
    "                    conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool,\n",
    "                    conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool,\n",
    "                    conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, \n",
    "                    fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output, dropout_rate = 0.25\n",
    "                )\n",
    "                \n",
    "                # Back propagation\n",
    "                gradients = backward_pass(\n",
    "                    layer_input, label, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool, layer1_indices, layer1_cache, dropout1_mask,\n",
    "                    conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool, layer2_indices, layer2_cache, dropout2_mask,\n",
    "                    conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool, layer3_indices, layer3_cache, dropout3_mask,\n",
    "                    conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool, layer4_indices, layer4_cache, dropout4_mask,\n",
    "                    conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, layer5_cache, dropout5_mask,\n",
    "                    fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output\n",
    "                )\n",
    "                \n",
    "                # Accumulate gradients\n",
    "                for k in range(len(batch_gradients)):\n",
    "                    # Access batch_gradients list, access gradients tuple\n",
    "                    batch_gradients[k] += gradients[k]\n",
    "            \n",
    "            # Average gradients after processing the batch\n",
    "            batch_gradients = [grad / batch_size for grad in batch_gradients]\n",
    "            \n",
    "            # Update parameters after processing the batch\n",
    "            update_params(*batch_gradients, learning_rate)\n",
    "\n",
    "        # Get training accuracy\n",
    "        train_accuracy = 0\n",
    "        for _ in range(100):\n",
    "            random_idx = np.random.randint(len(train_set))\n",
    "            test_input, true_label = train_set[random_idx]\n",
    "            layer1_indices, layer1_cache, dropout1_mask, layer2_indices, layer2_cache, dropout2_mask, layer3_indices, layer3_cache, dropout3_mask, layer4_indices, layer4_cache, dropout4_mask, layer5_cache, dropout5_mask = forward_propagation(\n",
    "                    test_input, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool,\n",
    "                    conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool,\n",
    "                    conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool,\n",
    "                    conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool,\n",
    "                    conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, \n",
    "                    fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output, dropout_rate = 0.25\n",
    "            )\n",
    "            prediction = get_prediction(layer6_2_output)\n",
    "            if np.argmax(true_label) == np.argmax(prediction):\n",
    "                train_accuracy += 1\n",
    "        print(\"Training Accuracy:\", train_accuracy / 100)\n",
    "        \n",
    "        # Get validation accuracy\n",
    "        val_accuracy = 0\n",
    "        for _ in range(100):\n",
    "            random_idx = np.random.randint(len(val_set))\n",
    "            test_input, true_label = val_set[random_idx]\n",
    "            layer1_indices, layer1_cache, dropout1_mask, layer2_indices, layer2_cache, dropout2_mask, layer3_indices, layer3_cache, dropout3_mask, layer4_indices, layer4_cache, dropout4_mask, layer5_cache, dropout5_mask = forward_propagation(\n",
    "                    test_input, conv1_1_kernel, conv1_1_bias, layer1_1_output, conv1_2_kernel, conv1_2_bias, layer1_2_output, gamma1_conv, beta1_conv, layer1_pool,\n",
    "                    conv2_1_kernel, conv2_1_bias, layer2_1_output, conv2_2_kernel, conv2_2_bias, layer2_2_output, conv2_3_kernel, conv2_3_bias, layer2_3_output, gamma2_conv, beta2_conv, layer2_pool,\n",
    "                    conv3_1_kernel, conv3_1_bias, layer3_1_output, conv3_2_kernel, conv3_2_bias, layer3_2_output, gamma3_conv, beta3_conv, layer3_pool,\n",
    "                    conv4_1_kernel, conv4_1_bias, layer4_1_output, conv4_2_kernel, conv4_2_bias, layer4_2_output, conv4_3_kernel, conv4_3_bias, layer4_3_output, gamma4_conv, beta4_conv, layer4_pool,\n",
    "                    conv5_1_kernel, conv5_1_bias, layer5_1_output, conv5_2_kernel, conv5_2_bias, layer5_2_output, gamma5_conv, beta5_conv, \n",
    "                    fc1_weights, fc1_bias, layer6_1_output, fc2_weights, fc2_bias, layer6_2_output, dropout_rate = 0.25\n",
    "            )\n",
    "            prediction = get_prediction(layer6_2_output)\n",
    "            if np.argmax(true_label) == np.argmax(prediction):\n",
    "                val_accuracy += 1\n",
    "        print(\"Validation Accuracy:\", val_accuracy / 100)\n",
    "        \n",
    "        # Save the parameters to a .npz file\n",
    "        np.savez('model_parameters.npz', **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "stochastic_gradient_descent(train_set, val_set, epochs, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ba6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loaded_params = np.load('model_parameters.npz')\n",
    "\n",
    "# Extract the parameters\n",
    "conv1_1_kernel = loaded_params['conv1_1_kernel']\n",
    "conv1_1_bias = loaded_params['conv1_1_bias']\n",
    "conv1_2_kernel = loaded_params['conv1_2_kernel']\n",
    "conv1_2_bias = loaded_params['conv1_2_bias']\n",
    "gamma1_conv = loaded_params['gamma1_conv']\n",
    "beta1_conv = loaded_params['beta1_conv']\n",
    "\n",
    "conv2_1_kernel = loaded_params['conv2_1_kernel']\n",
    "conv2_1_bias = loaded_params['conv2_1_bias']\n",
    "conv2_2_kernel = loaded_params['conv2_2_kernel']\n",
    "conv2_2_bias = loaded_params['conv2_2_bias']\n",
    "conv2_3_kernel = loaded_params['conv2_3_kernel']\n",
    "conv2_3_bias = loaded_params['conv2_3_bias']\n",
    "gamma2_conv = loaded_params['gamma2_conv']\n",
    "beta2_conv = loaded_params['beta2_conv']\n",
    "\n",
    "conv3_1_kernel = loaded_params['conv3_1_kernel']\n",
    "conv3_1_bias = loaded_params['conv3_1_bias']\n",
    "conv3_2_kernel = loaded_params['conv3_2_kernel']\n",
    "conv3_2_bias = loaded_params['conv3_2_bias']\n",
    "gamma3_conv = loaded_params['gamma3_conv']\n",
    "beta3_conv = loaded_params['beta3_conv']\n",
    "\n",
    "conv4_1_kernel = loaded_params['conv4_1_kernel']\n",
    "conv4_1_bias = loaded_params['conv4_1_bias']\n",
    "conv4_2_kernel = loaded_params['conv4_2_kernel']\n",
    "conv4_2_bias = loaded_params['conv4_2_bias']\n",
    "conv4_3_kernel = loaded_params['conv4_3_kernel']\n",
    "conv4_3_bias = loaded_params['conv4_3_bias']\n",
    "gamma4_conv = loaded_params['gamma4_conv']\n",
    "beta4_conv = loaded_params['beta4_conv']\n",
    "\n",
    "conv5_1_kernel = loaded_params['conv5_1_kernel']\n",
    "conv5_1_bias = loaded_params['conv5_1_bias']\n",
    "conv5_2_kernel = loaded_params['conv5_2_kernel']\n",
    "conv5_2_bias = loaded_params['conv5_2_bias']\n",
    "gamma5_conv = loaded_params['gamma5_conv']\n",
    "beta5_conv = loaded_params['beta5_conv']\n",
    "\n",
    "fc1_weights = loaded_params['fc1_weights']\n",
    "fc1_bias = loaded_params['fc1_bias']\n",
    "fc2_weights = loaded_params['fc2_weights']\n",
    "fc2_bias = loaded_params['fc2_bias']\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5137997,
     "sourceId": 8590005,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30715,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
