{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f74cad-a18b-4772-a59a-074086702076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0624881d-5967-451c-a759-0bc6805e5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 40000)\n",
      "(40000,)\n",
      "(28, 28, 1000)\n",
      "(1000,)\n",
      "(28, 28, 1000)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "\n",
    "np.random.shuffle(data) # Shuffles all the individual rows\n",
    "\n",
    "data_dev = data[0:1000].T #Take the first 1000 rows, and transpose the matrix to get 1000 examples as column vectors\n",
    "Y_dev = data_dev[0] #Takes the first row, which contains all of the answers to the numbers (the Y is what we want)\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev.reshape(28,28,1000)\n",
    "\n",
    "data_train = data[2000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train= data_train[1:n] #Takes all of the data corresponding to all of the entries\n",
    "X_train = X_train.reshape(28,28,-1)\n",
    "\n",
    "data_test = data[1000:2000].T\n",
    "Y_test = data_test[0]\n",
    "X_test= data_test[1:n] #Takes all of the data corresponding to all of the entries\n",
    "X_test = X_test.reshape(28,28,-1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5864e0c-acf3-4def-b894-6913bcc0d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_forward(x, gamma, beta, eps=1e-5):\n",
    "    mean = np.mean(x, axis=0)\n",
    "    variance = np.var(x, axis=0)\n",
    "    x_normalized = (x - mean) / np.sqrt(variance + eps)\n",
    "    out = gamma * x_normalized + beta\n",
    "    cache = (x, x_normalized, mean, variance, gamma, beta, eps)\n",
    "    return out, cache\n",
    "\n",
    "def batch_norm_backward(dout, cache):\n",
    "    x, x_normalized, mean, variance, gamma, beta, eps = cache\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgamma = np.sum(dout * x_normalized, axis=0)\n",
    "    \n",
    "    dx_normalized = dout * gamma\n",
    "    dvariance = np.sum(dx_normalized * (x - mean) * -0.5 * np.power(variance + eps, -1.5), axis=0)\n",
    "    dmean = np.sum(dx_normalized * -1 / np.sqrt(variance + eps), axis=0) + dvariance * np.sum(-2 * (x - mean), axis=0) / N\n",
    "    \n",
    "    dx = dx_normalized / np.sqrt(variance + eps) + dvariance * 2 * (x - mean) / N + dmean / N\n",
    "    return dx, dgamma, dbeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fb5c539-b2e6-4cea-ab1d-9f7580cd0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(input_data):\n",
    "    # 24, 24, 2\n",
    "    input_height, input_width, input_depth = input_data.shape\n",
    "\n",
    "    # Calculate the output dimensions\n",
    "    output_height = input_height // 2 # 12\n",
    "    output_width = input_width // 2 # 12\n",
    "    output_depth = input_depth # 2 - depth stays the same\n",
    "\n",
    "    # Initialize the output array and array to store indices\n",
    "    output_data = np.zeros((output_height, output_width, output_depth))\n",
    "    indices = np.zeros((output_height, output_width, output_depth, 2), dtype=int)\n",
    "\n",
    "    # Apply max pooling\n",
    "    for h in range(output_height):\n",
    "        for w in range(output_width):\n",
    "            for d in range(output_depth):\n",
    "                # Extract the 2x2 region of interest from the input data\n",
    "                region = input_data[h*2:(h+1)*2, w*2:(w+1)*2, d]\n",
    "                # Compute the maximum value in the region\n",
    "                max_val = np.max(region)\n",
    "                output_data[h, w, d] = max_val\n",
    "                # Find the indices of the maximum value in the region\n",
    "                max_indices = np.unravel_index(np.argmax(region), region.shape)\n",
    "                # Store the indices relative to the region and convert to global indices\n",
    "                indices[h, w, d] = [h*2 + max_indices[0], w*2 + max_indices[1]]\n",
    "\n",
    "    return output_data, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec77b82a-5105-48bd-bba6-eea0ae38b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_ReLU(Z):\n",
    "  return Z > 0\n",
    "\n",
    "def ReLU2(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "def ReLU(Z): # Takes in a scalar, returns a scalar\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU2(Z): # Takes in a scalar, returns a scalar\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "  return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Compute the sigmoid function element-wise\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def softmax(Z):\n",
    "    # Apply softmax column-wise\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0))  # Subtracting the maximum value in each column to avoid overflow\n",
    "    return exp_Z / np.sum(exp_Z, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ce95ada-3428-48ba-9156-6bcb650d8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def params():\n",
    "    layer_weights = np.random.randn(5, 5, 2) * np.sqrt(2. / 5)\n",
    "    layer_bias = np.zeros((24, 24, 2))\n",
    "    layer_output = np.zeros((24, 24, 2)) #(24, 24, 2)\n",
    "    fc_weights = np.random.randn(10, 288) * np.sqrt(2. / 288)\n",
    "    fc_bias = np.zeros((10, 1))\n",
    "    gamma_conv = np.ones((24, 24, 2))\n",
    "    beta_conv = np.zeros((24, 24, 2))\n",
    "    gamma_fc = np.ones((10, 1))\n",
    "    beta_fc = np.zeros((10, 1))\n",
    "    return layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "\n",
    "\n",
    "def forward_propagation(layer_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc, dropout_rate=0.5):\n",
    "    # Convolution\n",
    "    for i in range(2): # 2 filters in total\n",
    "        layer_output[:,:,i] = signal.correlate2d(layer_input, layer_weights[:,:,i], mode='valid')\n",
    "    layer_output = layer_output + layer_bias   # (24, 24, 2)\n",
    "    \n",
    "    # Batch Normalization for Convolutional Layer\n",
    "    layer_output, bn_cache_conv = batch_norm_forward(layer_output, gamma_conv, beta_conv)\n",
    "    \n",
    "    # Activation layer\n",
    "    layer_output = ReLU(layer_output)  # (24, 24, 2)\n",
    "    \n",
    "    # Pool layer\n",
    "    layer_pool, layer_indices = max_pooling(layer_output)  # (12, 12, 2)\n",
    "    \n",
    "    # Flattening\n",
    "    layer_pool = layer_pool.reshape(288, 1) # (288, 1)\n",
    "    \n",
    "    # Dropout\n",
    "    dropout_mask = (np.random.rand(*layer_pool.shape) < dropout_rate) / dropout_rate\n",
    "    layer_pool *= dropout_mask\n",
    "    \n",
    "    # Fully connected layer\n",
    "    final_output = fc_weights.dot(layer_pool)  # (10, 288) (288, 1) = (10, 1)\n",
    "    final_output = final_output + fc_bias # (10, 1) + (10, 1) = (10, 1)\n",
    "    \n",
    "    # Batch Normalization for Fully Connected Layer\n",
    "    final_output, bn_cache_fc = batch_norm_forward(final_output, gamma_fc, beta_fc)\n",
    "    \n",
    "    final_output = softmax(final_output) # (10, 1)\n",
    "    \n",
    "    return layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask\n",
    "\n",
    "\n",
    "def create(Y):\n",
    "  column_Y = np.zeros((10, 1))\n",
    "  column_Y[Y] = 1\n",
    "  column_Y = column_Y.T\n",
    "  return column_Y.reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea97843b-5871-4db2-bc7d-1baae6ec752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(layer_input, layer_output, layer_pool, layer_indices, final_output, label, layer_weights, layer_bias, fc_weights, fc_bias, bn_cache_conv, bn_cache_fc, dropout_mask):\n",
    "    # Initialize parameters\n",
    "    delta_conv_weights = np.zeros_like(layer_weights)\n",
    "    delta_conv_bias = np.zeros_like(layer_bias)\n",
    "    delta_fc_weights = np.zeros_like(fc_weights)\n",
    "    delta_fc_bias = np.zeros_like(fc_bias)\n",
    "    delta_fc_bias = delta_fc_bias.reshape(10, 1)\n",
    "    \n",
    "    # Backpropagate cost\n",
    "    x = create(label)\n",
    "    dZ = (final_output - x)  # (10, 1) - (10, 1) = (10, 1)\n",
    "    \n",
    "    # Backpropagate through Batch Normalization for Fully Connected Layer\n",
    "    dZ, dgamma_fc, dbeta_fc = batch_norm_backward(dZ, bn_cache_fc)\n",
    "    \n",
    "    # Backpropagate weights and biases for Fully Connected Layer\n",
    "    delta_fc_weights = dZ.dot(layer_pool.T)  # (10, 1) (1, 288) = (10, 288)\n",
    "    delta_fc_bias = dZ\n",
    "    \n",
    "    # Backpropagate error\n",
    "    dZ_pool_output = np.dot(fc_weights.T, dZ) * der_ReLU(layer_pool)  # (288, 10) (10, 1) = (288, 1)\n",
    "    \n",
    "    # Undo Dropout\n",
    "    dZ_pool_output *= dropout_mask\n",
    "    \n",
    "    # Unflattening\n",
    "    dZ_pool_output = dZ_pool_output.reshape(12, 12, 2)\n",
    "    \n",
    "    # Unpooling\n",
    "    dZ_pool_input = np.zeros((24, 24, 2))\n",
    "    for i in range(12):  # height\n",
    "        for j in range(12):  # width\n",
    "            for k in range(2):  # depth\n",
    "                # Get the global indices from layer_indices\n",
    "                x_index, y_index = layer_indices[i, j, k]\n",
    "                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n",
    "                dZ_pool_input[x_index, y_index, k] = dZ_pool_output[i, j, k]\n",
    "    \n",
    "    # Backpropagate through ReLU activation\n",
    "    dZ_pool_input *= der_ReLU(layer_output)\n",
    "    \n",
    "    # Backpropagate through Batch Normalization\n",
    "    dZ_pool_input, dgamma_conv, dbeta_conv = batch_norm_backward(dZ_pool_input, bn_cache_conv)\n",
    "    \n",
    "    # Backpropagate Conv layer\n",
    "    for i in range(2):  # For each filter in the kernel\n",
    "        delta_conv_weights[:, :, i] = signal.correlate(layer_input, dZ_pool_input[:, :, i], mode=\"valid\")\n",
    "    delta_conv_bias = dZ_pool_input\n",
    "    \n",
    "    return delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias, dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc\n",
    "\n",
    " \n",
    "def update_params(layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc,\n",
    "                  delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias,\n",
    "                  dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc, learning_rate):\n",
    "    layer_weights -= learning_rate * delta_conv_weights\n",
    "    layer_bias -= learning_rate * delta_conv_bias\n",
    "    fc_weights -= learning_rate * delta_fc_weights\n",
    "    fc_bias -= learning_rate * delta_fc_bias\n",
    "    gamma_conv -= learning_rate * dgamma_conv\n",
    "    beta_conv -= learning_rate * dbeta_conv\n",
    "    gamma_fc -= learning_rate * dgamma_fc\n",
    "    beta_fc -= learning_rate * dbeta_fc\n",
    "    return layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "\n",
    "\n",
    "def get_prediction(A2):\n",
    "  return np.argmax(A2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "82019a37-ccc9-4530-86e4-4ef9272aaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X_train, X_dev, Y_train, Y_dev, epochs, learning_rate, batch_size):\n",
    "    # Initialize parameters\n",
    "    layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = params()\n",
    "    gamma_conv = np.ones((24, 24, 2))\n",
    "    beta_conv = np.zeros((24, 24, 2))\n",
    "    gamma_fc = np.ones((10, 1))\n",
    "    beta_fc = np.zeros((10, 1))\n",
    "    num_examples = X_train.shape[2]\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch:\", i + 1)\n",
    "        \n",
    "        # Generate a random permutation of indices\n",
    "        permuted_indices = np.random.permutation(X_train.shape[2])\n",
    "        \n",
    "        # Shuffle both X_train and Y_train using the same permutation of indices\n",
    "        X_train_shuffled = X_train[:, :, permuted_indices]\n",
    "        Y_train_shuffled = Y_train[permuted_indices]\n",
    "        \n",
    "        for batch_start in range(0, len(X_train_shuffled), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, num_examples)\n",
    "            batch_gradients = [0, 0, 0, 0, 0, 0, 0, 0]  # Accumulate gradients over the batch\n",
    "            for j in range(batch_start, batch_end):\n",
    "                # Get a single training example\n",
    "                layer_input = X_train_shuffled[:, :, j]\n",
    "                label = Y_train_shuffled[j]\n",
    "                \n",
    "                # Forward propagation\n",
    "                layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask = forward_propagation(\n",
    "                    layer_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "                )\n",
    "                \n",
    "                # Back propagation\n",
    "                delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias, dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc = back_prop(\n",
    "                    layer_input, layer_output, layer_pool, layer_indices, final_output, label,\n",
    "                    layer_weights, layer_bias, fc_weights, fc_bias, bn_cache_conv, bn_cache_fc, dropout_mask\n",
    "                )\n",
    "                \n",
    "                # Accumulate gradients\n",
    "                batch_gradients[0] += delta_conv_weights\n",
    "                batch_gradients[1] += delta_conv_bias\n",
    "                batch_gradients[2] += delta_fc_weights\n",
    "                batch_gradients[3] += delta_fc_bias\n",
    "                batch_gradients[4] += dgamma_conv\n",
    "                batch_gradients[5] += dbeta_conv\n",
    "                batch_gradients[6] += dgamma_fc\n",
    "                batch_gradients[7] += dbeta_fc\n",
    "            \n",
    "            # Average gradients after processing the batch\n",
    "            batch_gradients = [grad / batch_size for grad in batch_gradients]\n",
    "            \n",
    "            # Update parameters after processing the batch\n",
    "            layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = update_params(\n",
    "                layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc,\n",
    "                *batch_gradients,  # Use averaged gradients\n",
    "                learning_rate\n",
    "            )\n",
    "        \n",
    "        # Get training accuracy\n",
    "        counter = 0\n",
    "        for j in range(int(X_train_shuffled.shape[2]/100)):\n",
    "            test_input = X_train_shuffled[:, :, j]\n",
    "            layer_output, layer_pool, layer_indices, final_output, _, _, _ = forward_propagation(\n",
    "                test_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "            )\n",
    "            prediction = get_prediction(final_output)\n",
    "            predicted_label = prediction[0]\n",
    "            if Y_train_shuffled[j] == predicted_label:\n",
    "                counter += 1\n",
    "        print(\"Training Accuracy:\", counter / int(X_train_shuffled.shape[2]/100))\n",
    "        counter = 0\n",
    "        for j in range(500):\n",
    "            test_input = X_dev[:, :, j]\n",
    "            layer_output, layer_pool, layer_indices, final_output, _, _, _ = forward_propagation(\n",
    "                test_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "            )\n",
    "            prediction = get_prediction(final_output)\n",
    "            predicted_label = prediction[0]\n",
    "            if Y_dev[j] == predicted_label:\n",
    "                counter += 1\n",
    "        print(\"Validation Accuracy:\", counter / 500)\n",
    "    return layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d00daad9-e573-40a7-bb41-e8c87e4b6b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Accuracy: 0.135\n",
      "Validation Accuracy: 0.13\n",
      "Epoch: 2\n",
      "Training Accuracy: 0.2075\n",
      "Validation Accuracy: 0.192\n",
      "Epoch: 3\n",
      "Training Accuracy: 0.2075\n",
      "Validation Accuracy: 0.186\n",
      "Epoch: 4\n",
      "Training Accuracy: 0.2675\n",
      "Validation Accuracy: 0.244\n",
      "Epoch: 5\n",
      "Training Accuracy: 0.265\n",
      "Validation Accuracy: 0.228\n",
      "Epoch: 6\n",
      "Training Accuracy: 0.315\n",
      "Validation Accuracy: 0.246\n",
      "Epoch: 7\n",
      "Training Accuracy: 0.285\n",
      "Validation Accuracy: 0.25\n",
      "Epoch: 8\n",
      "Training Accuracy: 0.29\n",
      "Validation Accuracy: 0.292\n",
      "Epoch: 9\n",
      "Training Accuracy: 0.32\n",
      "Validation Accuracy: 0.292\n",
      "Epoch: 10\n",
      "Training Accuracy: 0.3575\n",
      "Validation Accuracy: 0.316\n",
      "Epoch: 11\n",
      "Training Accuracy: 0.3575\n",
      "Validation Accuracy: 0.302\n",
      "Epoch: 12\n",
      "Training Accuracy: 0.395\n",
      "Validation Accuracy: 0.42\n",
      "Epoch: 13\n",
      "Training Accuracy: 0.355\n",
      "Validation Accuracy: 0.3\n",
      "Epoch: 14\n",
      "Training Accuracy: 0.42\n",
      "Validation Accuracy: 0.374\n",
      "Epoch: 15\n",
      "Training Accuracy: 0.4025\n",
      "Validation Accuracy: 0.398\n",
      "Epoch: 16\n",
      "Training Accuracy: 0.4275\n",
      "Validation Accuracy: 0.426\n",
      "Epoch: 17\n",
      "Training Accuracy: 0.4575\n",
      "Validation Accuracy: 0.426\n",
      "Epoch: 18\n",
      "Training Accuracy: 0.4775\n",
      "Validation Accuracy: 0.442\n",
      "Epoch: 19\n",
      "Training Accuracy: 0.4875\n",
      "Validation Accuracy: 0.502\n",
      "Epoch: 20\n",
      "Training Accuracy: 0.4925\n",
      "Validation Accuracy: 0.526\n",
      "Epoch: 21\n",
      "Training Accuracy: 0.4475\n",
      "Validation Accuracy: 0.506\n",
      "Epoch: 22\n",
      "Training Accuracy: 0.5075\n",
      "Validation Accuracy: 0.48\n",
      "Epoch: 23\n",
      "Training Accuracy: 0.5725\n",
      "Validation Accuracy: 0.564\n",
      "Epoch: 24\n",
      "Training Accuracy: 0.52\n",
      "Validation Accuracy: 0.572\n",
      "Epoch: 25\n",
      "Training Accuracy: 0.57\n",
      "Validation Accuracy: 0.574\n",
      "Epoch: 26\n",
      "Training Accuracy: 0.5175\n",
      "Validation Accuracy: 0.594\n",
      "Epoch: 27\n",
      "Training Accuracy: 0.565\n",
      "Validation Accuracy: 0.584\n",
      "Epoch: 28\n",
      "Training Accuracy: 0.5275\n",
      "Validation Accuracy: 0.576\n",
      "Epoch: 29\n",
      "Training Accuracy: 0.585\n",
      "Validation Accuracy: 0.59\n",
      "Epoch: 30\n",
      "Training Accuracy: 0.615\n",
      "Validation Accuracy: 0.656\n",
      "Epoch: 31\n",
      "Training Accuracy: 0.6025\n",
      "Validation Accuracy: 0.622\n",
      "Epoch: 32\n",
      "Training Accuracy: 0.6\n",
      "Validation Accuracy: 0.616\n",
      "Epoch: 33\n",
      "Training Accuracy: 0.615\n",
      "Validation Accuracy: 0.596\n",
      "Epoch: 34\n",
      "Training Accuracy: 0.615\n",
      "Validation Accuracy: 0.638\n",
      "Epoch: 35\n",
      "Training Accuracy: 0.66\n",
      "Validation Accuracy: 0.63\n",
      "Epoch: 36\n",
      "Training Accuracy: 0.585\n",
      "Validation Accuracy: 0.58\n",
      "Epoch: 37\n",
      "Training Accuracy: 0.645\n",
      "Validation Accuracy: 0.66\n",
      "Epoch: 38\n",
      "Training Accuracy: 0.66\n",
      "Validation Accuracy: 0.618\n",
      "Epoch: 39\n",
      "Training Accuracy: 0.655\n",
      "Validation Accuracy: 0.686\n",
      "Epoch: 40\n",
      "Training Accuracy: 0.6625\n",
      "Validation Accuracy: 0.68\n",
      "Epoch: 41\n",
      "Training Accuracy: 0.6875\n",
      "Validation Accuracy: 0.684\n",
      "Epoch: 42\n",
      "Training Accuracy: 0.6525\n",
      "Validation Accuracy: 0.658\n",
      "Epoch: 43\n",
      "Training Accuracy: 0.665\n",
      "Validation Accuracy: 0.696\n",
      "Epoch: 44\n",
      "Training Accuracy: 0.7\n",
      "Validation Accuracy: 0.682\n",
      "Epoch: 45\n",
      "Training Accuracy: 0.7025\n",
      "Validation Accuracy: 0.68\n",
      "Epoch: 46\n",
      "Training Accuracy: 0.6425\n",
      "Validation Accuracy: 0.682\n",
      "Epoch: 47\n",
      "Training Accuracy: 0.695\n",
      "Validation Accuracy: 0.702\n",
      "Epoch: 48\n",
      "Training Accuracy: 0.6775\n",
      "Validation Accuracy: 0.734\n",
      "Epoch: 49\n",
      "Training Accuracy: 0.67\n",
      "Validation Accuracy: 0.67\n",
      "Epoch: 50\n",
      "Training Accuracy: 0.7\n",
      "Validation Accuracy: 0.734\n",
      "Epoch: 51\n",
      "Training Accuracy: 0.7025\n",
      "Validation Accuracy: 0.71\n",
      "Epoch: 52\n",
      "Training Accuracy: 0.715\n",
      "Validation Accuracy: 0.736\n",
      "Epoch: 53\n",
      "Training Accuracy: 0.6625\n",
      "Validation Accuracy: 0.708\n",
      "Epoch: 54\n",
      "Training Accuracy: 0.685\n",
      "Validation Accuracy: 0.724\n",
      "Epoch: 55\n",
      "Training Accuracy: 0.7275\n",
      "Validation Accuracy: 0.702\n",
      "Epoch: 56\n",
      "Training Accuracy: 0.6975\n",
      "Validation Accuracy: 0.71\n",
      "Epoch: 57\n",
      "Training Accuracy: 0.7525\n",
      "Validation Accuracy: 0.716\n",
      "Epoch: 58\n",
      "Training Accuracy: 0.69\n",
      "Validation Accuracy: 0.71\n",
      "Epoch: 59\n",
      "Training Accuracy: 0.6925\n",
      "Validation Accuracy: 0.712\n",
      "Epoch: 60\n",
      "Training Accuracy: 0.7125\n",
      "Validation Accuracy: 0.736\n",
      "Epoch: 61\n",
      "Training Accuracy: 0.7175\n",
      "Validation Accuracy: 0.744\n",
      "Epoch: 62\n",
      "Training Accuracy: 0.74\n",
      "Validation Accuracy: 0.704\n",
      "Epoch: 63\n",
      "Training Accuracy: 0.74\n",
      "Validation Accuracy: 0.728\n",
      "Epoch: 64\n",
      "Training Accuracy: 0.6825\n",
      "Validation Accuracy: 0.734\n",
      "Epoch: 65\n",
      "Training Accuracy: 0.745\n",
      "Validation Accuracy: 0.776\n",
      "Epoch: 66\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.738\n",
      "Epoch: 67\n",
      "Training Accuracy: 0.73\n",
      "Validation Accuracy: 0.73\n",
      "Epoch: 68\n",
      "Training Accuracy: 0.7075\n",
      "Validation Accuracy: 0.7\n",
      "Epoch: 69\n",
      "Training Accuracy: 0.6825\n",
      "Validation Accuracy: 0.724\n",
      "Epoch: 70\n",
      "Training Accuracy: 0.7375\n",
      "Validation Accuracy: 0.768\n",
      "Epoch: 71\n",
      "Training Accuracy: 0.765\n",
      "Validation Accuracy: 0.754\n",
      "Epoch: 72\n",
      "Training Accuracy: 0.7325\n",
      "Validation Accuracy: 0.756\n",
      "Epoch: 73\n",
      "Training Accuracy: 0.69\n",
      "Validation Accuracy: 0.692\n",
      "Epoch: 74\n",
      "Training Accuracy: 0.7275\n",
      "Validation Accuracy: 0.77\n",
      "Epoch: 75\n",
      "Training Accuracy: 0.7275\n",
      "Validation Accuracy: 0.774\n",
      "Epoch: 76\n",
      "Training Accuracy: 0.7225\n",
      "Validation Accuracy: 0.788\n",
      "Epoch: 77\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.776\n",
      "Epoch: 78\n",
      "Training Accuracy: 0.7525\n",
      "Validation Accuracy: 0.76\n",
      "Epoch: 79\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.762\n",
      "Epoch: 80\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.784\n",
      "Epoch: 81\n",
      "Training Accuracy: 0.7725\n",
      "Validation Accuracy: 0.76\n",
      "Epoch: 82\n",
      "Training Accuracy: 0.775\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 83\n",
      "Training Accuracy: 0.73\n",
      "Validation Accuracy: 0.79\n",
      "Epoch: 84\n",
      "Training Accuracy: 0.7625\n",
      "Validation Accuracy: 0.768\n",
      "Epoch: 85\n",
      "Training Accuracy: 0.74\n",
      "Validation Accuracy: 0.748\n",
      "Epoch: 86\n",
      "Training Accuracy: 0.77\n",
      "Validation Accuracy: 0.766\n",
      "Epoch: 87\n",
      "Training Accuracy: 0.7125\n",
      "Validation Accuracy: 0.722\n",
      "Epoch: 88\n",
      "Training Accuracy: 0.78\n",
      "Validation Accuracy: 0.804\n",
      "Epoch: 89\n",
      "Training Accuracy: 0.76\n",
      "Validation Accuracy: 0.802\n",
      "Epoch: 90\n",
      "Training Accuracy: 0.775\n",
      "Validation Accuracy: 0.76\n",
      "Epoch: 91\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.77\n",
      "Epoch: 92\n",
      "Training Accuracy: 0.7625\n",
      "Validation Accuracy: 0.802\n",
      "Epoch: 93\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 94\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.804\n",
      "Epoch: 95\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.8\n",
      "Epoch: 96\n",
      "Training Accuracy: 0.7525\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 97\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.776\n",
      "Epoch: 98\n",
      "Training Accuracy: 0.7375\n",
      "Validation Accuracy: 0.782\n",
      "Epoch: 99\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.762\n",
      "Epoch: 100\n",
      "Training Accuracy: 0.7825\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 101\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.816\n",
      "Epoch: 102\n",
      "Training Accuracy: 0.78\n",
      "Validation Accuracy: 0.808\n",
      "Epoch: 103\n",
      "Training Accuracy: 0.795\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 104\n",
      "Training Accuracy: 0.7725\n",
      "Validation Accuracy: 0.752\n",
      "Epoch: 105\n",
      "Training Accuracy: 0.785\n",
      "Validation Accuracy: 0.802\n",
      "Epoch: 106\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.812\n",
      "Epoch: 107\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.754\n",
      "Epoch: 108\n",
      "Training Accuracy: 0.765\n",
      "Validation Accuracy: 0.784\n",
      "Epoch: 109\n",
      "Training Accuracy: 0.785\n",
      "Validation Accuracy: 0.79\n",
      "Epoch: 110\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 111\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.804\n",
      "Epoch: 112\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.83\n",
      "Epoch: 113\n",
      "Training Accuracy: 0.77\n",
      "Validation Accuracy: 0.81\n",
      "Epoch: 114\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 115\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 116\n",
      "Training Accuracy: 0.79\n",
      "Validation Accuracy: 0.812\n",
      "Epoch: 117\n",
      "Training Accuracy: 0.8075\n",
      "Validation Accuracy: 0.802\n",
      "Epoch: 118\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.798\n",
      "Epoch: 119\n",
      "Training Accuracy: 0.78\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 120\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.81\n",
      "Epoch: 121\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.766\n",
      "Epoch: 122\n",
      "Training Accuracy: 0.775\n",
      "Validation Accuracy: 0.79\n",
      "Epoch: 123\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 124\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 125\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.802\n",
      "Epoch: 126\n",
      "Training Accuracy: 0.79\n",
      "Validation Accuracy: 0.8\n",
      "Epoch: 127\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 128\n",
      "Training Accuracy: 0.7775\n",
      "Validation Accuracy: 0.78\n",
      "Epoch: 129\n",
      "Training Accuracy: 0.7825\n",
      "Validation Accuracy: 0.796\n",
      "Epoch: 130\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 131\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.812\n",
      "Epoch: 132\n",
      "Training Accuracy: 0.7725\n",
      "Validation Accuracy: 0.81\n",
      "Epoch: 133\n",
      "Training Accuracy: 0.865\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 134\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 135\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 136\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 137\n",
      "Training Accuracy: 0.7625\n",
      "Validation Accuracy: 0.786\n",
      "Epoch: 138\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.8\n",
      "Epoch: 139\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 140\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.816\n",
      "Epoch: 141\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 142\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 143\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 144\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.81\n",
      "Epoch: 145\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 146\n",
      "Training Accuracy: 0.795\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 147\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 148\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 149\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.792\n",
      "Epoch: 150\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 151\n",
      "Training Accuracy: 0.79\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 152\n",
      "Training Accuracy: 0.7925\n",
      "Validation Accuracy: 0.792\n",
      "Epoch: 153\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 154\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 155\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.812\n",
      "Epoch: 156\n",
      "Training Accuracy: 0.76\n",
      "Validation Accuracy: 0.8\n",
      "Epoch: 157\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 158\n",
      "Training Accuracy: 0.8125\n",
      "Validation Accuracy: 0.81\n",
      "Epoch: 159\n",
      "Training Accuracy: 0.8175\n",
      "Validation Accuracy: 0.83\n",
      "Epoch: 160\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.782\n",
      "Epoch: 161\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 162\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 163\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 164\n",
      "Training Accuracy: 0.7825\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 165\n",
      "Training Accuracy: 0.8175\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 166\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 167\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 168\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 169\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 170\n",
      "Training Accuracy: 0.8125\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 171\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.792\n",
      "Epoch: 172\n",
      "Training Accuracy: 0.8175\n",
      "Validation Accuracy: 0.824\n",
      "Epoch: 173\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 174\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 175\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.816\n",
      "Epoch: 176\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 177\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 178\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 179\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 180\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 181\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 182\n",
      "Training Accuracy: 0.7925\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 183\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 184\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.824\n",
      "Epoch: 185\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 186\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 187\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 188\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 189\n",
      "Training Accuracy: 0.79\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 190\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 191\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 192\n",
      "Training Accuracy: 0.7575\n",
      "Validation Accuracy: 0.774\n",
      "Epoch: 193\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 194\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 195\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 196\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 197\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.816\n",
      "Epoch: 198\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 199\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 200\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.782\n",
      "Epoch: 201\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 202\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 203\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.824\n",
      "Epoch: 204\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 205\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 206\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 207\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 208\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 209\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 210\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 211\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 212\n",
      "Training Accuracy: 0.8125\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 213\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 214\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 215\n",
      "Training Accuracy: 0.775\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 216\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 217\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 218\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.79\n",
      "Epoch: 219\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.824\n",
      "Epoch: 220\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 221\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.796\n",
      "Epoch: 222\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 223\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 224\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.83\n",
      "Epoch: 225\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.816\n",
      "Epoch: 226\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 227\n",
      "Training Accuracy: 0.7775\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 228\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 229\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 230\n",
      "Training Accuracy: 0.8575\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 231\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 232\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 233\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 234\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 235\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 236\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 237\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 238\n",
      "Training Accuracy: 0.8575\n",
      "Validation Accuracy: 0.83\n",
      "Epoch: 239\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 240\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 241\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.808\n",
      "Epoch: 242\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 243\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 244\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 245\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 246\n",
      "Training Accuracy: 0.8125\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 247\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 248\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 249\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 250\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 251\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 252\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 253\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 254\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 255\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 256\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 257\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 258\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 259\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 260\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 261\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 262\n",
      "Training Accuracy: 0.88\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 263\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 264\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 265\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 266\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 267\n",
      "Training Accuracy: 0.8075\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 268\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 269\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 270\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 271\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 272\n",
      "Training Accuracy: 0.8075\n",
      "Validation Accuracy: 0.878\n",
      "Epoch: 273\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 274\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 275\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 276\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 277\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 278\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.86\n",
      "Epoch: 279\n",
      "Training Accuracy: 0.8075\n",
      "Validation Accuracy: 0.886\n",
      "Epoch: 280\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 281\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 282\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.872\n",
      "Epoch: 283\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 284\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 285\n",
      "Training Accuracy: 0.8125\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 286\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 287\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.872\n",
      "Epoch: 288\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 289\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.878\n",
      "Epoch: 290\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 291\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 292\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 293\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 294\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 295\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 296\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.858\n",
      "Epoch: 297\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.88\n",
      "Epoch: 298\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 299\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 300\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.808\n",
      "Epoch: 301\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 302\n",
      "Training Accuracy: 0.8675\n",
      "Validation Accuracy: 0.824\n",
      "Epoch: 303\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.858\n",
      "Epoch: 304\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 305\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 306\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 307\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 308\n",
      "Training Accuracy: 0.8675\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 309\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.888\n",
      "Epoch: 310\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.86\n",
      "Epoch: 311\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 312\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 313\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.858\n",
      "Epoch: 314\n",
      "Training Accuracy: 0.875\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 315\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 316\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 317\n",
      "Training Accuracy: 0.8075\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 318\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 319\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 320\n",
      "Training Accuracy: 0.875\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 321\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.878\n",
      "Epoch: 322\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 323\n",
      "Training Accuracy: 0.875\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 324\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.86\n",
      "Epoch: 325\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 326\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 327\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 328\n",
      "Training Accuracy: 0.8675\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 329\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 330\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 331\n",
      "Training Accuracy: 0.8875\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 332\n",
      "Training Accuracy: 0.865\n",
      "Validation Accuracy: 0.88\n",
      "Epoch: 333\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 334\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 335\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 336\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 337\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 338\n",
      "Training Accuracy: 0.8725\n",
      "Validation Accuracy: 0.886\n",
      "Epoch: 339\n",
      "Training Accuracy: 0.88\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 340\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 341\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 342\n",
      "Training Accuracy: 0.8575\n",
      "Validation Accuracy: 0.858\n",
      "Epoch: 343\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.858\n",
      "Epoch: 344\n",
      "Training Accuracy: 0.865\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 345\n",
      "Training Accuracy: 0.8575\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 346\n",
      "Training Accuracy: 0.865\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 347\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 348\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 349\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 350\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 351\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 352\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 353\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 354\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 355\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 356\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.872\n",
      "Epoch: 357\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 358\n",
      "Training Accuracy: 0.8725\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 359\n",
      "Training Accuracy: 0.8725\n",
      "Validation Accuracy: 0.878\n",
      "Epoch: 360\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 361\n",
      "Training Accuracy: 0.8725\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 362\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 363\n",
      "Training Accuracy: 0.875\n",
      "Validation Accuracy: 0.88\n",
      "Epoch: 364\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.86\n",
      "Epoch: 365\n",
      "Training Accuracy: 0.8675\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 366\n",
      "Training Accuracy: 0.8775\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 367\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 368\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 369\n",
      "Training Accuracy: 0.8675\n",
      "Validation Accuracy: 0.872\n",
      "Epoch: 370\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 371\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.86\n",
      "Epoch: 372\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 373\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 374\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 375\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 376\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 377\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.876\n",
      "Epoch: 378\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 379\n",
      "Training Accuracy: 0.88\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 380\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.872\n",
      "Epoch: 381\n",
      "Training Accuracy: 0.8575\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 382\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 383\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.876\n",
      "Epoch: 384\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 385\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 386\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.876\n",
      "Epoch: 387\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 388\n",
      "Training Accuracy: 0.875\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 389\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 390\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.88\n",
      "Epoch: 391\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 392\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 393\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 394\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 395\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 396\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 397\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.878\n",
      "Epoch: 398\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 399\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 400\n",
      "Training Accuracy: 0.88\n",
      "Validation Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = stochastic_gradient_descent(\n",
    "    X_train, X_dev, Y_train, Y_dev, 400, 0.2, 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e4adbc49-f47e-4a24-92ae-822565c2c984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[2]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1df4f1a6-6227-4556-8395-3a9d1a2d20e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"layer weights\", layer_weights)\n",
    "# print(\"layer bias\", layer_bias)\n",
    "# print(\"layer output\", layer_output)\n",
    "# print(\"fc weights\", fc_weights)\n",
    "# print(\"fc_bias\", fc_bias)\n",
    "plt.imshow(layer_output[:,:,1], cmap='gray')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()\n",
    "plt.imshow(layer_output[:,:,0], cmap='gray')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e98a1531-056b-41dd-9bb5-550c39b421fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in range(1000):\n",
    "    test = X_dev[:,:,i]\n",
    "    label = Y_dev[i]\n",
    "    # print(label)\n",
    "    # plt.imshow(test, cmap='gray')\n",
    "    # plt.axis('off')  # Turn off axis\n",
    "    # plt.show()\n",
    "    # print(\"Testing model:\")\n",
    "    _, _, _, final_output, _, _, _ = forward_propagation(\n",
    "                    test, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "                )\n",
    "    # print(\"Model prediction: \")\n",
    "    prediction = get_prediction(final_output)\n",
    "    if (prediction == label):\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
=======
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f74cad-a18b-4772-a59a-074086702076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0624881d-5967-451c-a759-0bc6805e5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 40000)\n",
      "(40000,)\n",
      "(28, 28, 1000)\n",
      "(1000,)\n",
      "(28, 28, 1000)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "\n",
    "np.random.shuffle(data) # Shuffles all the individual rows\n",
    "\n",
    "data_dev = data[0:1000].T #Take the first 1000 rows, and transpose the matrix to get 1000 examples as column vectors\n",
    "Y_dev = data_dev[0] #Takes the first row, which contains all of the answers to the numbers (the Y is what we want)\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev.reshape(28,28,1000)\n",
    "\n",
    "data_train = data[2000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train= data_train[1:n] #Takes all of the data corresponding to all of the entries\n",
    "X_train = X_train.reshape(28,28,-1)\n",
    "\n",
    "data_test = data[1000:2000].T\n",
    "Y_test = data_test[0]\n",
    "X_test= data_test[1:n] #Takes all of the data corresponding to all of the entries\n",
    "X_test = X_test.reshape(28,28,-1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5864e0c-acf3-4def-b894-6913bcc0d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_forward(x, gamma, beta, eps=1e-5):\n",
    "    mean = np.mean(x, axis=0)\n",
    "    variance = np.var(x, axis=0)\n",
    "    x_normalized = (x - mean) / np.sqrt(variance + eps)\n",
    "    out = gamma * x_normalized + beta\n",
    "    cache = (x, x_normalized, mean, variance, gamma, beta, eps)\n",
    "    return out, cache\n",
    "\n",
    "def batch_norm_backward(dout, cache):\n",
    "    x, x_normalized, mean, variance, gamma, beta, eps = cache\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgamma = np.sum(dout * x_normalized, axis=0)\n",
    "    \n",
    "    dx_normalized = dout * gamma\n",
    "    dvariance = np.sum(dx_normalized * (x - mean) * -0.5 * np.power(variance + eps, -1.5), axis=0)\n",
    "    dmean = np.sum(dx_normalized * -1 / np.sqrt(variance + eps), axis=0) + dvariance * np.sum(-2 * (x - mean), axis=0) / N\n",
    "    \n",
    "    dx = dx_normalized / np.sqrt(variance + eps) + dvariance * 2 * (x - mean) / N + dmean / N\n",
    "    return dx, dgamma, dbeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fb5c539-b2e6-4cea-ab1d-9f7580cd0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(input_data):\n",
    "    # 24, 24, 2\n",
    "    input_height, input_width, input_depth = input_data.shape\n",
    "\n",
    "    # Calculate the output dimensions\n",
    "    output_height = input_height // 2 # 12\n",
    "    output_width = input_width // 2 # 12\n",
    "    output_depth = input_depth # 2 - depth stays the same\n",
    "\n",
    "    # Initialize the output array and array to store indices\n",
    "    output_data = np.zeros((output_height, output_width, output_depth))\n",
    "    indices = np.zeros((output_height, output_width, output_depth, 2), dtype=int)\n",
    "\n",
    "    # Apply max pooling\n",
    "    for h in range(output_height):\n",
    "        for w in range(output_width):\n",
    "            for d in range(output_depth):\n",
    "                # Extract the 2x2 region of interest from the input data\n",
    "                region = input_data[h*2:(h+1)*2, w*2:(w+1)*2, d]\n",
    "                # Compute the maximum value in the region\n",
    "                max_val = np.max(region)\n",
    "                output_data[h, w, d] = max_val\n",
    "                # Find the indices of the maximum value in the region\n",
    "                max_indices = np.unravel_index(np.argmax(region), region.shape)\n",
    "                # Store the indices relative to the region and convert to global indices\n",
    "                indices[h, w, d] = [h*2 + max_indices[0], w*2 + max_indices[1]]\n",
    "\n",
    "    return output_data, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec77b82a-5105-48bd-bba6-eea0ae38b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_ReLU(Z):\n",
    "  return Z > 0\n",
    "\n",
    "def ReLU2(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "def ReLU(Z): # Takes in a scalar, returns a scalar\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU2(Z): # Takes in a scalar, returns a scalar\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "  return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Compute the sigmoid function element-wise\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def softmax(Z):\n",
    "    # Apply softmax column-wise\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0))  # Subtracting the maximum value in each column to avoid overflow\n",
    "    return exp_Z / np.sum(exp_Z, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ce95ada-3428-48ba-9156-6bcb650d8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def params():\n",
    "    layer_weights = np.random.randn(5, 5, 2) * np.sqrt(2. / 5)\n",
    "    layer_bias = np.zeros((24, 24, 2))\n",
    "    layer_output = np.zeros((24, 24, 2)) #(24, 24, 2)\n",
    "    fc_weights = np.random.randn(10, 288) * np.sqrt(2. / 288)\n",
    "    fc_bias = np.zeros((10, 1))\n",
    "    gamma_conv = np.ones((24, 24, 2))\n",
    "    beta_conv = np.zeros((24, 24, 2))\n",
    "    gamma_fc = np.ones((10, 1))\n",
    "    beta_fc = np.zeros((10, 1))\n",
    "    return layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "\n",
    "\n",
    "def forward_propagation(layer_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc, dropout_rate=0.5):\n",
    "    # Convolution\n",
    "    for i in range(2): # 2 filters in total\n",
    "        layer_output[:,:,i] = signal.correlate2d(layer_input, layer_weights[:,:,i], mode='valid')\n",
    "    layer_output = layer_output + layer_bias   # (24, 24, 2)\n",
    "    \n",
    "    # Batch Normalization for Convolutional Layer\n",
    "    layer_output, bn_cache_conv = batch_norm_forward(layer_output, gamma_conv, beta_conv)\n",
    "    \n",
    "    # Activation layer\n",
    "    layer_output = ReLU(layer_output)  # (24, 24, 2)\n",
    "    \n",
    "    # Pool layer\n",
    "    layer_pool, layer_indices = max_pooling(layer_output)  # (12, 12, 2)\n",
    "    \n",
    "    # Flattening\n",
    "    layer_pool = layer_pool.reshape(288, 1) # (288, 1)\n",
    "    \n",
    "    # Dropout\n",
    "    dropout_mask = (np.random.rand(*layer_pool.shape) < dropout_rate) / dropout_rate\n",
    "    layer_pool *= dropout_mask\n",
    "    \n",
    "    # Fully connected layer\n",
    "    final_output = fc_weights.dot(layer_pool)  # (10, 288) (288, 1) = (10, 1)\n",
    "    final_output = final_output + fc_bias # (10, 1) + (10, 1) = (10, 1)\n",
    "    \n",
    "    # Batch Normalization for Fully Connected Layer\n",
    "    final_output, bn_cache_fc = batch_norm_forward(final_output, gamma_fc, beta_fc)\n",
    "    \n",
    "    final_output = softmax(final_output) # (10, 1)\n",
    "    \n",
    "    return layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask\n",
    "\n",
    "\n",
    "def create(Y):\n",
    "  column_Y = np.zeros((10, 1))\n",
    "  column_Y[Y] = 1\n",
    "  column_Y = column_Y.T\n",
    "  return column_Y.reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea97843b-5871-4db2-bc7d-1baae6ec752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(layer_input, layer_output, layer_pool, layer_indices, final_output, label, layer_weights, layer_bias, fc_weights, fc_bias, bn_cache_conv, bn_cache_fc, dropout_mask):\n",
    "    # Initialize parameters\n",
    "    delta_conv_weights = np.zeros_like(layer_weights)\n",
    "    delta_conv_bias = np.zeros_like(layer_bias)\n",
    "    delta_fc_weights = np.zeros_like(fc_weights)\n",
    "    delta_fc_bias = np.zeros_like(fc_bias)\n",
    "    delta_fc_bias = delta_fc_bias.reshape(10, 1)\n",
    "    \n",
    "    # Backpropagate cost\n",
    "    x = create(label)\n",
    "    dZ = (final_output - x)  # (10, 1) - (10, 1) = (10, 1)\n",
    "    \n",
    "    # Backpropagate through Batch Normalization for Fully Connected Layer\n",
    "    dZ, dgamma_fc, dbeta_fc = batch_norm_backward(dZ, bn_cache_fc)\n",
    "    \n",
    "    # Backpropagate weights and biases for Fully Connected Layer\n",
    "    delta_fc_weights = dZ.dot(layer_pool.T)  # (10, 1) (1, 288) = (10, 288)\n",
    "    delta_fc_bias = dZ\n",
    "    \n",
    "    # Backpropagate error\n",
    "    dZ_pool_output = np.dot(fc_weights.T, dZ) * der_ReLU(layer_pool)  # (288, 10) (10, 1) = (288, 1)\n",
    "    \n",
    "    # Undo Dropout\n",
    "    dZ_pool_output *= dropout_mask\n",
    "    \n",
    "    # Unflattening\n",
    "    dZ_pool_output = dZ_pool_output.reshape(12, 12, 2)\n",
    "    \n",
    "    # Unpooling\n",
    "    dZ_pool_input = np.zeros((24, 24, 2))\n",
    "    for i in range(12):  # height\n",
    "        for j in range(12):  # width\n",
    "            for k in range(2):  # depth\n",
    "                # Get the global indices from layer_indices\n",
    "                x_index, y_index = layer_indices[i, j, k]\n",
    "                # Assign the gradient from dZ_pool_output to the corresponding position in dZ_pool_input\n",
    "                dZ_pool_input[x_index, y_index, k] = dZ_pool_output[i, j, k]\n",
    "    \n",
    "    # Backpropagate through ReLU activation\n",
    "    dZ_pool_input *= der_ReLU(layer_output)\n",
    "    \n",
    "    # Backpropagate through Batch Normalization\n",
    "    dZ_pool_input, dgamma_conv, dbeta_conv = batch_norm_backward(dZ_pool_input, bn_cache_conv)\n",
    "    \n",
    "    # Backpropagate Conv layer\n",
    "    for i in range(2):  # For each filter in the kernel\n",
    "        delta_conv_weights[:, :, i] = signal.correlate(layer_input, dZ_pool_input[:, :, i], mode=\"valid\")\n",
    "    delta_conv_bias = dZ_pool_input\n",
    "    \n",
    "    return delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias, dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc\n",
    "\n",
    " \n",
    "def update_params(layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc,\n",
    "                  delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias,\n",
    "                  dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc, learning_rate):\n",
    "    layer_weights -= learning_rate * delta_conv_weights\n",
    "    layer_bias -= learning_rate * delta_conv_bias\n",
    "    fc_weights -= learning_rate * delta_fc_weights\n",
    "    fc_bias -= learning_rate * delta_fc_bias\n",
    "    gamma_conv -= learning_rate * dgamma_conv\n",
    "    beta_conv -= learning_rate * dbeta_conv\n",
    "    gamma_fc -= learning_rate * dgamma_fc\n",
    "    beta_fc -= learning_rate * dbeta_fc\n",
    "    return layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "\n",
    "\n",
    "def get_prediction(A2):\n",
    "  return np.argmax(A2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "82019a37-ccc9-4530-86e4-4ef9272aaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X_train, X_dev, Y_train, Y_dev, epochs, learning_rate, batch_size):\n",
    "    # Initialize parameters\n",
    "    layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = params()\n",
    "    gamma_conv = np.ones((24, 24, 2))\n",
    "    beta_conv = np.zeros((24, 24, 2))\n",
    "    gamma_fc = np.ones((10, 1))\n",
    "    beta_fc = np.zeros((10, 1))\n",
    "    num_examples = X_train.shape[2]\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch:\", i + 1)\n",
    "        \n",
    "        # Generate a random permutation of indices\n",
    "        permuted_indices = np.random.permutation(X_train.shape[2])\n",
    "        \n",
    "        # Shuffle both X_train and Y_train using the same permutation of indices\n",
    "        X_train_shuffled = X_train[:, :, permuted_indices]\n",
    "        Y_train_shuffled = Y_train[permuted_indices]\n",
    "        \n",
    "        for batch_start in range(0, len(X_train_shuffled), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, num_examples)\n",
    "            batch_gradients = [0, 0, 0, 0, 0, 0, 0, 0]  # Accumulate gradients over the batch\n",
    "            for j in range(batch_start, batch_end):\n",
    "                # Get a single training example\n",
    "                layer_input = X_train_shuffled[:, :, j]\n",
    "                label = Y_train_shuffled[j]\n",
    "                \n",
    "                # Forward propagation\n",
    "                layer_output, layer_pool, layer_indices, final_output, bn_cache_conv, bn_cache_fc, dropout_mask = forward_propagation(\n",
    "                    layer_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "                )\n",
    "                \n",
    "                # Back propagation\n",
    "                delta_conv_weights, delta_conv_bias, delta_fc_weights, delta_fc_bias, dgamma_conv, dbeta_conv, dgamma_fc, dbeta_fc = back_prop(\n",
    "                    layer_input, layer_output, layer_pool, layer_indices, final_output, label,\n",
    "                    layer_weights, layer_bias, fc_weights, fc_bias, bn_cache_conv, bn_cache_fc, dropout_mask\n",
    "                )\n",
    "                \n",
    "                # Accumulate gradients\n",
    "                batch_gradients[0] += delta_conv_weights\n",
    "                batch_gradients[1] += delta_conv_bias\n",
    "                batch_gradients[2] += delta_fc_weights\n",
    "                batch_gradients[3] += delta_fc_bias\n",
    "                batch_gradients[4] += dgamma_conv\n",
    "                batch_gradients[5] += dbeta_conv\n",
    "                batch_gradients[6] += dgamma_fc\n",
    "                batch_gradients[7] += dbeta_fc\n",
    "            \n",
    "            # Average gradients after processing the batch\n",
    "            batch_gradients = [grad / batch_size for grad in batch_gradients]\n",
    "            \n",
    "            # Update parameters after processing the batch\n",
    "            layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = update_params(\n",
    "                layer_weights, layer_bias, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc,\n",
    "                *batch_gradients,  # Use averaged gradients\n",
    "                learning_rate\n",
    "            )\n",
    "        \n",
    "        # Get training accuracy\n",
    "        counter = 0\n",
    "        for j in range(int(X_train_shuffled.shape[2]/100)):\n",
    "            test_input = X_train_shuffled[:, :, j]\n",
    "            layer_output, layer_pool, layer_indices, final_output, _, _, _ = forward_propagation(\n",
    "                test_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "            )\n",
    "            prediction = get_prediction(final_output)\n",
    "            predicted_label = prediction[0]\n",
    "            if Y_train_shuffled[j] == predicted_label:\n",
    "                counter += 1\n",
    "        print(\"Training Accuracy:\", counter / int(X_train_shuffled.shape[2]/100))\n",
    "        counter = 0\n",
    "        for j in range(500):\n",
    "            test_input = X_dev[:, :, j]\n",
    "            layer_output, layer_pool, layer_indices, final_output, _, _, _ = forward_propagation(\n",
    "                test_input, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "            )\n",
    "            prediction = get_prediction(final_output)\n",
    "            predicted_label = prediction[0]\n",
    "            if Y_dev[j] == predicted_label:\n",
    "                counter += 1\n",
    "        print(\"Validation Accuracy:\", counter / 500)\n",
    "    return layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d00daad9-e573-40a7-bb41-e8c87e4b6b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training Accuracy: 0.135\n",
      "Validation Accuracy: 0.13\n",
      "Epoch: 2\n",
      "Training Accuracy: 0.2075\n",
      "Validation Accuracy: 0.192\n",
      "Epoch: 3\n",
      "Training Accuracy: 0.2075\n",
      "Validation Accuracy: 0.186\n",
      "Epoch: 4\n",
      "Training Accuracy: 0.2675\n",
      "Validation Accuracy: 0.244\n",
      "Epoch: 5\n",
      "Training Accuracy: 0.265\n",
      "Validation Accuracy: 0.228\n",
      "Epoch: 6\n",
      "Training Accuracy: 0.315\n",
      "Validation Accuracy: 0.246\n",
      "Epoch: 7\n",
      "Training Accuracy: 0.285\n",
      "Validation Accuracy: 0.25\n",
      "Epoch: 8\n",
      "Training Accuracy: 0.29\n",
      "Validation Accuracy: 0.292\n",
      "Epoch: 9\n",
      "Training Accuracy: 0.32\n",
      "Validation Accuracy: 0.292\n",
      "Epoch: 10\n",
      "Training Accuracy: 0.3575\n",
      "Validation Accuracy: 0.316\n",
      "Epoch: 11\n",
      "Training Accuracy: 0.3575\n",
      "Validation Accuracy: 0.302\n",
      "Epoch: 12\n",
      "Training Accuracy: 0.395\n",
      "Validation Accuracy: 0.42\n",
      "Epoch: 13\n",
      "Training Accuracy: 0.355\n",
      "Validation Accuracy: 0.3\n",
      "Epoch: 14\n",
      "Training Accuracy: 0.42\n",
      "Validation Accuracy: 0.374\n",
      "Epoch: 15\n",
      "Training Accuracy: 0.4025\n",
      "Validation Accuracy: 0.398\n",
      "Epoch: 16\n",
      "Training Accuracy: 0.4275\n",
      "Validation Accuracy: 0.426\n",
      "Epoch: 17\n",
      "Training Accuracy: 0.4575\n",
      "Validation Accuracy: 0.426\n",
      "Epoch: 18\n",
      "Training Accuracy: 0.4775\n",
      "Validation Accuracy: 0.442\n",
      "Epoch: 19\n",
      "Training Accuracy: 0.4875\n",
      "Validation Accuracy: 0.502\n",
      "Epoch: 20\n",
      "Training Accuracy: 0.4925\n",
      "Validation Accuracy: 0.526\n",
      "Epoch: 21\n",
      "Training Accuracy: 0.4475\n",
      "Validation Accuracy: 0.506\n",
      "Epoch: 22\n",
      "Training Accuracy: 0.5075\n",
      "Validation Accuracy: 0.48\n",
      "Epoch: 23\n",
      "Training Accuracy: 0.5725\n",
      "Validation Accuracy: 0.564\n",
      "Epoch: 24\n",
      "Training Accuracy: 0.52\n",
      "Validation Accuracy: 0.572\n",
      "Epoch: 25\n",
      "Training Accuracy: 0.57\n",
      "Validation Accuracy: 0.574\n",
      "Epoch: 26\n",
      "Training Accuracy: 0.5175\n",
      "Validation Accuracy: 0.594\n",
      "Epoch: 27\n",
      "Training Accuracy: 0.565\n",
      "Validation Accuracy: 0.584\n",
      "Epoch: 28\n",
      "Training Accuracy: 0.5275\n",
      "Validation Accuracy: 0.576\n",
      "Epoch: 29\n",
      "Training Accuracy: 0.585\n",
      "Validation Accuracy: 0.59\n",
      "Epoch: 30\n",
      "Training Accuracy: 0.615\n",
      "Validation Accuracy: 0.656\n",
      "Epoch: 31\n",
      "Training Accuracy: 0.6025\n",
      "Validation Accuracy: 0.622\n",
      "Epoch: 32\n",
      "Training Accuracy: 0.6\n",
      "Validation Accuracy: 0.616\n",
      "Epoch: 33\n",
      "Training Accuracy: 0.615\n",
      "Validation Accuracy: 0.596\n",
      "Epoch: 34\n",
      "Training Accuracy: 0.615\n",
      "Validation Accuracy: 0.638\n",
      "Epoch: 35\n",
      "Training Accuracy: 0.66\n",
      "Validation Accuracy: 0.63\n",
      "Epoch: 36\n",
      "Training Accuracy: 0.585\n",
      "Validation Accuracy: 0.58\n",
      "Epoch: 37\n",
      "Training Accuracy: 0.645\n",
      "Validation Accuracy: 0.66\n",
      "Epoch: 38\n",
      "Training Accuracy: 0.66\n",
      "Validation Accuracy: 0.618\n",
      "Epoch: 39\n",
      "Training Accuracy: 0.655\n",
      "Validation Accuracy: 0.686\n",
      "Epoch: 40\n",
      "Training Accuracy: 0.6625\n",
      "Validation Accuracy: 0.68\n",
      "Epoch: 41\n",
      "Training Accuracy: 0.6875\n",
      "Validation Accuracy: 0.684\n",
      "Epoch: 42\n",
      "Training Accuracy: 0.6525\n",
      "Validation Accuracy: 0.658\n",
      "Epoch: 43\n",
      "Training Accuracy: 0.665\n",
      "Validation Accuracy: 0.696\n",
      "Epoch: 44\n",
      "Training Accuracy: 0.7\n",
      "Validation Accuracy: 0.682\n",
      "Epoch: 45\n",
      "Training Accuracy: 0.7025\n",
      "Validation Accuracy: 0.68\n",
      "Epoch: 46\n",
      "Training Accuracy: 0.6425\n",
      "Validation Accuracy: 0.682\n",
      "Epoch: 47\n",
      "Training Accuracy: 0.695\n",
      "Validation Accuracy: 0.702\n",
      "Epoch: 48\n",
      "Training Accuracy: 0.6775\n",
      "Validation Accuracy: 0.734\n",
      "Epoch: 49\n",
      "Training Accuracy: 0.67\n",
      "Validation Accuracy: 0.67\n",
      "Epoch: 50\n",
      "Training Accuracy: 0.7\n",
      "Validation Accuracy: 0.734\n",
      "Epoch: 51\n",
      "Training Accuracy: 0.7025\n",
      "Validation Accuracy: 0.71\n",
      "Epoch: 52\n",
      "Training Accuracy: 0.715\n",
      "Validation Accuracy: 0.736\n",
      "Epoch: 53\n",
      "Training Accuracy: 0.6625\n",
      "Validation Accuracy: 0.708\n",
      "Epoch: 54\n",
      "Training Accuracy: 0.685\n",
      "Validation Accuracy: 0.724\n",
      "Epoch: 55\n",
      "Training Accuracy: 0.7275\n",
      "Validation Accuracy: 0.702\n",
      "Epoch: 56\n",
      "Training Accuracy: 0.6975\n",
      "Validation Accuracy: 0.71\n",
      "Epoch: 57\n",
      "Training Accuracy: 0.7525\n",
      "Validation Accuracy: 0.716\n",
      "Epoch: 58\n",
      "Training Accuracy: 0.69\n",
      "Validation Accuracy: 0.71\n",
      "Epoch: 59\n",
      "Training Accuracy: 0.6925\n",
      "Validation Accuracy: 0.712\n",
      "Epoch: 60\n",
      "Training Accuracy: 0.7125\n",
      "Validation Accuracy: 0.736\n",
      "Epoch: 61\n",
      "Training Accuracy: 0.7175\n",
      "Validation Accuracy: 0.744\n",
      "Epoch: 62\n",
      "Training Accuracy: 0.74\n",
      "Validation Accuracy: 0.704\n",
      "Epoch: 63\n",
      "Training Accuracy: 0.74\n",
      "Validation Accuracy: 0.728\n",
      "Epoch: 64\n",
      "Training Accuracy: 0.6825\n",
      "Validation Accuracy: 0.734\n",
      "Epoch: 65\n",
      "Training Accuracy: 0.745\n",
      "Validation Accuracy: 0.776\n",
      "Epoch: 66\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.738\n",
      "Epoch: 67\n",
      "Training Accuracy: 0.73\n",
      "Validation Accuracy: 0.73\n",
      "Epoch: 68\n",
      "Training Accuracy: 0.7075\n",
      "Validation Accuracy: 0.7\n",
      "Epoch: 69\n",
      "Training Accuracy: 0.6825\n",
      "Validation Accuracy: 0.724\n",
      "Epoch: 70\n",
      "Training Accuracy: 0.7375\n",
      "Validation Accuracy: 0.768\n",
      "Epoch: 71\n",
      "Training Accuracy: 0.765\n",
      "Validation Accuracy: 0.754\n",
      "Epoch: 72\n",
      "Training Accuracy: 0.7325\n",
      "Validation Accuracy: 0.756\n",
      "Epoch: 73\n",
      "Training Accuracy: 0.69\n",
      "Validation Accuracy: 0.692\n",
      "Epoch: 74\n",
      "Training Accuracy: 0.7275\n",
      "Validation Accuracy: 0.77\n",
      "Epoch: 75\n",
      "Training Accuracy: 0.7275\n",
      "Validation Accuracy: 0.774\n",
      "Epoch: 76\n",
      "Training Accuracy: 0.7225\n",
      "Validation Accuracy: 0.788\n",
      "Epoch: 77\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.776\n",
      "Epoch: 78\n",
      "Training Accuracy: 0.7525\n",
      "Validation Accuracy: 0.76\n",
      "Epoch: 79\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.762\n",
      "Epoch: 80\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.784\n",
      "Epoch: 81\n",
      "Training Accuracy: 0.7725\n",
      "Validation Accuracy: 0.76\n",
      "Epoch: 82\n",
      "Training Accuracy: 0.775\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 83\n",
      "Training Accuracy: 0.73\n",
      "Validation Accuracy: 0.79\n",
      "Epoch: 84\n",
      "Training Accuracy: 0.7625\n",
      "Validation Accuracy: 0.768\n",
      "Epoch: 85\n",
      "Training Accuracy: 0.74\n",
      "Validation Accuracy: 0.748\n",
      "Epoch: 86\n",
      "Training Accuracy: 0.77\n",
      "Validation Accuracy: 0.766\n",
      "Epoch: 87\n",
      "Training Accuracy: 0.7125\n",
      "Validation Accuracy: 0.722\n",
      "Epoch: 88\n",
      "Training Accuracy: 0.78\n",
      "Validation Accuracy: 0.804\n",
      "Epoch: 89\n",
      "Training Accuracy: 0.76\n",
      "Validation Accuracy: 0.802\n",
      "Epoch: 90\n",
      "Training Accuracy: 0.775\n",
      "Validation Accuracy: 0.76\n",
      "Epoch: 91\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.77\n",
      "Epoch: 92\n",
      "Training Accuracy: 0.7625\n",
      "Validation Accuracy: 0.802\n",
      "Epoch: 93\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 94\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.804\n",
      "Epoch: 95\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.8\n",
      "Epoch: 96\n",
      "Training Accuracy: 0.7525\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 97\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.776\n",
      "Epoch: 98\n",
      "Training Accuracy: 0.7375\n",
      "Validation Accuracy: 0.782\n",
      "Epoch: 99\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.762\n",
      "Epoch: 100\n",
      "Training Accuracy: 0.7825\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 101\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.816\n",
      "Epoch: 102\n",
      "Training Accuracy: 0.78\n",
      "Validation Accuracy: 0.808\n",
      "Epoch: 103\n",
      "Training Accuracy: 0.795\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 104\n",
      "Training Accuracy: 0.7725\n",
      "Validation Accuracy: 0.752\n",
      "Epoch: 105\n",
      "Training Accuracy: 0.785\n",
      "Validation Accuracy: 0.802\n",
      "Epoch: 106\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.812\n",
      "Epoch: 107\n",
      "Training Accuracy: 0.755\n",
      "Validation Accuracy: 0.754\n",
      "Epoch: 108\n",
      "Training Accuracy: 0.765\n",
      "Validation Accuracy: 0.784\n",
      "Epoch: 109\n",
      "Training Accuracy: 0.785\n",
      "Validation Accuracy: 0.79\n",
      "Epoch: 110\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 111\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.804\n",
      "Epoch: 112\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.83\n",
      "Epoch: 113\n",
      "Training Accuracy: 0.77\n",
      "Validation Accuracy: 0.81\n",
      "Epoch: 114\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 115\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 116\n",
      "Training Accuracy: 0.79\n",
      "Validation Accuracy: 0.812\n",
      "Epoch: 117\n",
      "Training Accuracy: 0.8075\n",
      "Validation Accuracy: 0.802\n",
      "Epoch: 118\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.798\n",
      "Epoch: 119\n",
      "Training Accuracy: 0.78\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 120\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.81\n",
      "Epoch: 121\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.766\n",
      "Epoch: 122\n",
      "Training Accuracy: 0.775\n",
      "Validation Accuracy: 0.79\n",
      "Epoch: 123\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 124\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 125\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.802\n",
      "Epoch: 126\n",
      "Training Accuracy: 0.79\n",
      "Validation Accuracy: 0.8\n",
      "Epoch: 127\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 128\n",
      "Training Accuracy: 0.7775\n",
      "Validation Accuracy: 0.78\n",
      "Epoch: 129\n",
      "Training Accuracy: 0.7825\n",
      "Validation Accuracy: 0.796\n",
      "Epoch: 130\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 131\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.812\n",
      "Epoch: 132\n",
      "Training Accuracy: 0.7725\n",
      "Validation Accuracy: 0.81\n",
      "Epoch: 133\n",
      "Training Accuracy: 0.865\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 134\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 135\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 136\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 137\n",
      "Training Accuracy: 0.7625\n",
      "Validation Accuracy: 0.786\n",
      "Epoch: 138\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.8\n",
      "Epoch: 139\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 140\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.816\n",
      "Epoch: 141\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 142\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 143\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 144\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.81\n",
      "Epoch: 145\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 146\n",
      "Training Accuracy: 0.795\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 147\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 148\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 149\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.792\n",
      "Epoch: 150\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 151\n",
      "Training Accuracy: 0.79\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 152\n",
      "Training Accuracy: 0.7925\n",
      "Validation Accuracy: 0.792\n",
      "Epoch: 153\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 154\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 155\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.812\n",
      "Epoch: 156\n",
      "Training Accuracy: 0.76\n",
      "Validation Accuracy: 0.8\n",
      "Epoch: 157\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 158\n",
      "Training Accuracy: 0.8125\n",
      "Validation Accuracy: 0.81\n",
      "Epoch: 159\n",
      "Training Accuracy: 0.8175\n",
      "Validation Accuracy: 0.83\n",
      "Epoch: 160\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.782\n",
      "Epoch: 161\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 162\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 163\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 164\n",
      "Training Accuracy: 0.7825\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 165\n",
      "Training Accuracy: 0.8175\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 166\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 167\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 168\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 169\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 170\n",
      "Training Accuracy: 0.8125\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 171\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.792\n",
      "Epoch: 172\n",
      "Training Accuracy: 0.8175\n",
      "Validation Accuracy: 0.824\n",
      "Epoch: 173\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 174\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 175\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.816\n",
      "Epoch: 176\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 177\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.806\n",
      "Epoch: 178\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 179\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 180\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 181\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 182\n",
      "Training Accuracy: 0.7925\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 183\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 184\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.824\n",
      "Epoch: 185\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 186\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 187\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 188\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 189\n",
      "Training Accuracy: 0.79\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 190\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 191\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 192\n",
      "Training Accuracy: 0.7575\n",
      "Validation Accuracy: 0.774\n",
      "Epoch: 193\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 194\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 195\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 196\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 197\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.816\n",
      "Epoch: 198\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 199\n",
      "Training Accuracy: 0.805\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 200\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.782\n",
      "Epoch: 201\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 202\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 203\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.824\n",
      "Epoch: 204\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 205\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 206\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 207\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 208\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 209\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 210\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 211\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 212\n",
      "Training Accuracy: 0.8125\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 213\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 214\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 215\n",
      "Training Accuracy: 0.775\n",
      "Validation Accuracy: 0.814\n",
      "Epoch: 216\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 217\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 218\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.79\n",
      "Epoch: 219\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.824\n",
      "Epoch: 220\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 221\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.796\n",
      "Epoch: 222\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 223\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 224\n",
      "Training Accuracy: 0.8\n",
      "Validation Accuracy: 0.83\n",
      "Epoch: 225\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.816\n",
      "Epoch: 226\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 227\n",
      "Training Accuracy: 0.7775\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 228\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 229\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 230\n",
      "Training Accuracy: 0.8575\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 231\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.834\n",
      "Epoch: 232\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.818\n",
      "Epoch: 233\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 234\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 235\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 236\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 237\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 238\n",
      "Training Accuracy: 0.8575\n",
      "Validation Accuracy: 0.83\n",
      "Epoch: 239\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 240\n",
      "Training Accuracy: 0.7975\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 241\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.808\n",
      "Epoch: 242\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 243\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.826\n",
      "Epoch: 244\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 245\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 246\n",
      "Training Accuracy: 0.8125\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 247\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 248\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 249\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 250\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 251\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 252\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 253\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 254\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 255\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 256\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 257\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 258\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 259\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 260\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 261\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 262\n",
      "Training Accuracy: 0.88\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 263\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 264\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 265\n",
      "Training Accuracy: 0.7875\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 266\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 267\n",
      "Training Accuracy: 0.8075\n",
      "Validation Accuracy: 0.836\n",
      "Epoch: 268\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 269\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 270\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 271\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 272\n",
      "Training Accuracy: 0.8075\n",
      "Validation Accuracy: 0.878\n",
      "Epoch: 273\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 274\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 275\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 276\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 277\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 278\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.86\n",
      "Epoch: 279\n",
      "Training Accuracy: 0.8075\n",
      "Validation Accuracy: 0.886\n",
      "Epoch: 280\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 281\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 282\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.872\n",
      "Epoch: 283\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 284\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 285\n",
      "Training Accuracy: 0.8125\n",
      "Validation Accuracy: 0.82\n",
      "Epoch: 286\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 287\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.872\n",
      "Epoch: 288\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 289\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.878\n",
      "Epoch: 290\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 291\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 292\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 293\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 294\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 295\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 296\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.858\n",
      "Epoch: 297\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.88\n",
      "Epoch: 298\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 299\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 300\n",
      "Training Accuracy: 0.8025\n",
      "Validation Accuracy: 0.808\n",
      "Epoch: 301\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.822\n",
      "Epoch: 302\n",
      "Training Accuracy: 0.8675\n",
      "Validation Accuracy: 0.824\n",
      "Epoch: 303\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.858\n",
      "Epoch: 304\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 305\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.832\n",
      "Epoch: 306\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 307\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 308\n",
      "Training Accuracy: 0.8675\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 309\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.888\n",
      "Epoch: 310\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.86\n",
      "Epoch: 311\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 312\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 313\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.858\n",
      "Epoch: 314\n",
      "Training Accuracy: 0.875\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 315\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 316\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 317\n",
      "Training Accuracy: 0.8075\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 318\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 319\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 320\n",
      "Training Accuracy: 0.875\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 321\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.878\n",
      "Epoch: 322\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 323\n",
      "Training Accuracy: 0.875\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 324\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.86\n",
      "Epoch: 325\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 326\n",
      "Training Accuracy: 0.8225\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 327\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 328\n",
      "Training Accuracy: 0.8675\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 329\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.828\n",
      "Epoch: 330\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 331\n",
      "Training Accuracy: 0.8875\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 332\n",
      "Training Accuracy: 0.865\n",
      "Validation Accuracy: 0.88\n",
      "Epoch: 333\n",
      "Training Accuracy: 0.825\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 334\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.844\n",
      "Epoch: 335\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 336\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 337\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 338\n",
      "Training Accuracy: 0.8725\n",
      "Validation Accuracy: 0.886\n",
      "Epoch: 339\n",
      "Training Accuracy: 0.88\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 340\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 341\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 342\n",
      "Training Accuracy: 0.8575\n",
      "Validation Accuracy: 0.858\n",
      "Epoch: 343\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.858\n",
      "Epoch: 344\n",
      "Training Accuracy: 0.865\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 345\n",
      "Training Accuracy: 0.8575\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 346\n",
      "Training Accuracy: 0.865\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 347\n",
      "Training Accuracy: 0.815\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 348\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 349\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 350\n",
      "Training Accuracy: 0.84\n",
      "Validation Accuracy: 0.866\n",
      "Epoch: 351\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 352\n",
      "Training Accuracy: 0.8425\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 353\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.84\n",
      "Epoch: 354\n",
      "Training Accuracy: 0.85\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 355\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 356\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.872\n",
      "Epoch: 357\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.842\n",
      "Epoch: 358\n",
      "Training Accuracy: 0.8725\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 359\n",
      "Training Accuracy: 0.8725\n",
      "Validation Accuracy: 0.878\n",
      "Epoch: 360\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 361\n",
      "Training Accuracy: 0.8725\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 362\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 363\n",
      "Training Accuracy: 0.875\n",
      "Validation Accuracy: 0.88\n",
      "Epoch: 364\n",
      "Training Accuracy: 0.83\n",
      "Validation Accuracy: 0.86\n",
      "Epoch: 365\n",
      "Training Accuracy: 0.8675\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 366\n",
      "Training Accuracy: 0.8775\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 367\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 368\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.854\n",
      "Epoch: 369\n",
      "Training Accuracy: 0.8675\n",
      "Validation Accuracy: 0.872\n",
      "Epoch: 370\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 371\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.86\n",
      "Epoch: 372\n",
      "Training Accuracy: 0.81\n",
      "Validation Accuracy: 0.862\n",
      "Epoch: 373\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 374\n",
      "Training Accuracy: 0.855\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 375\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 376\n",
      "Training Accuracy: 0.87\n",
      "Validation Accuracy: 0.85\n",
      "Epoch: 377\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.876\n",
      "Epoch: 378\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.846\n",
      "Epoch: 379\n",
      "Training Accuracy: 0.88\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 380\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.872\n",
      "Epoch: 381\n",
      "Training Accuracy: 0.8575\n",
      "Validation Accuracy: 0.852\n",
      "Epoch: 382\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 383\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.876\n",
      "Epoch: 384\n",
      "Training Accuracy: 0.86\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 385\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 386\n",
      "Training Accuracy: 0.8475\n",
      "Validation Accuracy: 0.876\n",
      "Epoch: 387\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.868\n",
      "Epoch: 388\n",
      "Training Accuracy: 0.875\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 389\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 390\n",
      "Training Accuracy: 0.8525\n",
      "Validation Accuracy: 0.88\n",
      "Epoch: 391\n",
      "Training Accuracy: 0.835\n",
      "Validation Accuracy: 0.874\n",
      "Epoch: 392\n",
      "Training Accuracy: 0.8375\n",
      "Validation Accuracy: 0.856\n",
      "Epoch: 393\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 394\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 395\n",
      "Training Accuracy: 0.82\n",
      "Validation Accuracy: 0.864\n",
      "Epoch: 396\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.87\n",
      "Epoch: 397\n",
      "Training Accuracy: 0.8275\n",
      "Validation Accuracy: 0.878\n",
      "Epoch: 398\n",
      "Training Accuracy: 0.845\n",
      "Validation Accuracy: 0.848\n",
      "Epoch: 399\n",
      "Training Accuracy: 0.8325\n",
      "Validation Accuracy: 0.838\n",
      "Epoch: 400\n",
      "Training Accuracy: 0.88\n",
      "Validation Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc = stochastic_gradient_descent(\n",
    "    X_train, X_dev, Y_train, Y_dev, 400, 0.2, 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e4adbc49-f47e-4a24-92ae-822565c2c984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[2]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1df4f1a6-6227-4556-8395-3a9d1a2d20e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"layer weights\", layer_weights)\n",
    "# print(\"layer bias\", layer_bias)\n",
    "# print(\"layer output\", layer_output)\n",
    "# print(\"fc weights\", fc_weights)\n",
    "# print(\"fc_bias\", fc_bias)\n",
    "plt.imshow(layer_output[:,:,1], cmap='gray')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()\n",
    "plt.imshow(layer_output[:,:,0], cmap='gray')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e98a1531-056b-41dd-9bb5-550c39b421fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in range(1000):\n",
    "    test = X_dev[:,:,i]\n",
    "    label = Y_dev[i]\n",
    "    # print(label)\n",
    "    # plt.imshow(test, cmap='gray')\n",
    "    # plt.axis('off')  # Turn off axis\n",
    "    # plt.show()\n",
    "    # print(\"Testing model:\")\n",
    "    _, _, _, final_output, _, _, _ = forward_propagation(\n",
    "                    test, layer_weights, layer_bias, layer_output, fc_weights, fc_bias, gamma_conv, beta_conv, gamma_fc, beta_fc\n",
    "                )\n",
    "    # print(\"Model prediction: \")\n",
    "    prediction = get_prediction(final_output)\n",
    "    if (prediction == label):\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
