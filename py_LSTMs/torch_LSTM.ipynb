{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22669c6f-0f22-46cf-8bff-8ced55b6b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 32\n",
    "hidden_size = 30\n",
    "lr = 0.01\n",
    "time_steps = 8\n",
    "input_size = 27\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h0, c0):\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        return out, (hn, cn)\n",
    "\n",
    "model = SimpleLSTM(input_size, hidden_size, input_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "iterations = 0\n",
    "num_epochs = 2000\n",
    "\n",
    "h0 = torch.zeros(1, batch_size, hidden_size)\n",
    "c0 = torch.zeros(1, batch_size, hidden_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, Xtr_batched.size(1) - time_steps, time_steps):\n",
    "        Xb = Xtr_batched[:, i:i+time_steps, :]\n",
    "        Yb = Ytr_batched[:, i:i+time_steps]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, (h0, c0) = model(Xb, h0.detach(), c0.detach())\n",
    "        \n",
    "        loss = criterion(output.view(-1, input_size), Yb.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i // time_steps) % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Step [{i // time_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Evaluate on development set\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            dev_loss = 0\n",
    "            for j in range(0, Xdev_batched.size(1) - time_steps, time_steps):\n",
    "                Xb_dev = Xdev_batched[:, j:j+time_steps, :]\n",
    "                Yb_dev = Ydev_batched[:, j:j+time_steps]\n",
    "                \n",
    "                output_dev, _ = model(Xb_dev, h0, c0)\n",
    "                dev_loss += criterion(output_dev.view(-1, input_size), Yb_dev.view(-1)).item()\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Dev Loss: {dev_loss / (Xdev_batched.size(1) // time_steps):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093fa730-7ed0-4db8-9905-1c822abbd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming stoi is defined elsewhere\n",
    "# stoi = {'a': 0, 'b': 1, ..., 'z': 25, '.': 26}\n",
    "\n",
    "# Helper functions for encoding and batching\n",
    "def encode_words(words):\n",
    "    encoded = []\n",
    "    for w in words:\n",
    "        encoded.extend([stoi[ch] for ch in '.' + w + '.'])\n",
    "    return encoded\n",
    "\n",
    "encoded = encode_words(words)\n",
    "\n",
    "def create_pairs(seq, block_size):\n",
    "    X, Y = [], []\n",
    "    for i in range(0, len(seq) - block_size):\n",
    "        X.append(seq[i:i + block_size])\n",
    "        Y.append(seq[i + 1:i + block_size + 1])\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "    return X, Y\n",
    "\n",
    "n = len(encoded)\n",
    "n1 = int(0.8 * n)\n",
    "\n",
    "train_seq = encoded[:n1]\n",
    "dev_seq = encoded[n1:]\n",
    "\n",
    "block_size = 8\n",
    "\n",
    "Xtr, Ytr = create_pairs(train_seq, block_size)\n",
    "Xdev, Ydev = create_pairs(dev_seq, block_size)\n",
    "\n",
    "def split_into_batches(X, Y, batch_size):\n",
    "    num_batches = X.size(0) // batch_size\n",
    "    X = X[:num_batches * batch_size]\n",
    "    Y = Y[:num_batches * batch_size]\n",
    "    \n",
    "    X = X.view(batch_size, -1, X.size(-1))\n",
    "    Y = Y.view(batch_size, -1, Y.size(-1))\n",
    "    return X, Y\n",
    "\n",
    "Xtr_batched, Ytr_batched = split_into_batches(Xtr, Ytr, batch_size)\n",
    "Xdev_batched, Ydev_batched = split_into_batches(Xdev, Ydev, batch_size)\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h0, c0):\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        return out, (hn, cn)\n",
    "\n",
    "batch_size = 32\n",
    "hidden_size = 30\n",
    "lr = 0.01\n",
    "time_steps = 8\n",
    "input_size = 27\n",
    "output_size = 27\n",
    "\n",
    "model = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "iterations = 0\n",
    "num_epochs = 2000\n",
    "\n",
    "# Convert input indices to one-hot vectors\n",
    "def to_one_hot(indices, num_classes):\n",
    "    one_hot = torch.zeros(indices.size(0), indices.size(1), num_classes, device=indices.device)\n",
    "    one_hot.scatter_(2, indices.unsqueeze(2), 1.0)\n",
    "    return one_hot\n",
    "\n",
    "one_hot_Xtr = to_one_hot(Xtr_batched, input_size)\n",
    "one_hot_Xdev = to_one_hot(Xdev_batched, input_size)\n",
    "\n",
    "h0 = torch.zeros(1, batch_size, hidden_size)\n",
    "c0 = torch.zeros(1, batch_size, hidden_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    for i in range(0, one_hot_Xtr.size(1) - time_steps, time_steps):\n",
    "        Xb = one_hot_Xtr[:, i:i + time_steps, :]\n",
    "        Yb = Ytr_batched[:, i:i + time_steps]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, (h0, c0) = model(Xb, h0.detach(), c0.detach())\n",
    "        \n",
    "        loss = criterion(output.view(-1, output_size), Yb.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i // time_steps) % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Step [{i // time_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Evaluate on development set\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            dev_loss = 0\n",
    "            for j in range(0, one_hot_Xdev.size(1) - time_steps, time_steps):\n",
    "                Xb_dev = one_hot_Xdev[:, j:j + time_steps, :]\n",
    "                Yb_dev = Ydev_batched[:, j:j + time_steps]\n",
    "                \n",
    "                output_dev, _ = model(Xb_dev, h0, c0)\n",
    "                dev_loss += criterion(output_dev.view(-1, output_size), Yb_dev.view(-1)).item()\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Dev Loss: {dev_loss / (one_hot_Xdev.size(1) // time_steps):.4f}')\n",
    "\n",
    "    iterations += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
