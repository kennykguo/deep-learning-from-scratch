import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import os
import re
import io
# from tqdm.notebook import trange, tqdm

import torch
import torch.nn as nn
from torch import optim
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
import torch.nn.functional as F

from torchtext.datasets import AG_NEWS
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
import torchtext.transforms as T


# Root directory of the dataset
data_set_root = "datasets"


# https://pytorch.org/data/main/torchdata.datapipes.iter.html
# https://medium.com/deelvin-machine-learning/comparison-of-pytorch-dataset-and-torchdata-datapipes-486e03068c58

# AG_NEWS is a class that loads the data pipeline, storing it at the specified root directory. 
dataset_train = AG_NEWS(root=data_set_root, split="train")
dataset_test = AG_NEWS(root=data_set_root, split="test")

# Creates an iterator object. That allows you to traverse through the elements in the dataset
data = next(iter(dataset_train))
data


# Import the SentencePiece model
from torchtext.data.functional import generate_sp_model

# Opens train.csv
with open(os.path.join(data_set_root, "datasets/AG_NEWS/train.csv")) as f:
    # Creates file data.txt for writing
    with open(os.path.join(data_set_root, "datasets/AG_NEWS/data.txt"), "w") as f2:
        # Looping through all lines in train.csv
        for i, line in enumerate(f):
            # Clean up the text
            text_only = "".join(line.split(",")[1:])
            filtered = re.sub(r'\\|\\n|;', ' ', text_only.replace('"', ' ').replace('\n', ' ')) # remove newline characters
            filtered = filtered.replace(' #39;', "'")
            filtered = filtered.replace(' #38;', "&")
            filtered = filtered.replace(' #36;', "$")
            filtered = filtered.replace(' #151;', "-")
            f2.write(filtered.lower() + "\n")

# data.txt is used for the SentencePiece model
generate_sp_model(os.path.join(data_set_root, "datasets/AG_NEWS/data.txt"), vocab_size=20000, model_prefix='spm_ag_news')


# Inherits from Dataset
class AGNews(Dataset):
    def __init__(self, num_datapoints, test_train="train"):
        
        # Load the dataset from the specified train.csv file
        # Each entry has a label, title, and the text
        self.df = pd.read_csv(os.path.join(data_set_root, "datasets/AG_NEWS/" + test_train + ".csv"), names=["Class", "Title", "Content"])
        
        # Fill any missing values with empty strings
        self.df.fillna('', inplace=True)
        
        # Combine the title and content columns into a single article column (now we have just label and text)
        # Concatenate into a single string
        self.df['Article'] = self.df['Title'] + " : " + self.df['Content']
        
        # Drop the now redundant Title and Content columns
        self.df.drop(['Title', 'Content'], axis=1, inplace=True)
        
        # Clean the Article column by removing unwanted characters and replacing HTML codes
        self.df['Article'] = self.df['Article'].str.replace(r'\\n|\\|\\r|\\r\\n|\n|"', ' ', regex=True)
        self.df['Article'] = self.df['Article'].replace({' #39;': "'", 
                                                         ' #38;': "&", 
                                                         ' #36;': "$",
                                                         ' #151;': "-"}, 
                                                        regex=True)
        # Limit the number of data points
        if num_datapoints is not None:
            self.df = self.df.head(num_datapoints)

    def __getitem__(self, index):
        # Retrieve the article text and convert it to lowercase
        text = self.df.loc[index]["Article"].lower()
        
        # Retrieve the class label and convert it to an integer
        # Class label are all shifted to array indices
        class_index = int(self.df.loc[index]["Class"]) - 1

        # Return a tuple of the class index and the article text
        return class_index, text
    
    def __len__(self):
        # Return the number of data points in the dataset
        return len(self.df)



# Create training and testing datasets
dataset_train = AGNews(num_datapoints=2000, test_train="train")
dataset_test = AGNews(num_datapoints=2000, test_train="test")

# Access the first sample in the dataset (list of tuples)
sample = train_dataset[0]
print(sample)

# Create data loaders for the training and testing datasets
# num_workers is just for efficiency
# drop_last for train for training stability
# not drop_last for test to use all the data
data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)
data_loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)


# for batch_idx, (labels, texts) in enumerate(data_loader_train):
#     print(f"Batch {batch_idx + 1}")
#     print(f"Labels: {labels[:2]}")
#     print(f"Texts: {texts[:2]}")  # Print the first two texts for brevity
#     if batch_idx == 0:
#         break  # Inspecting only the first batch for demonstration


def yield_tokens(file_path):
    with io.open(file_path, encoding='utf-8') as f:
        # Iterate through each line in the file
        for line in f:
            # Yield the token from the first column (split by tab)
            yield [line.split("\t")[0]]

# Build a vocabulary from the tokens yielded by the yield_tokens function
# <pad> is a padding token that is added to the end of a sentence to ensure the length of all sequences in a batch is the same
# <sos> signals the "Start-Of-Sentence" aka the start of the sequence
# <eos> signal the "End-Of-Sentence" aka the end of the sequence
# <unk> "unknown" token is used if a token is not contained in the vocab
# From torchtext library
vocab = build_vocab_from_iterator(yield_tokens("spm_ag_news.vocab"), 
                                  specials=['<pad>', '<sos>', '<eos>', '<unk>'],
                                  special_first=True)

# Set the default index for unknown tokens to the index of the '<unk>' token
vocab.set_default_index(vocab['<unk>'])


# Define the yield_tokens function
def yield_tokens(file_path):
    with io.open(file_path, encoding='utf-8') as f:
        for line in f:
            yield [line.split("\t")[0]]

# Print the output of the yield_tokens function
file_path = "spm_ag_news.vocab"  # Replace with your file path
# for token in yield_tokens(file_path):
#     print(token)

# Build the vocabulary
vocab = build_vocab_from_iterator(yield_tokens(file_path), 
                                  specials=['<pad>', '<sos>', '<eos>', '<unk>'],
                                  special_first=True)

# Set the default index for unknown tokens
vocab.set_default_index(vocab['<unk>'])

# # Print the vocab object details
# print("Token to index mapping:")
# print(vocab.get_stoi())  # String-to-index mapping

# print("\nIndex to token mapping:")
# print(vocab.get_itos())  # Index-to-string mapping



# Data transform to turn text into vocab token
# Text transformation pipeline aliased T
text_transform = T.Sequential(
    # Tokenize with pretrained Tokenizer
    T.SentencePieceTokenizer("spm_ag_news.model"),
    # Converts the sentences to indices based on given vocabulary
    T.VocabTransform(vocab=vocab),
    # Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is 1 as seen in previous section
    T.AddToken(1, begin=True),
    # Crop the sentence if it is longer than the max length
    T.Truncate(max_seq_len=max_len),
    ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is
    # 2 as seen in previous section
    T.AddToken(2, begin=False),
    # Convert the list of lists to a tensor, this will also
    # Pad a sentence with the <pad> token if it is shorter than the max length
    # This ensures all sentences are the same length!
    T.ToTensor(padding_value=0),
)

text_transform


class TokenDrop(nn.Module):
    """For a batch of tokens indices, randomly replace a non-specical token with <pad>.
    prob (float): probability of dropping a token
    pad_token (int): index for the <pad> token
    num_special (int): Number of special tokens, assumed to be at the start of the vocab
    """

    def __init__(self, prob=0.1, pad_token=0, num_special=4):
        self.prob = prob
        self.num_special = num_special
        self.pad_token = pad_token

    def __call__(self, sample):
        # Randomly sample a bernoulli distribution with p = prob
        # Create a mask where 1 means we will replace that token
        # Discrete probability distribution
        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()
        
        # only replace if the token is not a special token
        # Ones or zeros. If false, zero, if true, 1
        can_drop = (sample >= self.num_special).long()
        # Multiply together to get the corresponding tokens that can be dropped, and that will be dropped
        mask = mask * can_drop

        # Make a mask of pad_token
        replace_with = (self.pad_token * torch.ones_like(sample)).long()

        # sample is the original sample
        # mask indicates what tokens can be replaced (0 to not be replaced, 1 to be replaced)
        # replace_with is a list of of the pad_token tokens
        # (1-mask) creates the complement mask. (now, 0 indicates to be replaced, 1 indicates to be not replaced)
        # Multiplying by sample, retains the original tokens that are not to be kept, and applies the mask on the sample
        # mask * replace_with does elementwise multiplication and adds the corresponding pad_token to the tokens replaced
        sample_out = (1 - mask) * sample + mask * replace_with
        
        return sample_out

# Sample tensor of token indices (e.g., a batch of sentences)
sample = torch.tensor([[5, 6, 7, 8], [9, 10, 11, 12]])

# Create an instance of TokenDrop with default parameters
token_drop = TokenDrop(prob=0.1, pad_token=0, num_special=4)

# Apply token dropping to the sample tensor
dropped_sample = token_drop(sample)

# Print the result
print(dropped_sample)


# Sinusoidal positional embeds
# Produce unique vector for the integer input vector
class SinusoidalPosEmb(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        device = x.device
        half_dim = self.dim // 2
        # Computes scaling factors
        emb = math.log(10000) / (half_dim - 1)
        # Generates sinusoidal frequencies
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        # Applies the frequencies to the posiitons
        emb = x[:, None] * emb[None, :]
        # Concatenates sin and cos functions to form positional embeddings
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb



# Encoder-only Transformer
class NanoTransformer(nn.Module):
    """
    This class implements a simplified Transformer model for sequence classification. 
    It uses an embedding layer for tokens, sinusoidal positional embeddings, 
    a Transformer, and a Linear layer.

    Args:
      num_emb: The number of unique tokens in the vocabulary. (vocab_size)
      output_size: The size of the output layer (number of classes). (4)
      hidden_size: The dimension of the hidden layer in the Transformer block (default: 128)
      num_heads: The number of heads in the multi-head attention layer (default: 4).
    """
    def __init__(self, num_emb, output_size, hidden_size=128, num_heads=4):
        # Inherits from nn.Module's attributes
        super(NanoTransformer, self).__init__()

        # Create an embedding for each token
        self.embedding = nn.Embedding(num_emb, hidden_size)
        # Scaling down the embedding weights
        self.embedding.weight.data = 0.001 * self.embedding.weight.data
        # Positional embedding
        self.pos_emb = SinusoidalPosEmb(hidden_size)

        # Multi-head attention
        self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True)

        # Linear layer
        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size),
                                 nn.LayerNorm(hidden_size),
                                 nn.ELU(),
                                 nn.Linear(hidden_size, hidden_size))
        
        self.fc_out = nn.Linear(hidden_size, output_size)

    def forward(self, input_seq):
        # batch_size, length
        bs, l = input_seq.shape
        input_embs = self.embedding(input_seq)

        # Add a unique embedding to each token embedding depending on it's position in the sequence
        seq_indx = torch.arange(l, device=input_seq.device)
        pos_emb = self.pos_emb(seq_indx).reshape(1, l, -1).expand(bs, l, -1)
        embs = input_embs + pos_emb
        output, attn_map = self.multihead_attn(embs, embs, embs)

        output = self.mlp(output)

        return self.fc_out(output), attn_map


# Example usage
num_emb = 1000  # Vocabulary size
output_size = 10  # Number of classes
hidden_size = 128
num_heads = 4

# Create a NanoTransformer instance
model = NanoTransformer(num_emb, output_size, hidden_size, num_heads)

# Sample input sequence (batch size = 2, sequence length = 4)
input_seq = torch.tensor([[5, 6, 7, 8], [9, 10, 11, 12]])

# Forward pass
output, attn_map = model(input_seq)

# Print the result
print("Output:", output)
print("Attention Map:", attn_map)



# Set the device to GPU if available, otherwise use CPU
device = torch.device(0 if torch.cuda.is_available() else 'cpu')

# Embedding size
hidden_size = 128

# Define the hyperparameters
# Learning rate for the optimizer
learning_rate = 1e-4

# Number of epochs for training
nepochs = 10

# Batch size for data loaders
batch_size = 32

# Maximum sequence length for text inputs
max_len = 127

# Create the Transformer model
tf_classifier = NanoTransformer(num_emb=len(vocab), output_size=4, hidden_size=hidden_size, num_heads=4).to(device)

# Initialize the optimizer
optimizer = optim.Adam(tf_classifier.parameters(), lr=learning_rate, weight_decay=1e-4)

# Cosine annealing scheduler to decay the learning rate
lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=nepochs, eta_min=0)

# Define the loss function
loss_fn = nn.CrossEntropyLoss()

# Custom transform to randomly replace a token with <pad>
td = TokenDrop(prob=0.1)

# Loggers for training and testing loss
training_loss_logger = []
test_loss_logger = []

# Loggers for training and testing accuracy
training_acc_logger = []
test_acc_logger = []


# Number of model parameters
num_model_params = 0
for param in tf_classifier.parameters():
    num_model_params += param.flatten().shape[0]

print("-This Model Has %d (Approximately %d Million) Parameters!" % (num_model_params, num_model_params//1e6))


for batch_idx, (labels, texts) in enumerate(data_loader_train):
    print(f"Batch {batch_idx + 1}")
    print(f"Labels: {labels[:2]}")
    print(f"Texts: {texts[:2]}")  # Print the first two texts for brevity
    if batch_idx == 0:
        break  # Inspecting only the first batch for demonstration

text_tokens = text_tranform(list(texts)).to(device)
label.shape


# Initialize progress bar for tracking epochs
# pbar = trange(0, nepochs, leave=False, desc="Epoch")
train_acc = 0
test_acc = 0

# Loop over each epoch
for epoch in range(nepochs):
    # Update the progress bar with current training and testing accuracy
    print('Accuracy: Train %.2f%%, Test %.2f%%' %(train_acc * 100, test_acc * 100))
    
    # Set the model to training mode
    tf_classifier.train()
    steps = 0
    
    # Loop over each batch in the training dataset
    for batch_idx, (labels, texts) in enumerate(data_loader_train):
        # Number of examples in the batch
        bs = labels.shape[0]
        
        # Transform the text to tokens
        text_tokens = text_tranform(list(texts)).to(device)
        labels = labels.to(device)

        # TokenDrop
        text_tokens = td(text_tokens)

        # Get the model predictions
        pred, _ = tf_classifier(text_tokens)

        # Compute the loss using cross-entropy loss
        loss = loss_fn(pred[:, 0, :], labels)
        
        # Backpropagation and optimization step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Log the training loss
        training_loss_logger.append(loss.item())
        
        # Update training accuracy
        train_acc += (pred[:, 0, :].argmax(1) == labels).sum()
        steps += bs
    
    # Calculate average training accuracy
    train_acc = (train_acc / steps).item()
    training_acc_logger.append(train_acc)
    
    # Update learning rate
    lr_scheduler.step()
    
    # Set the model to evaluation mode
    tf_classifier.eval()
    steps = 0
    
    # Loop over each batch in the testing dataset
    with torch.no_grad():
        for batch_idx, (labels, texts) in enumerate(data_loader_train):
            bs = labels.shape[0]
            
            # Transform the text to tokens and move to the GPU
            text_tokens = text_tranform(list(texts)).to(device)
            labesl = labels.to(device)

            # Get the model predictions (Do not return attention map)
            pred, _ = tf_classifier(text_tokens)

            # Compute the loss using cross-entropy loss
            loss = loss_fn(pred[:, 0, :], labels)
            test_loss_logger.append(loss.item())

            # Update testing accuracy
            test_acc += (pred[:, 0, :].argmax(1) == labels).sum()
            steps += bs

        # Calculate average testing accuracy
        test_acc = (test_acc / steps).item()
        test_acc_logger.append(test_acc)


_ = plt.figure(figsize=(10, 5))
_ = plt.plot(np.linspace(0, nepochs, len(training_loss_logger)), training_loss_logger)
_ = plt.plot(np.linspace(0, nepochs, len(test_loss_logger)), test_loss_logger)

_ = plt.legend(["Train", "Test"])
_ = plt.title("Training Vs Test Loss")
_ = plt.xlabel("Epochs")
_ = plt.ylabel("Loss")


_ = plt.figure(figsize=(10, 5))
_ = plt.plot(np.linspace(0, nepochs, len(training_acc_logger)), training_acc_logger)
_ = plt.plot(np.linspace(0, nepochs, len(test_acc_logger)), test_acc_logger)

_ = plt.legend(["Train", "Test"])
_ = plt.title("Training Vs Test Accuracy")
_ = plt.xlabel("Epochs")
_ = plt.ylabel("Accuracy")
print("Max Test Accuracy %.2f%%" % (np.max(test_acc_logger) * 100))


ag_news_classes = [
    "World",
    "Sports",
    "Business",
    "Science/Technology"
]

with torch.no_grad():
    label, text = next(iter(data_loader_train))
    text_tokens = text_tranform(list(text)).to(device)
    pred, attention_map = tf_classifier(text_tokens)


test_index = 3

# Choose a text index smaller than the total number in the batch!
assert test_index < label.shape[0]

# Select the attention map for a single sample and the first attention head
att_map = attention_map[test_index, 0]
pred_class = ag_news_classes[pred[test_index, -1].argmax().item()]
top5 = att_map.argsort(descending=True)[:5]
top5_tokens = vocab.lookup_tokens(text_tokens[test_index, top5].cpu().numpy())


print("Article:")
print(text[test_index])
print("\nPredicted label:")
print(pred_class)
print("True label:")
print(ag_news_classes[label[test_index].item()])

print("\nTop 5 Tokens:")
print(top5_tokens)


_ = plt.plot(att_map.cpu().numpy())
_ = plt.title("Attention score per token")
