with open ('tiny-shakespeare.txt', 'r', encoding = 'utf-8') as f:
    text = f.read()


print(len(text))


print(text[:1000])


# Get the set of all characters that occur in the set - then call list to create this list, and then sort alphabetically
# These are the possible characters our model can output

# Set of characters
chars = sorted(list(set(text)))
vocab_size = len(chars)
print(''.join(chars))
print(vocab_size)



# Tokenize the input text
# Convert thei input text into a sequence of numbers
# Trans late individual characdters into integers
# SentencePiece
# OpenAI tiktoken

# Build the encoder and decoder
stoi = {ch:i for i,ch in enumerate(chars)}
itos = {i:ch for i,ch in enumerate(chars)}

# Encoded text function
encode = lambda s: [stoi[c] for c in s] # Take a string, then output a vector

# Decoded text function
decode = lambda l: ''.join([itos[i] for i in l]) # Take a vector, output a string


import torch


# Convert our encoded text into a tensor (len(data) total)
data = torch.tensor(encode(text), dtype = torch.long)
data.shape


# 90% split
n = int(0.9 * len(data))
train_data = data[:n]
val_data = data[:n]

# Train on chunks (SGD)


decode(train_data[:100].tolist())


block_size = 8
# When training, we train for the transformer to make a prediction at every one of the 8 positions
# The labels at each index in the x batch would be:
train_data[:block_size + 1]


# In the context of 18, 47 comes next. In the context of 18, 47, 56 comes next


x = train_data[:block_size]
# Next characters, at every index in the block
y = train_data[1: block_size+1]
for t in range(block_size):
    context = x[:t+1]
    target = y[t]
    print(f" When input is {context} the target is {target}")

# For a block_size of n, there are n training examples


# We want the Transformer to see a size anywhere between 1 and 8
# The Transformer should know how to predict even just one character


# Process multiple chunks at the same time, but completely indepedently


batch_size = 4 # Total number of processes running in parallel
block_size = 8 # Maximum content length for predictions

def get_batch(split):
    data = train_data if split == 'train' else val_data
    # Generate random positions batch size number of random offsets
    # @torch documentation
    
    # Generates 4 random numbers
    ix = torch.randint(len(data) - block_size, (batch_size,))
    # torch.stack stacks the plucked examples
    # Each row represents a training example. Each column represents a word
    x = torch.stack([data[i:i+block_size]  for i in ix])
    # Offset by 1 to get the corresponding plucked labels
    y = torch.stack([data[i+1:i+block_size+1]  for i in ix])
    return x, y


xb, yb = get_batch('train')
print('inputs:')
print(xb.shape)
print(xb)

print('targets:')
print(yb.shape)
print(yb)

for i in range(batch_size):
    print(decode(xb[i].tolist()))

for i in range(batch_size):
    print(decode(yb[i].tolist()))


# Bigram language model

import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1337)

# Inherits from nn.Module
class BigramLanguageModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        # Creating embedding table (vocab size x vocab size) -> maps token ID to corresponding embedding vector (1, vocab_size)
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def forward(self, idx, targets=None):
        # idx is a tensor of shape (B,T) and so is targets

        # Convert idx into corresponding embeddings (B, T, C)
        logits = self.token_embedding_table(idx) # (B, T, C) for batch, time, channel
        # The time channel denotes the ability for the language model to use multiple training examples, from just one row in a batch
        # Scores for next character in the sequence

        if targets is None:
            loss = None
            
        else:
            # Reshaping for the loss function in PyTorch
            B, T, C = logits.shape
            logits = logits.view(B * T, C) # Flattens out in the 1st dimension
            # For every row, compute loss (B * T inputs, B * T labels)
            targets = targets.view(B * T)
            
            # Use cross-entropy loss function (expects 2D arrays)
            loss = F.cross_entropy(logits, targets)
        
        return logits, loss

    def generate(self, idx, max_new_tokens):
        # idx is (B, T) array of indices
        for _ in range (max_new_tokens):
            logits, loss = self(idx) # Get predictions
            
            # Extracts the logits from the last time_step in every iteration (given idx as input)
            logits = logits[:, -1, :] # (B, C)
            probs = F.softmax(logits, dim = -1) #(B, C)
            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)
            idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)
        return idx


m = BigramLanguageModel(vocab_size)

# PyTorch automatically computes forward if you pass in arguments like this
logits, loss = m(xb, yb)
# print(logits.shape)
# print(loss)


# Create a 1x1 tensor, holding a zero. Pass this in to generate method
print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))


# Create a PyTorch optimizer
optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)


batch_size = 32
for steps in range (1000):
    xb, yb = get_batch('train')

    logits, loss = m(xb, yb)
    optimizer.zero_grad(set_to_none = True)
    loss.backward()
    optimizer.step()

print(loss.item())


print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens = 300)[0].tolist()))





# Mathematical trick
# 8 tokens should "talk to eachother"
# 8 tokens at an index should only talk to tokens before it (previous context to current timestep)
torch.manual_seed(1337)
B, T, C  = 4, 8, 2
x = torch.randn(B,T,C)
x.shape


# Easiest way for tokens to communicate with its past is to do an average of the preceding elements
# Take channels from one step, and average channels from previous steps that summarizes features for example


# Version 1
# x[b,t] = mean {i<=t} x[b,i]
# Bag of words
# Weighted aggregation
xbow = torch.zeros((B,T,C))
for b in range (B):
    for t in range (T):
        # Previous dimensions are everything in the batch, up to and including the t'th token
        # This line gets all of the last dimension channels aswell
        xprev = x[b,:t+1] # (t, C)
        # print(xprev.shape)
        # Average over the zero dimension
        xbow[b,t] = torch.mean(xprev,0) # (1, C)
        print(xbow[b,t].shape)


# Version 2
wei = torch.tril(torch.ones(T,T))
wei = wei / wei.sum(1, keepdim = True)
# How much of every row we want to average up
print(wei)

# Applies for all batches
xbow2 = wei @ x # (B, T, T) @ (B, T, C) = (B, T, C)
# Show that both operations are doing the same thing
torch.allclose(xbow, xbow2)


# Version 3 - softmax
tril = torch.tril(torch.ones(T,T))
# Weights begin at zero. How much of tokens from past do we want to add
wei = torch.zeros((T,T))
# Tokens from the future cannot communicate
wei = wei.masked_fill(tril == 0, float('-inf'))
print(wei)
wei = F.softmax(wei, dim = 1)
xbow3 = wei @ x
print(wei)
torch.allclose(xbow, xbow3)


wei


# Show both are identical
xbow[0], xbow2[0]


x[0]


xbow[0]


torch.manual_seed(42)
# a = torch.ones(3,3)
a = torch.tril(torch.ones(3,3))
a = a / torch.sum(a, 1, keepdim = True)
b = torch.randint(0,10,(3,2)).float()
c = a @ b
print( 'a= ')
print(a)
print( 'b= ')
print(b)
print( 'c= ')
print(c)


torch.tril(torch.ones(3,3))
# Can normalize rows to sum to 1!
# Triangular lower portion of ones


# Version 4 - self attention head

torch.manual_seed(1337)
B, T, C = 4, 8, 32
x = torch.randn(B,T,C)

# Don't actually want uniformity - data dependent way
# Every single node / token at each position emits two vectors - query and key
# Query vector - What am I looking for?
# Key vector - What do I contain?
# Getting affinities - dot product between keys and queries
# Dot product Q and K becomes wei to learn about a token

head_size = 16
key = nn.Linear(C, head_size, bias = False)
query = nn.Linear(C, head_size, bias = False)
value = nn.Linear(C, head_size, bias = False)

k = key(x) # (B, T, 16)
q = query(x) # (B, T, 16)
# All of them in parallel produce a key and query
# Tranpose last dim, and second last
wei = q @ k.transpose(-2, -1) # (B,T,16) @ (B,16,T) -> (B, T, T)


tril = torch.tril(torch.ones(T,T))
# wei = torch.zeros((T,T))
wei = wei.masked_fill(tril == 0, float('-inf'))
wei = F.softmax(wei, dim = -1)
print(wei)

# V is the elements aggregated
# Attention is a communication mechanism
# A number of nodes in a directed graph with edges pointed towards other nodes
# Every node has a vector of information and gets to aggregate information via weighted sum in a data-dependent manner
# Can think of a token as nodes, and previous nodes pointing to the token
# Nodes have no idea where they are in the space
# Elements across batch dimensions never talk to eachother
# Not always should future tokens be zero'd out. 
# Encoder block - no trill
# Decoder block - with trill
# Self-attention - K, Q, V all come from the same source
# Attention is more general - K and Q may come from other places
# Cross attention - separate source of nodes to pool them into
# Scaled attention - divide by sqrt(dk) to preserve the variance of wei
# Softmax converges to one hot vectors. We do not want values to be too extreme

v = value(x)
out = wei @ v

out.shape


wei[0] 
# If a token finds previous tokens interesting, more of its information will be aggregated as an affinity
# Each instance of wei is encoded with the content and position


# Multi-head attention - multiple heads in parallel


# Decoder-only transformer
# Triangular mask on transformer - autoregressive property
# Encoder - decoder for translation
