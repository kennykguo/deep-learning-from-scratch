{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49835bb4",
   "metadata": {},
   "source": [
    "### Predicting Sequential Data\n",
    "The MLP will take in a sequence of days and predict the information for the next day. By setting day_range larger than (days_in + 1) we can see what happens when we feed the models prediction back in as an input during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from Dataset import WeatherDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81794c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file containing the weather dataset\n",
    "dataset_file = \"../data/weather.csv\"\n",
    "\n",
    "# Define the date to split the dataset into training and testing sets\n",
    "split_date = pd.to_datetime('2023-01-01')\n",
    "\n",
    "# Number of days in the input sequence\n",
    "day_range = 15\n",
    "\n",
    "# Number of days the MLP will take as input\n",
    "days_in = 14\n",
    "\n",
    "# Ensure that the total number of days in the input sequence is larger than the MLP input size\n",
    "assert day_range > days_in, \"The total day range must be larger than the input days for the MLP\"\n",
    "\n",
    "# Define the hyperparameters for training the model\n",
    "learning_rate = 1e-4  # Learning rate for the optimizer\n",
    "nepochs = 500  # Number of training epochs\n",
    "batch_size = 32  # Batch size for training\n",
    "\n",
    "# Create training dataset\n",
    "# This will load the weather data, consider sequences of length day_range, and split the data such that data before split_date is used for training\n",
    "dataset_train = WeatherDataset(dataset_file, day_range=day_range, split_date=split_date, train_test=\"train\")\n",
    "\n",
    "# Create testing dataset\n",
    "# This will load the weather data, consider sequences of length day_range, and split the data such that data after split_date is used for testing\n",
    "dataset_test = WeatherDataset(dataset_file, day_range=day_range, split_date=split_date, train_test=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(dataset_train)}')\n",
    "print(f'Number of testing examples: {len(dataset_test)}')\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "data_loader_test = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028df664",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "_ = plt.title(\"Melbourne Max Daily Temperature (C)\")\n",
    "\n",
    "_ = plt.plot(dataset_train.dataset.index, dataset_train.dataset.values[:, 1])\n",
    "_ = plt.plot(dataset_test.dataset.index, dataset_test.dataset.values[:, 1])\n",
    "\n",
    "_ = plt.legend([\"Train\", \"Test\"])\n",
    "# Note:see here how we can just directly access the data from the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a residual MLP block\n",
    "class ResBlockMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ResBlockMLP, self).__init__()\n",
    "        # Layer normalization for the input\n",
    "        self.norm1 = nn.LayerNorm(input_size)\n",
    "        # First fully connected layer that reduces the dimensionality by half\n",
    "        self.fc1 = nn.Linear(input_size, input_size // 2)\n",
    "        \n",
    "        # Layer normalization after the first fully connected layer\n",
    "        self.norm2 = nn.LayerNorm(input_size // 2)\n",
    "        # Second fully connected layer that outputs the desired output size\n",
    "        self.fc2 = nn.Linear(input_size // 2, output_size)\n",
    "        \n",
    "        # Skip connection layer to match the output size\n",
    "        self.fc3 = nn.Linear(input_size, output_size)\n",
    "\n",
    "        # Activation function\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply normalization and activation function to the input\n",
    "        x = self.act(self.norm1(x))\n",
    "        # Compute the skip connection output\n",
    "        skip = self.fc3(x)\n",
    "        \n",
    "        # Apply the first fully connected layer, normalization, and activation function\n",
    "        x = self.act(self.norm2(self.fc1(x)))\n",
    "        # Apply the second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Add the skip connection to the output\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class ResMLP(nn.Module):\n",
    "    def __init__(self, seq_len, output_size, num_blocks=1):\n",
    "        super(ResMLP, self).__init__()\n",
    "        \n",
    "        # Compute the length of the sequence data\n",
    "        seq_data_len = seq_len * 2\n",
    "        \n",
    "        # Define the input MLP with two fully connected layers and normalization\n",
    "        self.input_mlp = nn.Sequential(\n",
    "            nn.Linear(seq_data_len, 4 * seq_data_len),\n",
    "            nn.ELU(),\n",
    "            nn.LayerNorm(4 * seq_data_len),\n",
    "            nn.Linear(4 * seq_data_len, 128)\n",
    "        )\n",
    "\n",
    "        # Define the sequence of residual blocks\n",
    "        blocks = [ResBlockMLP(128, 128) for _ in range(num_blocks)]\n",
    "        self.res_blocks = nn.Sequential(*blocks)\n",
    "        \n",
    "        # Final output fully connected layer\n",
    "        self.fc_out = nn.Linear(128, output_size)\n",
    "        # Activation function\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Reshape the input sequence to be a flat vector\n",
    "        input_seq = input_seq.reshape(input_seq.shape[0], -1)\n",
    "        # Pass the input through the input MLP\n",
    "        input_vec = self.input_mlp(input_seq)\n",
    "\n",
    "        # Pass the output through the residual blocks and activation function\n",
    "        x = self.act(self.res_blocks(input_vec))\n",
    "        \n",
    "        # Compute the final output\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "weather_mlp = ResMLP(seq_len=days_in, output_size=2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(weather_mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the number of epochs\n",
    "for epoch in trange(nepochs, desc=\"Epochs\", leave=False):\n",
    "    # Set the model to training mode\n",
    "    weather_mlp.train()\n",
    "    \n",
    "    # Iterate over the training data loader\n",
    "    for day, month, data_seq in tqdm(data_loader_train, desc=\"Training\", leave=False):\n",
    "        \n",
    "        # Extract the initial sequence block to be used as input for the model\n",
    "        seq_block = data_seq[:, :days_in].to(device)\n",
    "        \n",
    "        # Initialize the loss for the current batch\n",
    "        loss = 0\n",
    "        \n",
    "        # Iterate over the remaining sequence to predict the next day values\n",
    "        for i in range(day_range - days_in):\n",
    "            # Get the target sequence block for the next day\n",
    "            target_seq_block = data_seq[:, i + days_in].to(device)\n",
    "            \n",
    "            # Make predictions using the model\n",
    "            data_pred = weather_mlp(seq_block)\n",
    "            \n",
    "            # Accumulate the loss for the current prediction\n",
    "            loss += loss_fn(data_pred, target_seq_block)\n",
    "            \n",
    "            # Update the input sequence by removing the oldest date and adding the new prediction\n",
    "            # Detach the new sequence to prevent backpropagation through the old sequence\n",
    "            seq_block = torch.cat((seq_block[:, 1:, :], data_pred.unsqueeze(1)), 1).detach()\n",
    "\n",
    "        # Average the accumulated loss over the number of steps\n",
    "        loss /= i + 1\n",
    "        \n",
    "        # Zero the gradients before performing backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform backpropagation to compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model parameters using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training loss for later analysis\n",
    "        training_loss_logger.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66946d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(training_loss_logger)\n",
    "_ = plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test dataset values to a PyTorch tensor\n",
    "data_tensor = torch.FloatTensor(dataset_test.dataset.values)\n",
    "\n",
    "# Initialize a list to log predictions\n",
    "log_predictions = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "weather_mlp.eval()\n",
    "\n",
    "# Disable gradient calculation for the prediction process\n",
    "with torch.no_grad():\n",
    "    # Extract the initial sequence block to be used as input for the model\n",
    "    # - unsqueeze(0) adds a batch dimension to the input\n",
    "    seq_block = data_tensor[:days_in, :].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Iterate over the sequence to predict the next day values\n",
    "    for i in range(data_tensor.shape[0] - days_in):\n",
    "        # Make predictions using the model\n",
    "        data_pred = weather_mlp(seq_block)\n",
    "        \n",
    "        # Log the prediction\n",
    "        log_predictions.append(data_pred.cpu())\n",
    "        \n",
    "        # Update the input sequence by removing the oldest date and adding the new prediction\n",
    "        seq_block = torch.cat((seq_block[:, 1:, :], data_pred.unsqueeze(1)), 1)\n",
    "\n",
    "# Concatenate the logged predictions into a single tensor\n",
    "predictions_cat = torch.cat(log_predictions)\n",
    "\n",
    "# Unnormalize the predictions using the dataset's standard deviation and mean\n",
    "un_norm_predictions = (predictions_cat * dataset_test.std) + dataset_test.mean\n",
    "\n",
    "# Unnormalize the original data using the dataset's standard deviation and mean\n",
    "un_norm_data = (data_tensor * dataset_test.std) + dataset_test.mean\n",
    "\n",
    "# Trim the initial sequence from the unnormalized data to match the length of predictions\n",
    "un_norm_data = un_norm_data[days_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d337f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = (un_norm_data - un_norm_predictions).pow(2).mean().item()\n",
    "print(\"Test MSE value %.2f\" % test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ea26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 5))\n",
    "_ = plt.plot(un_norm_data[:, 0])\n",
    "_ = plt.plot(un_norm_predictions[:, 0])\n",
    "_ = plt.title(\"Rainfall (mm)\")\n",
    "\n",
    "_ = plt.legend([\"Ground Truth\", \"Prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab48c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10, 10))\n",
    "_ = plt.plot(un_norm_data[:, 1])\n",
    "_ = plt.plot(un_norm_predictions[:, 1])\n",
    "_ = plt.title(\"Max Daily Temperature (C)\")\n",
    "\n",
    "_ = plt.legend([\"Ground Truth\", \"Prediction\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep-learning)",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
